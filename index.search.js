var relearn_search_index=[{content:`Welcome and What is this…? Welcome to the archived parts of my development brain… what?
As a busy, full stack, multi-language developer working on multiple projects I have come to realise that there is a limit to how much I can fit into my head at the same time.
In my experience if I am actively using a few a languages or tools these will be at the front of mind and quick to access and use for solving the problems at hand.
But what about those things (languages, tools, techniques) you are not using so often? Maybe you previously solved a problem using a certain tool but if you are not actively using that tool now, maybe it has been a while, what then? Maybe you are just starting to learn a new language or tool, but you are still getting the hang of the syntax.
Sure you could try to mine the depths of your memories, or google hard until you stumble blindly across it (or an alternative). But this just seems a little at odd with me, remember you have done this before, you just need a little reminder!
Enter this site. This site (and the GitHub repo that backs it) is my attempt to capture those little notes, those little ‘ahaa’ moments when you just got it. When things just clicked, and you got that concept or the job done.
This site enables me to archive those thoughts and (hopefully) be able to easily find them and use them when they are needed next time. The information here is not intended to be a tutorial, although my contain links to some, but merely serves as a nudge to quickly recall a technical topic or solution, a cheat sheet.
If you have stumbled upon this site and find its contents useful, that is fantastic! Happy to have helped!
Regardless of how you have arrived here, Welcome to my Development Cheat Sheets!
`,description:"",tags:null,title:" ",uri:"/index.html"},{content:`An overview of how I set up Jenkins to be able to build (docker) images with kaniko inside my Kubernetes cluster.
Goals and situation This project spawns out of a want/need to automate the creation of custom Docker images. I wanted to be able to create these custom images within my home CI/CD environment (Jenkins) and be able to publish them to my private image registry using a GitOps style pattern which is triggered from a commit to my private git repository (gogs).
I also wanted to parameterise the location of my private image registry so that if it changes in the future it can be easily updated in the build.
kaniko:debug image The entrypoint on the standard kaniko image is set to /kaniko/executor. This is not an issue if the required build arguments are passed into the container as soon as it start.
Unfortunately this doesn’t work in (my?) Jenkins setup as the build job spins up a pod with all containers needed to do the build tasks. When this happens we need the kaniko container to wait until the earlier build stages/steps, such as git checkout of the source code building of any required artifacts ect, to have been completed before we are ready to build the image.
To accommodate this pattern we can use the kaniko:debug image which includes a busybox shell. We can then use a simple sleep <duration> command to keep the kaniko container alive until it is needed.
For this to work we need to override the default image entrypoint, by using the command: [ "/busybox/sleep" ] in our Kubernetes pod.
Project Folder structure Below is an overview of the important files within this project:
.jenkins/build-pod.yaml Dockerfile Jenkinsfile Jenkins pod template I like to externalise the pod template that Jenkins uses in a separate file. For this project I came up the Jenkins pod template below:
# # .jenkins/build-pod.yaml # This is the pod template that Jenkins will use for the build # --- apiVersion: v1 kind: Pod metadata: labels: build-job: kaniko-build spec: containers: - name: jnlp image: "jenkins/inbound-agent:4.11.2-4" - name: kaniko # Use the 'debug' version as it contains busybox (and sleep) # which means we can get the kaniko container to wait for the other Jenkins build stages to complete image: "kaniko-project/executor:debug" # Override the default Kaniko entrypoint/command command: [ "/busybox/sleep" ] args: ["10m"] tty: true volumeMounts: - mountPath: /kaniko/.docker name: kaniko-secret volumes: - name: kaniko-secret secret: secretName: kaniko-image-registry-secret nodeSelector: # Best to build on 'standard' chip architecture kubernetes.io/arch: amd64 Image registry secret For kaniko to push the built image to a private, or protected, image registry it needs the authentication details. We are going to use a Kubernetes secret which will be mapped to the kaniko container at runtime.
base64 encoded credentials First we need to base64 encode the image registry credentials for the kaniko configuration:
echo -n <USERNAME>:<PASSWORD> | base64 kaniko config.json Next we need to embed these credentials into kaniko’s configuraiton file which looks like this:
{ "auths": { "https://nexus.dev-space.duckdns.org:5001" : { "auth": "<YOUR_BASE64_ENCODED_CREDENTIALS>" } } } Create Kubernetes secret Lastly we need to create a secret in Kubernetes to store the kaniko configuration. For this we use the command:
kubectl create secret generic kaniko-image-registry-secret --from-file=config.json Jenkins Properties I wanted to parameterise my private image registry location so that is more portable and easier to change in the future.
To do this I used Jenkins Global Environmental variables, which can be reference in the build job or Jenkinfile and are resolved to their set values during the build.
To set these in Jenkins go to: Dashboard –> Manage Jenkins –> System –> Global properties –> Environment variables
Here you can set whatever values you need, for me this was: 1.
Name: IMAGE_REGISTRY_PULL Value: <IMAGE_REGISTRY_HOST>: Name: IMAGE_REGISTRY_PUSH Value: <IMAGE_REGISTRY_HOST>: Dockerfile For this project we have a very simple Dockerfile as follows:
# # Description: Builds a simple image with inotify-tools and rsync # ARG IMAGE_REGISTRY="" FROM \${IMAGE_REGISTRY}alpine:3.18.4 RUN /bin/sh -c set -eux; apk update; apk add inotify-tools rsync ; rm -rf /var/cache/apk/* Note the use of the Docker ARG IMAGE_REGISTRY so that we can set a pull registry rather than using the kaniko defaults.
Jenkinsfile Next we create our Jenkinfile something like the following for using a Dockerfile
pipeline { agent { kubernetes { yamlFile '.jenkins/build-pod.yaml' //retries 1 } } options { // We want to skip any of the later stages if the Build becomes unstable skipStagesAfterUnstable() } stages { stage("echo variables"){ steps { echo "Hello from Jenkins build script" echo "Using image pull registry: \${IMAGE_REGISTRY_PULL}" echo "Using image push registry: \${IMAGE_REGISTRY_PUSH}" } } stage('kaniko build') { steps { echo 'Kaniko building...' container('kaniko') { script { sh ''' /kaniko/executor --dockerfile \`pwd\`/Dockerfile \\ --context \`pwd\` \\ --build-arg "IMAGE_REGISTRY=\${IMAGE_REGISTRY_PULL}/" \\ --destination=\${IMAGE_REGISTRY_PUSH}/container-tools/inotify-rsync:latest ''' } } } } } } Jenkins build job and commit hook The last things to do is to set up the Jenkins build job (just pointing it at your git repository and using the Jenkinsfile), and a commit hook to trigger the multibranch-scan-webhook-trigger at JENKINS_URL/multibranch-webhook-trigger/invoke?token=BUILD_JOB_TOKEN_HERE when commits are made to the git repository.
Sorry, I am not detailing these steps in this post you will need to find/discover these details on your own :)
Done! You should now have a working Jenkins build project which uses kaniko within your Kubernetes cluster to build your docker images and push them to your image registry.
Helpful links Below are some links to the pages I found helpful for this project:
kaniko - Build Images In Kubernetes How to use Kaniko to build container image on Jenkins Kubernetes - Define a Command and Arguments for a Container Dockerfile FROM + ARG trick — parameterized dockerfile base image `,description:"",tags:["kaniko","jenkins","kubernetes","containers","docker"],title:"Building images with Kaniko in Jenkins",uri:"/posts/2023-10-27-kaniko-in-jenkins/index.html"},{content:"",description:"",tags:null,title:"containers",uri:"/tags/containers/index.html"},{content:"",description:"",tags:null,title:"docker",uri:"/tags/docker/index.html"},{content:"",description:"",tags:null,title:"jenkins",uri:"/tags/jenkins/index.html"},{content:"",description:"",tags:null,title:"kaniko",uri:"/tags/kaniko/index.html"},{content:"",description:"",tags:null,title:"kubernetes",uri:"/tags/kubernetes/index.html"},{content:`Posts Building images with Kaniko in JenkinsAn overview of how I set up Jenkins to be able to build (docker) images with kaniko inside my Kubernetes cluster.
Dell Inspiron 7537 - Preventing Linux shutdown when closing the lidThis is a little post about how to prevent a Dell Inspiron laptop from shutting down or going into standby when you close the lid on the laptop.
Hack for SQLite use on KubernetesThis page captures a technique I developed to work around the issues you can encounter when using Persistent Volumes (PVs) that contain SQLite databases used by containers in Kubernetes.
K3s on Raspberry PiA project to set up a small scale multi worker Kubernetes instance at home (sometimes called a ‘home lab’).
K3s with customised Traefik IngressThis post contains some details on how to customise the k3s (default) traefik ingress controller.
Migrating from Jekyll to HugoThis post contains information about the migration of my Github development-cheat-sheets page (this site) from Jekyll.
Migrating Gogs from SQLite to MariaDBThis page documents my efforts to migrate my Gogs database from SQLite to MariaDB.
My First Hugo PostWelcome Hello world, this is my first Hugo blog post. I hope you like it!
Oracle XE 11g to 19c UpgradeThis post captures the steps that I went through to upgrade my Oracle XE 11g database, which I use for some of my development projects, to Oracle XE version 19c.
Photo Finder Script(s)This post contains information about creating various Windows powershell scripts to assist with managing our photos.
Running Stable Diffusion on low resource PC (Dell Inspiron 7537)This page documents my efforts to get Stable Diffusion running on my old Dell Inspiron 7537 laptop.
`,description:"",tags:null,title:"Posts",uri:"/posts/index.html"},{content:"",description:"",tags:null,title:"Tags",uri:"/tags/index.html"},{content:"",description:"",tags:null,title:"development",uri:"/tags/development/index.html"},{content:"",description:"",tags:null,title:"github",uri:"/tags/github/index.html"},{content:"",description:"",tags:null,title:"github_pages",uri:"/tags/github_pages/index.html"},{content:"",description:"",tags:null,title:"hugo",uri:"/tags/hugo/index.html"},{content:"",description:"",tags:null,title:"jekyll",uri:"/tags/jekyll/index.html"},{content:`This post contains information about the migration of my Github development-cheat-sheets page (this site) from Jekyll.
Background Jekyll has a notoriously slow build time.
Ror me this is a real annoyance. I don’t claim to be the best developer and believe that you need to be able to iterate quickly through a change-test-fix cycle when developing anything.
Additionally, I like to use Docker containers for anything that has a complex setup/configuration. Using Docker containers allows me to setup/configure something once and have it run the same way where-ever it is deplopyed/used.
Jekyll’s slow build times, even with a relatively small static website like this one, and its relatively difficult Ruby installation and setup has made me look for an alternative static site generator.
Enter Hugo…
Migration Process As stated above I prefer to use Docker where I can. Do to a limitation with Docker on Windows which prevents Windows file systems changes from propagating into the linux file system within the container.
This means Hugo (in Docker) is unable to detect file systems changes and rebuild the site.
Unfortunately this meant that I couldn’t use the Hugo Docker image for the migration.
The initial steps below were based upon the Hugo Quick Start guide.
Create the basic Hugo project structure with:
hugo new site quickstart Note: from here on out you need to use the quickstart directory for all of the hugo commands/content.
(Optional) If your project/folder is not a git repo initialise one there with git init
Add a cool theme, I selected the relearn theme, by:
hugo mod init github.com/<your_user>/<your_project> In my case this was hugo mod init github.com/ben-kemister/development-cheat-sheets
Update the config.toml with the theme module to:
[module] [[module.imports]] path = 'github.com/McShelby/hugo-theme-relearn' # Change the default theme to be use when building the site with Hugo theme = "hugo-theme-relearn" Create the first post with:
hugo new posts/my-first-hugo-post.md Start serving the site (showing drafts) with:
hugo serve -D --disableFastRender Enable the search functionality, within config.toml add:
# For search functionality [outputs] home = [ "HTML", "RSS", "SEARCH", "SEARCHPAGE"] Add my favicon to the site static/images/favicon.png as per the theme’s instructions.
Move and update the Jekyll pages (*.md files) to the Hugo content directory and syntax. This mostly involved the following changes:
copying the page to Hugo’s content directory updated the page’s frontmatter by changing any name properties to be title creating _index.md files for any group/sections adding to autogenerate child pages where required fixed syntax highlighting by replacing {% highlight java %} blocks with \`\`\`java blocks Make sure to commit the go.mod and go.sum files
Deleted the remaining Jekyll files and folders and moved the contents of the quickstart directory into the root of the git repository/
Lastly, followed Hugo’s Host on GitHub instruction to be able to host the site on github pages.
All Done!
`,description:"",tags:["jekyll","hugo","development","github","github_pages"],title:"Migrating from Jekyll to Hugo",uri:"/posts/jekyll-to-hugo-migration/index.html"},{content:'This post contains information about creating various Windows powershell scripts to assist with managing our photos.\nDo I have these files in another directory? # # Description: This script tries to locate the files from one directory -InputPath within another -SearchPath # # Created: 03 April 2023 # param ( [string]$InputPath = $(throw "-InputPath is required"), [string]$FileFilter = "*", [bool]$MatchSize = $true, [bool]$Verbose = $false, [string]$SearchPath = $(throw "-SearchPath is required") ) Write-Host "" Write-Host "File Finder script started" -ForegroundColor Green Write-Host "" $filesToFind = Get-ChildItem -Recurse -File -Filter $FileFilter -Path $InputPath $message = [string]::Format("Looking for {0} files", $filesToFind.Length) Write-Host $message if($MatchSize){ Write-Host " with matching file sizes" } Write-Host "" $filesFound = [System.Collections.Generic.List[System.Object]]::new() $filesNotFound = [System.Collections.Generic.List[System.Object]]::new() function AddToNotFound { param($FileNotFound) if($Verbose){ Write-Host "Cound not file: \'$FileNotFound\'" -ForegroundColor Yellow } [void]$filesNotFound.Add($FileNotFound) } foreach( $file in $filesToFind){ $found = Get-ChildItem -Recurse -File -Filter $file -Path $SearchPath # $found = Get-ChildItem -Recurse -File -Filter "not.here" -Path $SearchPath if(-not $Verbose){ Write-Host "." -NoNewline } if($found){ if( -not $MatchSize){ if($Verbose) { Write-Host "Found file \'$file\' in"$found.Count"locations: " foreach ($matchingFile in $found) { Write-Host " "$matchingFile.Directory } } [void]$filesFound.Add($found) } else { # Length matching $matchesLenght = $false foreach($matchingFile in $found) { if( $file.Length -eq $matchingFile.Length){ # Matches length $matchesLenght = $true if($Verbose) { Write-Host "Found \'$file\' in: "$matchingFile.Directory } [void]$filesFound.Add($found) } } if( -not $matchesLenght){ # Does not match length AddToNotFound -FileNotFound $file } } } else { AddToNotFound -FileNotFound $file } } Write-Host "" # Write-Host "Files to find Length: " $filesToFind.Length # Write-Host "Files found Length: " $filesFound.Count # Write-Host "Files not found Length: " $filesNotFound.Count if($filesToFind.Length -eq $filesFound.Count) { Write-Host "" $message = [string]::Format("All {0} files found in {1}", $filesToFind.Length, $SearchPath) Write-Host $message -ForegroundColor Blue Write-Host "" } else { Write-Host "" $message = [string]::Format("Only {0}/{1} files found!", $filesFound.Count, $filesToFind.Length) Write-Host $message -ForegroundColor Red # Missing files Write-Host "" $message = [string]::Format("Missing {0} files!", $filesNotFound.Count) Write-Host $message -ForegroundColor Yellow foreach($file in $filesNotFound){ Write-Host " $file" -ForegroundColor Yellow } } ',description:"",tags:["powershell","scripting","photos"],title:"Photo Finder Script(s)",uri:"/posts/photo-helper-scripts/index.html"},{content:"",description:"",tags:null,title:"photos",uri:"/tags/photos/index.html"},{content:"",description:"",tags:null,title:"powershell",uri:"/tags/powershell/index.html"},{content:"",description:"",tags:null,title:"scripting",uri:"/tags/scripting/index.html"},{content:`Welcome Hello world, this is my first Hugo blog post.
I hope you like it!
`,description:"",tags:["hugo"],title:"My First Hugo Post",uri:"/posts/my-first-hugo-post/index.html"},{content:`Apache Tomcat is a free and open-source implementation of the Jakarta Servlet, Jakarta Expression Language, and WebSocket technologies. It provides a “pure Java” HTTP web server environment in which Java code can also run.
It is a Java web application server, although not a full JEE application server.
Environmental Variables The setenv.[bat|sh] script can be used to set all environment variables apart from CATALINA_HOME and CATALINA_BASE.
The script is placed either into CATALINA_BASE/bin or into CATALINA_HOME/bin directory and is named setenv.bat (on Windows) or setenv.sh (on *nix). The file has to be readable and by default the setenv script file is absent.
You can use this script to configure the JRE_HOME variable (for example) by creating following script file:
On Windows, %CATALINA_BASE%\\bin\\setenv.bat: set "JRE_HOME=%ProgramFiles%\\Java\\jre8" exit /b 0 For more information see https://tomcat.apache.org/tomcat-10.0-doc/RUNNING.txt
Manager Interface The Tomcat Manager App is a web application that is packaged with the Tomcat server. It provides the basic functionality needed to manage deployed web applications.
There two management interface options here:
A web-based (HTML) application - for humans A text-based web service - for scripting The web-based management management interface at http[s]://<server>:<port>/manager/html/
A user account is needed to access the management interface (i.e. a role of manager-gui) which needs to be configured in the <TOMCAT_HOME>/conf/tomcat-users.xml file.
For example:
<user username="admin" password="admin" roles="manager-gui"/> `,description:"",tags:["java","tomcat","webserver"],title:"(Apache) Tomcat",uri:"/languages/java/tomcat/index.html"},{content:"",description:"",tags:null,title:"actuator",uri:"/tags/actuator/index.html"},{content:"",description:"",tags:null,title:"ai",uri:"/tags/ai/index.html"},{content:"",description:"",tags:null,title:"alpine",uri:"/tags/alpine/index.html"},{content:"Alpine Linux is a Linux distribution designed to be small, simple, and secure.\nIt uses musl, BusyBox, and OpenRC instead of the more commonly used glibc, GNU Core Utilities, and systemd.\nDue to its small size it is often used as the base for Docker images.\nAdding packages Alpine uses apk to install packages, but the syntax is similar to apt and apt-get. The example below shows how to install some packages:\n# Install rysnc and inotifywait apk update; apk add inotify-tools rsync Within Docker image If you are building a Docker image you should remove any apk caches from the image. So a line from a Dockerfile might look something like:\nRUN /bin/sh -c set -eux; apk update; apk add inotify-tools rsync ; rm -rf /var/cache/apk/* # Alternatively RUN apk add --no-cache inotify-tools rsync Users & Groups The commands to add users and groups in Alpine linux are a little different to the standard linux command used in debian based distributions.\nCreate a Group # Create the group, ignoring any errors if a group with the same ID already exists addgroup -g $PGID -S backup-group || true Create a User Setting up a new user # Create the user with a given UID adduser --uid $PUID --system backup-user Add a user to a group $ addgroup --help BusyBox v1.29.3 (2019-01-24 07:45:07 UTC) multi-call binary. Usage: addgroup [-g GID] [-S] [USER] GROUP Add a group or add a user to a group -g GID Group id -S Create a system group `\n# Add a user to a group (given the Group ID) addgroup backup-user $(getent group $PGID | awk -F ':' '{ print $1 }') ",description:"",tags:["operating_system","linux","alpine"],title:"Alpine",uri:"/operating-systems/alpine/index.html"},{content:"",description:"",tags:null,title:"android",uri:"/tags/android/index.html"},{content:"",description:"",tags:null,title:"ansible",uri:"/tags/ansible/index.html"},{content:`Ansible provides open-source automation that reduces complexity and runs everywhere.
Using Ansible lets you automate virtually any task. Ansible is available as a Community version and a Red Hat licenced version.
Pages InventoryInventories organize managed nodes in centralized files that provide Ansible with system information and network locations.
VaultAnsible Vault encrypts variables and files so you can protect sensitive content such as passwords or keys rather than leaving it visible as plaintext in playbooks or roles.
`,description:"",tags:["ansible","automation"],title:"Ansible",uri:"/tools/ansible/index.html"},{content:"",description:"",tags:null,title:"apache",uri:"/tags/apache/index.html"},{content:`The Apache HTTP Server is a free and open-source cross-platform web server software, released under the terms of Apache License 2.0.
Using Environmental variables in *.conf files You can use the \${VARIABLE_NAME} syntax to use an environmental variables within *.conf files.
For example:
ProxyPass /\${SOME_PATH}/ \${TOMCAT_URL} Proxying a Tomcat backend To connect Apache httpd to a Tomcat backend you first need to load some additional modules:
LoadModule proxy_module modules/mod_proxy.so LoadModule proxy_http_module modules/mod_proxy_http.so Then you need to define which path(s) are going to be proxied. You do this by adding the following line to your configuration:
# Make sure that you end both parameters with a slash '/' ProxyPass /my-path/ http://localhost:8080/app_context/ `,description:"",tags:["apache","http","httpd","tomcat"],title:"Apache HTTP server (httpd)",uri:"/tools/httpd/index.html"},{content:"",description:"",tags:null,title:"array",uri:"/tags/array/index.html"},{content:`This page contains information, and simple code examples, about the use of Arrays in JavaScript.
Introducing Arrays Arrays allow you to store an ordered list of data of any type. The order of the elements are preserved and the keys are automatically assigned. An array can also store mixed data types (e.g. booleans, Strings, and objects can all exist in the same array).
Defining an Array //Empty Array var myArray = []; var daysOfTheWeek = ['Sunday', 'Monday', 'Tuesday']; // Array storing mixed types var arrayOfStuff = [ {'name': 'value'}, [1,2,3], true, 'nifty' ]; Accessing Array Items Arrays use a 0 based index, so the index of the first element an array is 0.
You access the elements in the array using the square braces notation.
var chips = [ 'Smiths', 'Doritos', 'Twisties', 'Thins' ]; chips[2] //returns 'Twisties' push() - Adding items to an Array There are two ways that you can add elements to an array, using the square braces notation or the push() method.
var chips = [ 'Smiths', 'Doritos', 'Twisties', 'Thins' ]; // Adds "Red Rock Deli" to the end of the array chips[chips.length] = "Red Rock Deli" // Adds "Lays" to the end of the array chips.push("Lays"); //returns 6, the number of items in the array pop() - Removing an item from the end of an Array To remove (and return) the last item from an array use the pop() method.
var chips = [ 'Smiths', 'Doritos', 'Twisties', 'Thins' ]; // Removes and returns the last item in the array chips.pop() // returns 'Thins' splice() - Removing items from the middle of an Array If you simply delete a particular index in an array (e.g. delete chips[2]) it will not change the size of the array, it will just replace the value of that index with empty.
To remove an item and reduce the arrays length use the splice(index, items) method.
var chips = [ 'Smiths', 'Doritos', 'Twisties', 'Thins' ]; // Starting at index 2 removes 1 item from the array chips.splice(2, 1); // returns 'Twisties' chips.length // returns 3. map() method The map() method accepts a callback function which operates on each item in the array and returns a new array.
function doubleIt(number) { return (number *= 2); } var myNumbers = [1, 2, 3, 4, 5]; var myDoubles = myNumbers.map(doubleIt); myDoubles; // Returns [2, 4, 6, 8, 10] forEach() method Operates on each element in an array, but does not return anything.
Note that the break keyword will not work inside the foreach() method.
find() method The find() method returns the value of the first element in the provided array that satisfies the provided testing function. If no matching element is found then undefined is returned.
const find = (code: string): AirlineCode => { const upperCode = code.toUpperCase(); let found = airlineCodes.find( element => element.code === upperCode); // If no matching element is found then undefined is returned if (found) { return found; } throw new Error(No matching code found.'); } `,description:"",tags:["javascript"],title:"Arrays",uri:"/languages/javascript/arrays/index.html"},{content:`This page contains information, syntax, and simple code examples, about the use of the AssertJ test library. The AssertJ library is a fluent API to help create readable assertions within you Java unit tests.
Testing thrown exceptions - AssertThatThrownBy() import static org.assertj.core.api.Assertions.assertThatThrownBy; @Test void test_methodShouldThrowException(){ assertThatThrownBy(() -> { List<String> list = Arrays.asList("String one", "String two"); list.get(2); }).isInstanceOf(IndexOutOfBoundsException.class) .hasMessageContaining("Index: 2, Size: 2"); } Testing methods that don’t return a value import static org.assertj.core.api.Assertions.assertThatCode; @Test void test_methodWithNoReturnValue(){ assertThatCode(() -> doWork()) .doesNotThrowAnyException(); } `,description:"",tags:["java","testing","development"],title:"AssertJ",uri:"/languages/java/assertj/index.html"},{content:"",description:"",tags:null,title:"automation",uri:"/tags/automation/index.html"},{content:`AWK is a domain-specific language designed for text processing and typically used as a data extraction and reporting tool.
Like sed and grep, it is a filter, and is a standard feature of most Unix-like operating systems.
Simple output formatting > echo "ONE two" | awk '{ print $1, "Something else", $2 }' ONE Something else two Use a different Separator By default, AWK uses a space as a separator. You can change the separator as follows:
# Use the colon (:) as the separator > echo "ONE:two" | awk -F ':' '{ print $1 }' ONE `,description:"",tags:["unix","linux"],title:"awk",uri:"/operating-systems/linux/tools/awk/index.html"},{content:"",description:"",tags:null,title:"aws",uri:"/tags/aws/index.html"},{content:`The AWS Command Line Interface (CLI) is a unified tool to manage AWS services. Using the tool you can control multiple AWS services from the command line and automate them through scripts.
Configuration Quickly Configuring the AWS CLI If you just want to use some simple commands, maybe with something like localstack, then you can use the aws configure command:
PS> aws configure AWS Access Key ID [None]: accessKeyId AWS Secret Access Key [None]: secretAccessKey Default region name [None]: ap-southeast-2 Default output format [None]: PS> DynamoDB List tables The command below will return a list of the tables in the DB at the end of the url.
aws dynamodb list-tables --endpoint-url http://localhost:4569 SQS (Simple Queue Service) CLI commands for the SQS can be found at the cli reference.
List Queues The aws sqs list-queues will return a list of the queues available at the given endpoint.
PS> aws sqs list-queues --endpoint-url http://localhost:4576 { "QueueUrls": [ "http://localhost:4576/queue/FileQueue.fifo" ] } Queue Attributes (including message count) The aws sqs get-queue-attributes will return the attributes for a given queue, including a ApproximateNumberOfMessages property.
PS> aws sqs get-queue-attributes --queue-url http://localhost:4576/queue/FileQueue --attribute-names All --endpoint-url http://localhost:4576 { "Attributes": { "VisibilityTimeout": "30", "DelaySeconds": "0", "ReceiveMessageWaitTimeSeconds": "0", "ApproximateNumberOfMessages": "0", "ApproximateNumberOfMessagesNotVisible": "0", "ApproximateNumberOfMessagesDelayed": "0", "CreatedTimestamp": "1585549003", "LastModifiedTimestamp": "1585549003", "QueueArn": "arn:aws:sqs:us-east-1:000000000000:FileQueue", "FifoQueue": "false", "ContentBasedDeduplication": "true" } } Purge Messages The aws sqs purge-queue will delete all messages on the queue.
PS> aws sqs purge-queue --queue-url http://localhost:4576/queue/FileQueue --endpoint-url http://localhost:4576 PS> Send Message to Queue The aws sqs send-message will add a message to the queue.
PS> aws sqs send-message --message-body "{}" --queue-url http://localhost:4576/queue/FileQueue --endpoint-url http://localhost:4576 { "MD5OfMessageBody": "99914b932bd37a50b983c5e7c90ae93b", "MessageId": "d9f0cade-1427-4950-81db-cbe14e5df786" } `,description:"",tags:["cloud","aws"],title:"AWS Command Line Interface (CLI)",uri:"/tools/aws_cli/index.html"},{content:"",description:"",tags:null,title:"bash",uri:"/tags/bash/index.html"},{content:`This page contains handy information on the basics of the Javascript Syntax.
Basic JavaScript types/primatives The basic JavaScript Types/primatives are:
String Boolean Number BigInt Null Undefined Symbol (starting in ES6) Object Variables Can be named anything that starts with a letter, underscore ( _ ), or dollar sign ( $ ).
var monster1 = "Grover", monster2 = "Cookie Monster";
camelCase is a common naming convention used for Javascript variables.
You cannot use reserved words (such as var) for variable names.
Variable Scopes Variables can either be global or local.
The use of global variables is generally harmful. Global variables are shared amongst all of the scripts that are running, and as a result you might get unexpected behavior if another script is using a variable with the same name.
Global Variables If you assign a variable without using a keyword (such as var) it will be created as a global variable.
// Global Variable myPet = "cat"; If you have enabled strict mode (by adding "use strict"; at the top of your file) you will get a warning if you using any global variables.
Local Variables To define a local variable use the keyword var.
// Local variable var myState = "ACT"; Functions are the main delimiter of variable scope when using the var keyword.
In ECMAScript 2015 or above you can also use let and/or const, these have block scope.
Strings String literals are Strings which are contained within quotation marks, for example:
var myString = "A String Literal";
Escaping and Multi-line Strings The backslash ( \\ ) is the Javascript escape character.
"This is Joes' \\"favorite\\" string";
The backslash ( \\ ) can also be used for strings that run over multiple lines.
This is \\ Joe'/s favorite \\ string EVER String properties String properties are accessed using the dot ( . ) notation to access the desired property.
var myString = "This is my string."; myString.length //returns 18 String functions String functions are accessed similar to Java methods using the format string.funciton(), for example:
var myString = "This is my string."; myString.toUpperCase() //returns "THIS IS MY STRING." String concatenation To concatenate Strings use the plus ( + ) symbol.
"Cat " + "Dog"; //returns "Cat Dog" Template Strings Template strings allow you to use the contents of variables inside a String;
Note the use of backticks ( \` ) NOT the use of single quotes ( ' ) around the statement.
var myName = 'Bob'; // myGreeting is "Hello, my name is Bob!" var myGreeting = \`Hello, my name is \${myName}!\`; function hello(name){ console.log(\`Hello, my name is \${name}!\`); } // Prints 'Hello, my name is Fred!' hello('Fred') Numbers Unlike other programming languages, all numbers in Javascript are of the same type called number.
Javascript can also represent the number infinity, by using the keyword Infinity.
Javascript also as the concept of ‘Not a Number’, which is represented by the keyword NaN.
The Number Object There is a built in Number object in Javascript which has some handy methods.
The Number.isNaN(number) method is handy for checking if the number is a valid number.
Number.isNaN(NaN); // returns true The Math Object There is a global object called Math which contains a heap of properties and methods for dealing with number types.
Math.round(12.522141415254) // returns 13 Booleans Booleans are either true or false values, which must be spelt correctly and be all lowercase.
Objects Javascript uses objects to represent and structure data beyond simple types such as booleans and Strings.
You can define object literals by using the curly braces ( {} ), for example:
var myEmptyObject = {}; //Empty object var notEmptyObject = { 'label': 'value', 'label2': 25, 'label3': true, }; You can also define the labels in an object without using quotation marks like this:
var bird = { commonName: 'raven', callType: "squawky", quote: "Nevermore", noisy: true, deadly: false, }; Object Properties To retrieve a property from an object you can use either dot notation or square braces. Square braces is useful if you have spaces or special characters in the property name.
// Dot notation bird.quote // returns "Nevermore" // Square braces bird["quote"] //returns "Nevermore" Adding or Deleting properties You can add new properties to an object by just using assignment.
// Dot notation bird.whereItLives = "in a tree" You remove properties from an object by using the delete keyword. Deleting works with either Dot or Square braces notation.
//Removes the whereItLives property from the object delete bird.whereItLives; Object References Be aware that variables are really references to the object (similar to the Java programming language).
This means that if you assign one variable to equal another, they both refer to the same object.
Objects (and Arrays) are passed by reference to functions. Take this into consideration if you have a case where you do not want to modify the original object. In this case you may want to create and return a new object.
Copying objects A common quick way to make an exact copy of an object is by using the JSON object as follows:
//Makes a copy of the animal object safely. animal2 = JSON.parse(JSON.stringify(animal)); Arrays Arrays allow you to store an ordered list of data of any type. The order of the elements are preserved and the keys are automatically assigned. An array can also store mixed data types (e.g. booleans, Strings, and objects can all exist in the same array).
The sections below show some of the basics of creating and using arrays, more detailed information can be found on the
Defining an Array //Empty Array var myArray = []; var daysOfTheWeek = ['Sunday', 'Monday', 'Tuesday']; // Array storing mixed types var arrayOfStuff = [ {'name': 'value'}, [1,2,3], true, 'nifty' ]; Accessing Array Items Arrays use a 0 based index, so the index of the first element an array is 0.
You access the elements in the array using the square braces notation.
var chips = [ 'Smiths', 'Doritos', 'Twisties', 'Thins' ]; chips[2] //returns 'Twisties' Adding items to an Array There are two ways that you can add elements to an array, using the square braces notation or the push() method.
var chips = [ 'Smiths', 'Doritos', 'Twisties', 'Thins' ]; // Adds "Red Rock Deli" to the end of the array chips[chips.length] = "Red Rock Deli" // Adds "Lays" to the end of the array chips.push("Lays"); //returns 6, the number of items in the array Removing an item from the end of an Array To remove (and return) the last item from an array use the pop() method.
var chips = [ 'Smiths', 'Doritos', 'Twisties', 'Thins' ]; // Removes and returns the last item in the array chips.pop() // returns 'Thins' Comments Javascript supports two styles of comments, line comments and block comments.
Be careful when using block comments if you have valid */ characters in your code.
// This is a line comment /* This is a block comment, which can be multiple lines */ Regular Expressions To define a regular expression you use forward slashes ( / ) around the regular expression you want to use. Any flags appear after the closing forward slash.
var string1 = 'Wow, this is the longest string Ever.'; // This is a regular expression var regex = /ever/; console.log( regex.test(string1) ); //returns false // a case insensitive regular expression regex = /ever/i; console.log( regex.test(string1) ); //returns true Simple Comparisons Strict Equality ( === ) Strict equality compares two values for equality. If the values have the same type, are not numbers, and have the same value, they’re considered equal. Finally, if both values are numbers, they’re considered equal if they’re both not NaN and are the same value, or if one is +0 and one is -0.
For more information see the Mozilla Developer Equality page.
The Strict Equality operator ( === ) test whether the thing on the left is identical to the thing on the right.
There is also a Strict Inequality operator ( !== ) which test whether the thing on the left is not identical to the thing on the right.
var one = 1, two = 2; one === one; // Returns true one !== one; // Returns false one !== two; // Returns true Loose Equality ( == ) Loose equality compares two values for equality, after converting both values to a common type. After conversions (one or both sides may undergo conversions), the final equality comparison is performed exactly as === performs it.
For more information see the Mozilla Developer Equality page.
var one = 1, two = 2; one == one; // Returns true one == "1"; // Returns true one != "1"; // Returns false Arithmetic Operators In general Javascript arithmetic operators use the same syntax as Java.
Modulus Operator ( % ) The modulus operator ( % ) return the remainder after the division.
20 % 2; // Returns 0 21 % 2; // Returns 1 //Using modules to test if a number is even or odd 10 % 2 === 0; // Returns true Incrementing and Decrementing var counter = 1; counter += 1; // Returns 2 counter++; //Returns 2, but counter is now 3 counter *= 2; // Returns 6 Logical Operators Logical AND ( && ) Logical OR ( || ) Logical NOT ( ! ) Conditionals If Statement Javascript if statements use the same syntax as Java if and if-else statements.
var answer = window.confirm("Click OK, get true. Click CANCEL, get false."); if(answer === true){ console.log("You said true!"); } else { console.log("You said something else"); } Switch Statement Javascript switch statements use the same syntax as Java switch statements.
Remember that if you do not have a break the execution will ‘fall through’ each case until a break statement is encountered.
switch (answer){ case "YES": console.log("You said YES!"); break; case "MAYBE": console.log("You said MAYBE..."); break; case "NO": console.log("You said NO. :("); break; default: console.log("Not sure how to deal with that"); break; } Terse ifs Single line ifs You can leave off the curly braces of an if statement and it will execute the next line. Note it only executes a single line.
var cherub = "Cupid"; if( cherub === "Cupid" ) console.log("Ouch, an arrow!"); else console.log("I feel nothing"); if (variable) ‘Truthy’ In JavaScript, a truthy value is a value that is considered true when encountered in a Boolean context. All values are truthy unless they are defined as falsy (i.e., except for false, 0, 0n, “”, null, undefined, and NaN).
For more information see the Truthy page on the Mozilla Developer network.
if("variable"){ console.log("The value is truthy!") } Ternary Operator The Javascript ternary operator follows the same syntax as Java, it allows for a short form of an if statement.
var animal = "cat"; animal === "cat" ? console.log("You'll need a cat herder!") // Executed if true : console.log("Call the dog catcher."); // Executed if false Type Checking Because Javascript is a loosely typed language you may need to check the type of an object before you perform operations on it.
Type checking is done using the typeof keyword. The typeof function returns a String which is the name of the data type all in lowercase.
var thing = 12; typeof thing; // Returns "number" thing = "twelve"; typeof thing; // Returns "string" thing = {}; typeof thing; // Returns "object" thing = []; typeof thing; // Returns "object" typeof NaN; // Returns "number" typeof null; // Returns "object" hasOwnProperty() method Every object in Javascript has the method hasOwnProperty("propertyName"), this method allows you to check that an object has a particular property prior to accessing it.
var thing = []; typeof thing; // Returns "object" typeof thing === "object" && thing.hasOwnProperty("length") // Returns true instanceof Operator The instanceof operator checks that the instance of the class has a constructor which matches the given class.
class MyClass { someProperty: number; } let obj = new MyClass(1); console.log( obj instanceof MyClass); // Returns true. Use on Custom Errors Note if you are using this on custom Error objects (i.e. Objects that extends Error) you will need to be using ES6 or greater, otherwise it will not work as expected.
test('Test \\'instanceof\\' on Custom Errors', () => { class MyError extends Error { someProperty: number; } let obj = new MyError('an error'); let result = obj instanceof Error; expect(result).toBe(true); /** * If you are using < ES6 then the use of 'instanceof' * on a Custom Error will not work as expected. */ result = obj instanceof MyError; expect(result).toBe(true); // This will fail on < ES6 }); Loops Remember you can break (short circuit) a for loop, just the same as you can in Java.
(Sequential) For Loops The Javascript for loop syntax is the same as Java.
for (var i = 0; i < 10; i++) { console.log(i); // Prints 0...9 } Enumerative (for each) Loops Allows you to iterate over an object or array. Note that the order of the iteration is not guaranteed, if you need a guaranteed order use the Sequential For Loop.
//Iterating over an array var pageNames = [ "Home", "About Us", "Contact", "News", "Posts" ]; for ( var p in pageNames) { console.log(p, pageNames[p]); // p is the index of element } //Iterating over the properties of an Object var pages = { first: "Home", second: "About Us", third: "Contact", fourth: "News", fifth: "Posts" }; for (var p in pages) { if(pages.hasOwnProperty(p)) { console.log( p, pages[p] ); // p is the name of the property } } While Loops While loops are handy if you don’t know how many times you will be iterating over an array or object.
Be careful that if the terminating condition always equates to true then the while loop will continue indefinately.
var i = 0; while (i < 10 ) { // This is the terminating condition console.log(i + "... This will go until we hit 10"); i += 1; } Do While loop The Do While loop is handy if you want the loop to execute at least once.
var myArray = [true, true, true, false, true, true]; var myItem = false; do { console.log("myArray has" + myArray.length + " items now. This loop will go until we pop a false." ); myItem = myArray.pop(); }while (myItem !== false); Functions Functions are one of the fundamental building blocks in JavaScript. A function is a JavaScript procedure—a set of statements that performs a task or calculates a value.
A function definition (also called a function declaration, or function statement) consists of the function keyword, followed by:
The name of the function. A list of parameters to the function, enclosed in parentheses and separated by commas. The JavaScript statements that define the function, enclosed in curly brackets, {…}. For more information see the Functions Page on Mozilla Developer network
// Function Definition function speak() { console.log('Woof'); console.log('Meow'); console.log('Quack'); console.log('Mooooo'); } // Invoke the function speak(); Functions are first class citizens in Javascript, they are objects that have the power to be invoked.
All functions in Javascript inherit from the global Function object, which means that they are objects.
This also means that functions can be assigned to (and passed around using) variables. An example of this can be seen when dealing with anonymous functions.
// Anonymous function assigned to a variable var say = function(what) { what = what || "Speaking!"; for (var i = 0; i < 10; 1 += 1) { console.log(what); } } // Invoke the function speak(); Return statement A function can return a value by using the return keyword. The return keyword can also be used to terminate execution of the function, in this case the value returned is undefined.
function myFunction(test) { if (test) { return; } else { return 0; } } myFunction(false); //Returns 0 myFunction(true); //Returns undefined Function Arguments Primitive parameters (such as a number) are passed to functions by value; the value is passed to the function, but if the function changes the value of the parameter, this change is not reflected globally or in the calling function.
If you pass an object (i.e. a non-primitive value, such as Array or a user-defined object) as a parameter and the function changes the object’s properties, that change is visible outside the function.
function isEven(num) { return num % 2 === 0; } // Invoke the function isEven(44); //Returns true The arguments object and variable numbers of arguments Every function in Javascript has an arguments object. The arguments object is an array like object which can be used to access the functions arguments.
By using the arguments object you can make functions that can accept a variable amount of arguments.
function total() { var total = 0; for (var i = 0; i < arguments.length; i += 1) { var number = arguments[i]; if (typeof number === 'number') { total += number; } } return total; } // Invoke the function total(); // Returns 0 total(1,2,3,4); // Returns 10 Default Arguments In ECMAScript 2015 (ES6) you can define default values for the function parameters when they are declared.
Be careful using defaults as the first argument provided to the function will always be considered to be the first, regardless of type and any defaults.
// Default values in ES6 function speakNew(what = "Default speech", times) { for (var i = 0; i < times, i += 1) { console.log(what + " (" + i + ")" ); } } // Default values in < ES6 function speakOld(what, times) { // Check if the argument is defined. var what = (typeof what !== "undefined") ? what : "Default speech"; for (var i = 0; i < times, i += 1) { console.log(what + " (" + i + ")" ); } } Member Functions (a.k.a methods) You can defined a function so that it is a top level member function, also called a method. In this way the function can be invoked by calling the property name assigned to that function.
var obj = { // The function is assigned to the property 'sayHello' sayHello: function() { console.log("Hello"); } }; // Invoking the function obj.sayHello(); Callback Functions A callback is jargon for a function that is passed into another function as an argument, and executed in that function.
Arrow Functions In ECMAScript 2015 (ES6) there is a function called an arrow function. This is a very concise way of writing a function that reduces the amount of boiler-plate code that needs to be written.
// Typical Function Definition function doubleIt(number) { return (number *= 2); } // In arrow function format doubleIt = number => (number *= 2); Arrow functions follow the form:
function name = arguments => return statement;
getter function (get keyword) The get keyword binds an object property to a function that will be called when that property is looked up.
const obj = { log: ['a', 'b', 'c'], get latest() { if (this.log.length == 0) { return undefined; } return this.log[this.log.length - 1]; } } console.log(obj.latest); // expected output: "c" setter function (set keyword) The set keyword binds an object property to a function to be called when there is an attempt to set that property.
const language = { set current(name) { this.log.push(name); }, log: [] } language.current = 'EN'; language.current = 'FA'; console.log(language.log); // expected output: Array ["EN", "FA"] Promises Promises are objects that captures the result of an asynchronous action with a particular programming interface (API) for handling the data when it finally came through, or failed.
Defining a Promise Promises need to define a resolve argument, we call resolve(...) when what we were doing asynchronously was successful, and reject(...) when it failed.
let myFirstPromise = new Promise((resolve, reject) => { // We call resolve(...) when what we were doing asynchronously was successful, and reject(...) when it failed. // In this example, we use setTimeout(...) to simulate async code. // In reality, you will probably be using something like XHR or an HTML5 API. setTimeout( function() { resolve("Success!") // Yay! Everything went well! }, 250) }); Using Promises Promises have a method named then() which accepts a callback/function which will be invoked after the asynchronous action has completed.
The then() method can be used to chain functions together that will be invoked in order,
For more information see the Promise page on the Mozilla Developer Network.
// Function which returns a promise. const delay = seconds => { return new Promise(resolve => { // The 'resolve' function is called after seconds * 1000 seconds. setTimeout(resolve, seconds * 1000); }); } console.log('Zero Seconds'); // The anonymous function console.log() is called after 1 second. delay(1).then(() => console.log('One Second')); // The anonymous function console.log() is called after 3 second. delay(3).then(() => console.log('Three Second')); async and await keywords async and await are a part of the ECMAScript 2017 specification.
These keywords make working with promises much easier, and mean that you can reduce the code clutter.
// One request aysnc function getOneThing() { // await makes the return of a promise var response = await axios.get("https://httpbin.org/get"); // Now I have the response } Many browsers (and Node.js) support async and await.
Prototypes and Classes Object-oriented Javascript uses Prototypical Inheritance, where child objects have a link ( __proto__ ) to their parent object.
You can use prototypes to assign functions and properties to a prototype so that it is available to all objects that use that prototype.
Cake.prototype.bake = function(temp, minutes) { // function details... } The class keyword was introduced to help developers migrating from other languages. It is Syntactic Sugar and doesn’t really change the way that the programming language works.
Ultimately classes get turned into prototypes eventually.
For more information on classes see Classes in JavaScript.
import Dynamic Imports is the specification which describes how to load related scripts or library files in JavaScript.
The static import statement is used to import bindings which are exported by another module. More Information
// One of the many versions of the import statement. import { export1 as alias1 } from "module-name"; Browser support for Dynamic Imports is still a bit flakey so tools (such as webpack and rollup.js) have been written to help with this.
`,description:"",tags:["javascript"],title:"Basic Syntax",uri:"/languages/javascript/syntax/index.html"},{content:`This page contains information about the basics of TypeScript syntax.
Describing your code with types The general rule of thumb about using types is the more information you can provide the more the tooling will be able to help you.
Variables Simply add a semicolon after the variable name and then the type.
let trackingNumber: string = 'FD1234567'; let createDate: Date = new Date(); Functions and parameters function getItem(trackingNumber: string): object { // Do stuff in here... return null; } // Using a predefined type or interface function getItemBetter(trackingNumber: string): InventoryItem { // Do stuff in here... return null; } You can also define the structure of the return object in line as follows:
function getItem(trackingNumber: string): { displayName: string; trackingNumber: string; createDate: Date; originalCost: number; } { // Do stuff in here... return null; } Casting to different Types The as keyword allows you to cast between different types.
The any type The any type allows you to bypass TypeScript’s inbuilt type checking allowing you to use the dynamic typing native to JavaScript.
Bewared that using the any type you are not using TypeScripts ability to help with typing checking, effectively opting out of type checking.
Interfaces Interfaces allow you to define the structure of a type in TypeScript.
Note that interface definitions do not end up in the compiled JavaScript code.
interface InventoryItem { displayName: string; trackingNumber: string; createDate: Date; originalCost: number; } Method definitions You can define method definitions within an interface in two ways:
interface InventoryItem { displayName: string; trackingNumber: string; createDate: Date; originalCost: number; // addNote method definition #2 addNote: (note: string) => string; } Optional properties You can mark properties and methods as optional in your interface by using the question mark ( ? ).
interface InventoryItem { displayName: string; trackingNumber: string; createDate: Date; // Optional Property originalCost?: number; // Optional method addNote?: (note: string) => string; } Read only properties You can mark properties of an object as being read only by using the readonly keyword.
TypeScript will throw an error if you try to change a readonly property to a new value after it has been defined.
interface InventoryItem { displayName: string; // Read Only property readonly trackingNumber: string; createDate: Date; originalCost?: number; addNote?: (note: string) => string; } Enumerations (a.k.a. enum) An enum type is a type which defines a set of known values.
enum InventoryItemType { Computer, Furniture } By default TypeScript assigns the values of enums to numbers. You can override this default behavior and set the values to something else, such as Strings.
enum InventoryItemType { Computer = 'computer', Furniture = 'furniture' } Literal types Literal Types act similarly to enums in that you can define a restricted set of values for a particular property.
Literal Types tend require less code to setup than an enum.
interface InventoryItem { displayName: string; // Literal Types inventoryType: 'computer' | 'furniture' readonly trackingNumber: string; createDate: Date; originalCost?: number; addNote?: (note: string) => string; } Multiple types (Union types) You can define a property which can be one of many types (union types) using the pipe ( | ).
let originalCost: number | string = 425; orignalCost = 'Sooooo much money!'; You can also define a type so that you can reuse this union in multiple locations or definitions with a meaningful name.
type Cost = number | string; Be careful when assigning union types to another (single typed) variable as TypeScript will throw an error. In this case you need to do some type checking before the assignment.
type Cost = number | string; let originalCost: Cost; if (typeof originalCost === 'number'){ let cost: number = originalCost; } Classes Although it is a very common JavaSCript pattern to define properties in the objects constructor this is not allowed in TypeScript.
In TypeScript you must declare the class’s properties at the class level for example:
class InventoryStore { // You need to define a class's properties at this level in TypeScript _categories; _items; constructor(){ this._categories = []; this._items = []; } } `,description:"",tags:["typescript"],title:"Basic Syntax",uri:"/languages/typescript/syntax/index.html"},{content:"",description:"",tags:null,title:"battery",uri:"/tags/battery/index.html"},{content:"",description:"",tags:null,title:"blob",uri:"/tags/blob/index.html"},{content:`The Broadlink (RM Mini) black bean is a small universal Infra Red (IR) remote control which connects to and receives commands over WiFi.
It is often used in Home Automation setups due to its low price and relatively easy integration with other systems.
IR Commands The IR commands used by the Black Bean need to be in a specific format. Typically, this is in a Base64 encoded byte array.
Converting Pronto IR codes Often IR codes are published on the internet in Pronto format, which looks something like this:
0000 006D 0000 0022 00AC 00AB 0015 0041 0015 0041 0015 0041 0015 0016 0015 0016 0015 0016 0015 0016 0015 0016 0015 0041 0015 0041 0015 0041 0015 0016 0015 0016 0015 0016 0015 0016 0015 0016 0015 0041 0015 0016 0015 0016 0015 0041 0015 0016 0015 0041 0015 0041 0015 0041 0015 0016 0015 0041 0015 0041 0015 0016 0015 0041 0015 0016 0015 0016 0015 0016 0015 0689 To use these with a Broadlink device you will need to convert them to a Base 64 encoded byte array.
I found Sensus web interface the easiest way to do this conversion.
Just paste the Pronto formatted string, making sure there are spaces between the groups of 4 digits, and press convert. The Broadlink B64 field will contain the value which you can use with your Broadlink device.
`,description:"",tags:["openhab","automation","infra_red"],title:"Broadlink (Black Bean) Devices",uri:"/hardware/broadlink/index.html"},{content:"",description:"",tags:null,title:"camera",uri:"/tags/camera/index.html"},{content:`Adding a camera to a Raspberry Pi opens up a new category of potential Raspberry Pi projects. Popular projects include time-lapse photography, face recognition, DIY CCTV, pet and 3D printer monitors, car cams and more.
Using or accessing the Camera Picamera is a great Python package which provides an interface to the Raspberry Pi camera module for Python 2.7 (or above) or Python 3.2 (or above).
Timelapse Links Raspberry Pi Zero W as a headless time-lapse camera Raspberry Pi Time-Lapse App - Github How to Shoot Time-Lapse Videos with Raspberry Pi Raspberry Pi Time-Lapse in Four Easy Steps Links About the Camera Modules - Raspberry Pi Documentation Cameras Compared for Raspberry Pi Raspberry Pi - High Quality Camera Product Brief Adjust Raspberry Pi camera focus to fix the blurry issue Camera & Photography Terms ISO In very basic terms, ISO is simply a camera setting that will brighten or darken a photo. As you increase your ISO number, your photos will grow progressively brighter. For that reason, ISO can help you capture images in darker environments.
A photo taken at too high of an ISO will show a lot of grain, also known as noise, and might not be usable. So, brightening a photo via ISO is always a trade-off. You should only raise your ISO when you are unable to brighten the photo via shutter speed or aperture.
Shutter Speed Shutter speed is the length of time the camera shutter is open, exposing light onto the camera sensor. Essentially, it’s how long your camera spends taking a photo.
When you use a long shutter speed (also known as a “slow” shutter speed), you end up exposing your sensor for a significant period of time. The first big effect of it is motion blur. If your shutter speed is long, moving subjects in your photo will appear blurred.
The other big effect of shutter speed is on exposure, which relates to the brightness of an image. If you use a long shutter speed, your camera sensor gathers a lot of light, and the resulting photo will be quite bright.
Dynamic Range Compression Dynamic range is the ratio of the largest to the smallest value of a measured signal. In photography it commonly refers to the ratio of the brightest element of a scene to the darkest. An outdoor scene on a very foggy day commonly has very little difference between the brightest and darkest elements, which is known as a low dynamic range scene. In contrast, an indoor scene with a visible sunny sky through a window is known as a high dynamic range scene.
The dynamic range of a scene can easily exceed the dynamic range of the “sensor” that captures the scene. Photography and image processing needs to deal with mapping high dynamic ranges to lower ones. This is done by either discarding some of the data outside the range or by using compression.
`,description:"",tags:["raspberry_pi","camera","timelapse"],title:"Camera Module (Raspberry Pi)",uri:"/hardware/raspberry-pi/camera-module/index.html"},{content:`Cascading Style Sheets (CSS) is a style sheet language used for describing the presentation of a document written in a markup language such as HTML or XML (including XML dialects such as SVG, MathML or XHTML).
CSS is a cornerstone technology of the World Wide Web, alongside HTML and JavaScript.
Comments /* This is a single-line comment */ p { color: red; } `,description:"",tags:["html","css"],title:"Cascading Style Sheets (CSS)",uri:"/languages/css/index.html"},{content:"",description:"",tags:null,title:"Categories",uri:"/categories/index.html"},{content:"",description:"",tags:null,title:"certificates",uri:"/tags/certificates/index.html"},{content:"",description:"",tags:null,title:"ci_cd",uri:"/tags/ci_cd/index.html"},{content:`This page contains information, syntax, and simple code examples, about the use of Java’s Class class/API.
getCanonicalName() Returns the full classname including package.
LOGGER.info("ClassName is: " + this.getClass().getCanonicalName()); `,description:"",tags:["java","development"],title:"Class",uri:"/languages/java/class/index.html"},{content:`This page contains more detailed information about classes in JavaScript.
Introduction The class keyword was introduced to help developers migrating from other languages. It is Syntactic Sugar and doesn’t really change the way that the programming language works.
Ultimately classes get turned into JavaScript prototypes eventually.
Constructor A constructor is a special function that is called immediately after the object is created. It can be used to hold any initialisation logic (such as initialise any properties) for the object before it is used.
class MyClass { constructor() { // Initialise properties this._name = 'Default Name'; // load initial set of data this._isInitialised = this._load(); } } Naming Conventions The name of an object’s private properties start with an underscore ( _ ) for example:
class Lady { constructor() { // naming convention for private properties this._age; } /** * A protected/private method. * / _save() { // do something. } } Note: This does not actually make the property private, it is just a naming convention, and is meant to discourage other developers from changing the value of the property.
Singletons You can structure a class to work as a singleton, so that there is only one instance of the class for the entire application.
class MySingleton { constructor() { // initialisation logic } // Create a 'static' singleton for the entire application MySingleton.instance = new MySingleton(); // Expose the singleton in its own variable cost mySingleton = MySingleton.instance; } `,description:"",tags:["javascript"],title:"Classes",uri:"/languages/javascript/classes/index.html"},{content:"",description:"",tags:null,title:"cli",uri:"/tags/cli/index.html"},{content:"",description:"",tags:null,title:"clob",uri:"/tags/clob/index.html"},{content:"",description:"",tags:null,title:"cloud",uri:"/tags/cloud/index.html"},{content:`This page contains information, syntax, and simple code examples, about the use of collections in Java.
Arrays Arrays.asList(array) Converts an array of objects into a list, great one-liner to use when you need to iterate over the array.
// Great way to create a collection within one line List<String> strings = Arrays.asList("one", "two", "three"); Sets Copying/Cloning Sets Copy Constructor One way of copying a Set is to use the copy constructor of a Set implementation. The copy constructor is a special type of constructor that is used to create a new object by copying an existing object.
Set<T> copy = new HashSet<>(original); Note we are not really cloning the elements of the given set. We’re just copying the object references into the new set. For that reason, each change made in one element will affect both sets.
Java 10 Java 10 adds a new method to the Set interface that allows us to create an immutable set from the elements of a given collection:
Set<T> copy = Set.copyOf(original); Note that Set.copyOf expects a non-null parameter.
`,description:"",tags:["java","list","array"],title:"Collections",uri:"/languages/java/collections/index.html"},{content:`cmd.exe is the default command-line interpreter for OS/2, eComStation, Windows, Windows CE, and the ReactOS operating systems.
The name refers to its executable filename. It is also commonly referred to as cmd or the Command Prompt, referring to the default window title on Windows.
type The type command is similar to linux’s cat command, it can be used to output the contents of a file. For example:
>type test.bat type test.bat @echo off echo Content-Type: text/plain echo. echo Hello, World! Environmental Variables Environment variables are global settings for your Linux, Mac, or Windows computer, stored for the system shell to use when executing commands. For more information see HowTo: Set an Environment Variable in Windows - Command Line and Registry.
Print Environmental Variables To list all environment variables.
C:\\> set To print a particular environment variable:
C:\\> echo %JAVA_HOME% `,description:"",tags:["windows","terminal"],title:"Command Prompt (cmd.exe)",uri:"/languages/cmd/index.html"},{content:"",description:"",tags:null,title:"concept",uri:"/tags/concept/index.html"},{content:"",description:"",tags:null,title:"Concepts",uri:"/concepts/index.html"},{content:`Java Concurrency This page contains information, syntax, and simple code examples, about the use of Java’s concurrency functionality.
Common ForkJoin Pool Java has a common static ForkJoin pool which is appropriate for most applications.
Using the common pool normally reduces resource usage (its threads are slowly reclaimed during periods of non-use, and reinstated upon subsequent use).
In practice what this means is there is an easy way to submit tasks to a background thread without needing to deal with the complexities of an ExecutorService.
javafx.concurrent.Task<String> longRunningTask = new Task<String>() { @Override protected String call() throws Exception { //The call() method is run on a background thread. return longRunningStringProducer(); } }; ForkJoinPool.commonPool().execute(longRunningTask); See the ForkJoinPool JavaDocs for more information.
`,description:"",tags:["java"],title:"Concurrency",uri:"/languages/java/concurrency/index.html"},{content:"",description:"",tags:null,title:"configuration",uri:"/tags/configuration/index.html"},{content:`This page provides information about how to configure linux.
Change the hostname For more information on changing the hostname see this webpage.
Temporary change In the terminal, type the following, replacing new-hostname with the name you choose:
sudo hostname new-hostname Permanent change Use set-hostname to Change the Hostname, type the following command:
hostnamectl set-hostname new-hostname Use your own hostname choice instead of new-hostname.
You can confirm the change using hostnamectl and checking the output.
Change/Set IP address For Debian based distros:
# find the interface name to change with $ ifconfig eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500 inet 192.168.0.167 netmask 255.255.255.0 broadcast 192.168.0.255 inet6 fe80::e280:ee50:6b3b:efd prefixlen 64 scopeid 0x20<link> ether b8:27:eb:4c:65:1f txqueuelen 1000 (Ethernet) RX packets 4534 bytes 831537 (812.0 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 1553 bytes 196406 (191.8 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 # change the IP address sudo ifconfig eth0 192.168.0.5 netmask 255.255.255.0 https://www.howtogeek.com/118337/stupid-geek-tricks-change-your-ip-address-from-the-command-line-in-linux/
If the IP address revert, check the /etc/dhcpcd.conf file
`,description:"",tags:["linux","configuration","bash","sh"],title:"Configuration (Linux)",uri:"/operating-systems/linux/configuration/index.html"},{content:"",description:"",tags:null,title:"container",uri:"/tags/container/index.html"},{content:"",description:"",tags:null,title:"css",uri:"/tags/css/index.html"},{content:"",description:"",tags:null,title:"curl",uri:"/tags/curl/index.html"},{content:"",description:"",tags:null,title:"database",uri:"/tags/database/index.html"},{content:"",description:"",tags:null,title:"db",uri:"/tags/db/index.html"},{content:"",description:"",tags:null,title:"debian",uri:"/tags/debian/index.html"},{content:`The Linux Debian operating system is used as the base for many other linux operating systems such as Ubuntu and the Raspberry Pi OS (formerly Raspbian).
apt Advanced Package Tool, or APT, is a free-software user interface that works with core libraries to handle the installation and removal of software on Debian, and Debian-based Linux distributions. APT simplifies the process of managing software on Unix-like computer systems by automating the retrieval, configuration and installation of software packages, either from precompiled files or by compiling source code.
Remove a repo If you are getting an error when trying to run an update sudo apt update you can remove the repo as a source with the following:
Check your /etc/apt/sources.list and remove unwanted sources. sudo nano /etc/apt/sources.list If step 1 does not solve your problem, see what other sources are used. sudo ls -al /etc/apt/sources.list.d/ If you have found a source-list causing the problem, remove it (in the example below, NodeJS source list is removed): sudo rm /etc/apt/sources.list.d/nodesource.list `,description:"",tags:["operating_system","linux","debian"],title:"Debian",uri:"/operating-systems/debian/index.html"},{content:"",description:"",tags:null,title:"dell",uri:"/tags/dell/index.html"},{content:`This is a little post about how to prevent a Dell Inspiron laptop from shutting down or going into standby when you close the lid on the laptop.
This issue came up when I was trying to repurpose an old Dell Inspiron 7537 laptop as a Kubernetes (k3s) worker node.
For some reason I couldn’t get Ubuntu server to install on the laptop so instead I just installed Linux Mint which has worked really well on that laptop in the past
It’s all about /etc/systemd/logind.conf After discovering that changing the settings in the Mint GUI didn’t seem to work I came across this post on stackoverflow.
For me this meant changing /etc/systemd/logind.conf as follows:
kem@dell-inspiron-7537:~$ sudo nano /etc/systemd/logind.conf ... # Old default value #HandleLidSwitch=suspend HandleLidSwitch=ignore ... # Old default value #HandleLidSwitchExternalPower=suspend HandleLidSwitchExternalPower=ignore ... Then:
sudo systemctl restart systemd-logind sudo reboot This looks to have worked for my laptop. Reading further it looks like you can also add drop “drop-ins” in the logind.conf.d/ subdirectory.
Defaults can be restored by simply deleting this file and all drop-ins.
`,description:"",tags:["dell","Inspiron","k3s","kubernetes","linux","mint"],title:"Dell Inspiron 7537 - Preventing Linux shutdown when closing the lid",uri:"/posts/2023-06-01-dell-inspiron-7537-preventing-shutdown-on-lid-close/index.html"},{content:`This page contains information and simple code examples about some useful design patterns in Python
Singleton Pattern A Singleton pattern in python is a design pattern that allows you to create just one instance of a class, throughout the lifetime of a program. Using a singleton pattern has many benefits. A few of them are:
To limit concurrent access to a shared resource. To create a global point of access for a resource. To create just one instance of a class, throughout the lifetime of a program. Different ways to implement a Singleton: A singleton pattern can be implemented in three different ways. They are as follows:
Module-level Singleton Classic Singleton Borg Singleton For more information see Singleton Pattern in Python – A Complete Guide.
Classic Singleton class SingletonClass(object): def __new__(cls): if not hasattr(cls, 'instance'): cls.instance = super(SingletonClass, cls).__new__(cls) return cls.instance singleton = SingletonClass() new_singleton = SingletonClass() print(singleton is new_singleton) singleton.singl_variable = "Singleton Variable" print(new_singleton.singl_variable) `,description:"",tags:["python"],title:"Design patterns",uri:"/languages/python/design_patterns/index.html"},{content:"",description:"",tags:null,title:"directory",uri:"/tags/directory/index.html"},{content:`This page provides examples about the use of PowerShell path and directory commands.
Join-Path If you need to combine (directory) paths in PowerShell you can use Join-Path:
PS C:\\> Join-Path -Path "path" -ChildPath "childpath" path\\childpath For more information see the PowerShell doco
Remove-Item - Delete File(s) You can use the Remove-Item function to delete files:
# For a single file Remove-Item 'D:\\temp\\Test Folder\\test.txt' # Or a folder and sub-files/folders recursively with Remove-Item 'D:\\temp\\Test Folder' -Recurse Check if a path is a file or folder # Check if file (works with files with and without extension) Test-Path -Path 'C:\\Demo\\FileWithExtension.txt' -PathType Leaf Test-Path -Path 'C:\\Demo\\FileWithoutExtension' -PathType Leaf # Check if folder Test-Path -Path 'C:\\Demo' -PathType Container Get full path of executing script - $PSScriptRoot $PSScriptRoot is an automatic variable in PowerShell which contains the current file’s/module’s directory.
Current Directory Path $PWD (print working directory) in PowerShell gets current path of the current working directory to the standard output.
PS C:\\Dev_Apps\\projects\\GitHub\\development-cheat-sheets> $pwd Path ---- C:\\Dev_Apps\\projects\\GitHub\\development-cheat-sheets `,description:"",tags:["scripting","windows","powershell","terminal","utilities","file","directory"],title:"Directory commands",uri:"/languages/powershell/directories/index.html"},{content:'This page provides examples about the use of environment variables in PowerShell.\nEnvironmental Variables Environment variables are global settings for your Linux, Mac, or Windows computer, stored for the system shell to use when executing commands. For more information see HowTo: Set an Environment Variable in Windows - Command Line and Registry.\nPrint Environmental Variables To list all environment variables.\nGet-ChildItem Env: `` To print a particular environment variable: ```shell echo $Env:ProgramFiles C:\\Program Files Temporary variables You can set a temporary environmental variables using the following command:\nPS > $Env:FOO = "hello world" PS > $Env:FOO hello world YOu can Remove/Clear a temporary variable\nPS > $env:SLS_DEBUG = "" PS > $Env:SLS_DEBUG PS > User variables You can set user environmental variables using the following command:\nPS > setx GLOBAL_AGENT_HTTP_PROXY \'http://userId:password@proxy:8080\' PS > $Env:GLOBAL_AGENT_HTTP_PROXY http://userId:password@proxy:8080 ',description:"",tags:["scripting","windows","powershell","terminal","variables"],title:"Directory commands",uri:"/languages/powershell/env_variables/index.html"},{content:`Docker is a set of platform as a service products that use OS-level virtualization to deliver software in packages called containers. The service has both free and premium tiers. The software that hosts the containers is called the Docker Engine.
Note that Docker is one of many container runtimes that are available, other runtimes include rkt, containerd and podman.
Acronyms and Terms Term Explanation OCI Open Container Initiative Other Docker Pages Docker (run) commandsThis page contains examples about the use of Docker commands when running containers.
Docker buildThis page contains examples about the use of Docker commands when building containers.
`,description:"",tags:["container","linux","development","docker"],title:"Docker",uri:"/tools/docker/index.html"},{content:`This page contains examples about the use of Docker commands when running containers.
Remove container on exit Add the --rm flag to automatically remove the container when it exits.
docker run --rm hello-world Setting environmental variables You can set environmental variables in your docker container by using the syntax: -e "VARIABLE_NAME=value".
Just add this argument to your docker run command before you specify the image.
Exposing UDP ports To expose UDP ports when running a container add the /udp suffix to the port mapping.
docker run -p 80:80/tcp -p 80:80/udp my_app
The same goes for use within a Dockerfile.
# Expose the UI, and the UDP port (used for device discovery) EXPOSE 3000 50000/udp For more information see the Docker expose doco.
Copy files from container to host docker cp <container_id>:/path/filename.txt ~/Desktop/filename.txt
Networking Accessing the host from the container Within the container you can use the hostname host.docker.internal to refer to the machine running the container. Using host.docker.internal effectively equates to localhost on the host machine.
For more information see: How to connect to the Docker host from inside a Docker container?
Mode The docker host network topologies works on linux, and tends to be flaky in Windows. In host mode the port bindings are ignored as the container’s network is shared with the host.
For more information see the Docker Network doco.
Other handy Commands Format the response returned from Docker:
sudo docker container ls --format "\\{\\{.ID\\}\\} \\{\\{.Image\\}\\} \\{\\{.Names\\}\\}" sudo docker container inspect -f '{{ .NetworkSettings.IPAddress }}' mysql-basic diff Use the docker diff command to examine the differences in the container between the image and the new layer created by the container.
sudo docker container diff official-nginx Errors standard_init_linux.go:228: exec user process caused: exec format error That error usually means you’re trying to run a amd64 image on a non-amd64 host (such as 32-bit or ARM).
`,description:"",tags:["container","linux","development","cli","run"],title:"Docker (run) commands",uri:"/tools/docker/run/index.html"},{content:`This page contains examples about the use of Docker commands when building containers.
Building with Dockerfile docker build -f .\\Dockerfile -t [PRIVATE_IMAGE_REGISTRY[:PORT]/]<IMAGE_NAME>[:<VERSION_TAG>] . Labels The OCI Containers Specification defines several conventional labels that encapsulate common use cases for container images. These exist within the org.opencontainers.image namespace.
org.opencontainers.image.created – Image creation time. org.opencontainers.image.url – URL to get information about the image. org.opencontainers.image.version – Version of the main software inside the container (not the image’s version). org.opencontainers.image.licenses – Container licensing information. org.opencontainers.image.title – A human-readable name for the container.
To add these to your built image use the following syntax:
LABEL org.opencontainers.image.authors="awesome.developer@email.com" Environmental Variables (ENV) You can create, set and provide default values for environmental variables in a Dockerfile using ENV.
For example:
ENV MY_VARIABLE="defaultValue" # You can also define multiple environmental variables on the same line like: ENV VAR_1="Value_1" VAR_2="Value-2" Errors buildx build –push to private registry fails with ’tcp: lookup <registry_hostname> on <container_ip_address>: no such host’ error This happens when the build container is not able to resolve the <registry_hostname> to an IP address. When this happens you receive an error something like:
------ > exporting to image: ------ ERROR: failed to solve: failed to do request: Head "https://<registry_hostname>/v2/<image_name>/blobs/sha256:699020637dddc10710f773812992bbccd136879ea4680f4a3a819e95c6b65055": dial tcp: lookup <registry_hostname> on 192.168.65.5:53: no such host Even using the builder created with docker buildx create --driver docker-container --driver-opt network=host --use fails to resolve the IP address of the registry. For more details on the creation of a buildx builder see this page.
The workaround is to add the details of the private registry to the running build container’s /etc/hosts file:
Find the id if the running build container: docker ps Get a terminal in the container: docker exec -it <build_container_id> /bin/sh Add your private regisrty to the hosts file echo "192.168.0.xxx <your_private_registry_hostname>" >> /etc/hosts Done! This problem looks to be related to: Docker Buildx issue 1461.
`,description:"",tags:["container","linux","development","cli","image","dockerfile"],title:"Docker build",uri:"/tools/docker/build/index.html"},{content:"",description:"",tags:null,title:"dockerfile",uri:"/tags/dockerfile/index.html"},{content:"",description:"",tags:null,title:"encrypt",uri:"/tags/encrypt/index.html"},{content:"",description:"",tags:null,title:"environment",uri:"/tags/environment/index.html"},{content:`This page provides information about the use of environmental variables in linux.
Environment variables are variables that contain values necessary to set up a shell environment. Contrary to shell variables, environment variables persist in the shell’s child processes.
Set an Environment Variable How to link
How to Check Environment Variables View All Environment Variables Use the printenv command to view all environment variables. Since there are many variables on the list, use the less command to control the view:
printenv | less Set a Shell Variable in Linux The simplest way to set a variable using the command line is to type its name followed by a value:
[VARIABLE_NAME]=[variable_value] Note: variables created in this way is a shell variable. It will not persist or be passed to child processes.
How to Export an Environment Variable If you want to turn a shell variable into an environment variable, return to the parent shell and export it with the export command:
export [VARIABLE_NAME] The environment variable created in this way disappears after you exit the current shell session.
Set an Environment Variable in Linux Permanently If you wish a variable to persist after you close the shell session, you need to set it as an environmental variable permanently. You can choose between setting it for the current user or all users.
To set permanent environment variables for a single user edit the .bashrc file sudo nano ~/.bashrc Write a line for each variable you wish to add using the following syntax: export [VARIABLE_NAME]=[variable_value] Save and exit the file. The changes are applied after you restart the shell. If you want to apply the changes during the current session, use the source command: source ~/.bashrc To set permanent environment variables for all users create an .sh file in the /etc/profile.d folder sudo nano /etc/profile.d/[filename].sh Write a line for each variable you wish to add using the following syntax: export [VARIABLE_NAME]=[variable_value] Save and exit the file. The changes are applied at the next login. `,description:"",tags:["operating_system","linux","variables","environment"],title:"Environmental Variables (Linux)",uri:"/operating-systems/linux/environmental-variables/index.html"},{content:"",description:"",tags:null,title:"extensions",uri:"/tags/extensions/index.html"},{content:"",description:"",tags:null,title:"fedora",uri:"/tags/fedora/index.html"},{content:"",description:"",tags:null,title:"file",uri:"/tags/file/index.html"},{content:`This page contains information and simple code examples about using Python to interact with the file system.
File exists() The OS module in Python lets us interact with the operating system. This comes under Python’s standard utility modules and gives a portable way to use the dependent functions of the operating system. The exists() function in Python exists in the os.path module, which is a submodule of the python’s OS module and is used to check if a particular file exists or not.
Syntax from os.path import exists file_exists = exists(path_to_file) Delete a file To delete a file, you must import the OS module, and run its os.remove() function:
Simple Example Remove the file “demofile.txt”:
import os os.remove("demofile.txt") Check if File exist To avoid getting an error, you might want to check if the file exists before you try to delete it:
Example Check if file exists, then delete it:
import os if os.path.exists("demofile.txt"): os.remove("demofile.txt") else: print("The file does not exist") Create a directory os.mkdir() method in Python is used to create a directory named path with the specified numeric mode. This method raise FileExistsError if the directory to be created already exists.
# importing os module import os # Directory to create directory = "test" # Parent Directory path parent_dir = "D:/projects/" # Path path = os.path.join(parent_dir, directory) # Create the directory # 'test' in # 'D:/projects/' os.mkdir(path) # unless a mode is supplied the file permissions will default to mode = 0o777 print("Directory '% s' created" % directory) Copy a file The shutil.copy() method in Python is used to copy the files or directories from the source to the destination. The source must represent the file, and the destination may be a file or directory.
import shutil import os src = r'C:\\Work\\office\\employee.txt' dst = r'C:\\Work\\employee.txt' shutil.copyfile(src, dst) `,description:"",tags:["python"],title:"File operations",uri:"/languages/python/file_operations/index.html"},{content:"",description:"",tags:null,title:"files",uri:"/tags/files/index.html"},{content:`File and file permissions are core to the security model used by Linux systems.
They determine who can access files and directories on a system and how.
Renaming a file Use the mv command to rename a file, for example:
mv oldname.txt newname.txt Changing file permissions To change the permissions for a file/folder use chmod [permission] [file_name], for example:
chmod 0777 public_file Explaining file permissions Every file and folder contains 8-bit data that controls the permissions. In its basic binary form, 000 means that no permissions of any form are granted.
When you set a “Read” permission, it adds 4-bit to the data, making it “100” (in binary format) or a “4” in the usual decimal format. Setting a “Write” permission will add 2-bit to the data, making it “010” and “2” in decimal form. Lastly, setting an “Execute” permission adds 1-bit to the data, which will result in “001,” or “1” in decimal form. In short:
Read is equivalent to “4.” Write is equivalent to “2.” Execute is equivalent to “1.”* In a nutshell, setting permissions is basic math. For example, to set “Read and Write” permissions, we combine 4 and 2 to get 6. Of course, there are other permutations:
0: No permission
1: Execute
2: Write
3: Write and Execute
4: Read
5: Read and Execute
6: Read and Write
7: Read, Write, and Execute
A complete set of file permissions assigns the first digit to the Owner, the second digit to the Group, and the third to Others. Here are some of the commonly used permissions:
755 This set of permissions is commonly used by web servers. The owner has all the permissions to read, write and execute. Everyone else can read and execute but cannot make changes to the file. 644 Only the owner can read and write. Everyone else can only read. No one can execute this file. 655 Only the owner can read and write and cannot execute the file. Everyone else can read and execute and cannot modify the file. As for 777, this means every user can Read, Write, and Execute. Because it grants full permissions, it should be used with care. However, in some cases, you’ll need to set the 777 permissions before you can upload any file to the server. For more information on 0777
`,description:"",tags:["sh","bash","linux","file","permissions"],title:"Files",uri:"/operating-systems/linux/files/index.html"},{content:"",description:"",tags:null,title:"frameworks",uri:"/tags/frameworks/index.html"},{content:"",description:"",tags:null,title:"games",uri:"/tags/games/index.html"},{content:"",description:"",tags:null,title:"gaming",uri:"/tags/gaming/index.html"},{content:"",description:"",tags:null,title:"git",uri:"/tags/git/index.html"},{content:`Git is a distributed version-control system for tracking changes in source code during software development. It is designed for coordinating work among programmers, but it can be used to track changes in any set of files. Its goals include speed, data integrity, and support for distributed, non-linear workflows.
Add a git configuration To add/alter a git configuration setting use the following command:
# git config [options] <configuration.item> <value> git config --global http.sslverify true You can make the configuration change global by using the --global option.
Shallow clone Use git clone --depth=1 <url> to do a shallow clone. These clones truncate the commit history to reduce the clone size. This creates some unexpected behavior issues, limiting which Git commands are possible. These clones also put undue stress on later fetches, so they are strongly discouraged for developer use however they are helpful for some build environments where the repository will be deleted after a single build.
See this post for more information.
Showing progress git clone --progress --verbose .....
Add a remote To add a remote for the given git project use:
# git remote add [-t <branch>] [-m <master>] [-f] [--[no-]tags] [--mirror=(fetch|push)] <name> <url> git remote add gogs https://gogs.example.org/repoGroup/project.git Changing upstream for a branch To change the default upstream that a particular branch is set to:
# git push -u <remote_name> <local_branch_name> git push -u gogs main Submodules Populate (pull) contents of submodule If you have cloned a git repo without using the --recurse-submodules you will need to run some addition commands to pull in the contents of the submodules.
git submodule init git submodule update Handy Commands Command Description git remote -v Prints the remotes for this git repo git branch -vv Prints the remotes associated with each branch git commit --amend --reset-author Reset the author information on your last commit git config http.sslverify false Set the current project to disable ssl certificate checks, handy for self hosted repos Troubleshooting git checkout/pull returns ’error: invalid path' This is caused by git complaining about invalid paths, typically encountered on Windows.
The workaround is to set the git config flag: git config core.protectNTFS false
Note: core.protectNTFS If set to true, do not allow checkout of paths that would cause problems with the NTFS filesystem, e.g. conflict with 8.3 "short" names. Defaults to true on Windows, and false elsewhere. `,description:"",tags:["git","version_control"],title:"git",uri:"/tools/git/index.html"},{content:`Github is a popular public Source Code Repository (SRC) which was recently purchased by Microsoft?. As the name suggests Github uses git for use of the repository.
Token authentication Sometime in 2021? Github stated to enforce the use of tokens for CLI and API calls, see here for more info.
What this means is that you can no longer use a simple git command line client and username/passwords to pull/push to git, you need to setup personal access tokens, a credential manager or use another application to interact with Github.
Setup for Raspberry Pi’s Given the above authentication restrictions, one method of interacting with Github on a Raspberry Pi is to install the Github cli, then authenticate with Github using gh auth login.
Following that you can then interact with GitHub using your standard git cli client.
See Caching your GitHub credentials in Git for more information.
Troubleshooting Error “Your push would publish a private email address” I found this solution on StackOverFlow.
Find your GitHub noreply address in your GitHub’s Personal Settings → Emails. It’s mentioned in the description of the Keep my email address private checkbox. Usually, it starts with a unique identifier, plus your username.
Set your email address for the single repository
git config user.email "{ID}+{username}@users.noreply.github.com" (Optional) Set your email address for every repository on your computer git config --global user.email "{ID}+{username}@users.noreply.github.com" Reset the author information on your last commit git commit --amend --reset-author `,description:"",tags:["git","github","development"],title:"Github",uri:"/tools/github/index.html"},{content:"",description:"",tags:null,title:"go",uri:"/tags/go/index.html"},{content:`Go is a statically typed, compiled high-level programming language designed at Google.
It is syntactically similar to C, but also has memory safety, garbage collection, structural typing, and CSP-style concurrency.
Installation (Windows) There are a number of installation methods for windows.
My preference is to download the .msi file from the Go releases page and install.
Should I commit my ‘go.sum’ file as well as my ‘go.mod’ file? Yes. Ensure your go.sum file is committed along with your go.mod file. See the github FAQ below for more details and rationale.
From the FAQ:
Should I commit my ‘go.sum’ file as well as my ‘go.mod’ file? Typically, your module’s go.sum file should be committed along with your go.mod file.
go.sum contains the expected cryptographic checksums of the content of specific module versions. If someone clones your repository and downloads your dependencies using the go command, they will receive an error if there is any mismatch between their downloaded copies of your dependencies and the corresponding entries in your go.sum. In addition, go mod verify checks that the on-disk cached copies of module downloads still match the entries in go.sum. Note that go.sum is not a lock file as used in some alternative dependency management systems. (go.mod provides enough information for reproducible builds). `,description:"",tags:["go","google"],title:"Go",uri:"/languages/go/index.html"},{content:"",description:"",tags:null,title:"gogs",uri:"/tags/gogs/index.html"},{content:"",description:"",tags:null,title:"google",uri:"/tags/google/index.html"},{content:"",description:"",tags:null,title:"grep",uri:"/tags/grep/index.html"},{content:`grep is a command-line utility for searching plain-text data sets for lines that match a regular expression.
Its name comes from the ed command g/re/p, which has the same effect. grep was originally developed for the Unix operating system, but later available for all Unix-like systems and some others such as OS-9.
OR operator There are 2 ways that you can do OR operations with grep.
You can use \\| to separate multiple patterns for example: grep 'pattern1\\|pattern2' filename Alternately you can use grep with the -E (extended regexp mode) for example: grep -E 'pattern1|pattern2' `,description:"",tags:["unix","linux"],title:"grep",uri:"/operating-systems/linux/tools/grep/index.html"},{content:"",description:"",tags:null,title:"h2",uri:"/tags/h2/index.html"},{content:`H2 is a lightweight Java SQL database.
The main features of H2 are:
Very fast, open source, JDBC API Embedded and server modes; in-memory databases Browser based Console application Small footprint: around 2.5 MB jar file size Settings of the H2 Console The settings of the H2 Console are stored in a configuration file called .h2.server.properties in you user home directory. For Windows installations, the user home directory is usually C:\\Users\\[username]. The configuration file contains the settings of the application and is automatically created when the H2 Console is first started
For more information see: H2 Documentation
`,description:"",tags:["h2","database","sql","development"],title:"H2 Database",uri:"/tools/h2_database/index.html"},{content:`This page captures a technique I developed to work around the issues you can encounter when using Persistent Volumes (PVs) that contain SQLite databases used by containers in Kubernetes.
Situation For containers within my Home Lab Kubernetes (k3s) that needed a persistent state I have been using a pattern where I would create a PV (and PVC) to be able to store these files. The PV is mapped to a NFS share on my Synology NAS.
Most of the time this pattern works without any issues, albeit it can sometimes be a little slow if the application is frequently accessing the files stored in the PV/NAS.
However recently I encountered a locked database error with my Mealie instance running in my home cluster.
Issue After some investigation and some googling I came across this comment in github.
So for my case the latency of the calls to the SQLite database file(s) stored on my NAS was causing the application to report the database as locked. Apparently a known limitation of using SQLite on network drives.
My file based hacked solution So ideally in this type of situation you would move to using a database that is better suited to handling network traffic, like my Gogs DB migration. But in this instance I wanted to try a container based solution still using SQLite.
For most small scale situations SQLite performs great if the database file(s) are stored on a drive on the same host where the application resides.
Given this I came up with this hacky solution:
Copy the needed files from the NAS to an emptyDir volume (which is really a directory on the host) Expose the volume to the application that needs access to SQLite Monitor the files (using inotifywait), and when a file change is detected Copy the modified files back to their original location on the NAS using rsync For my Mealie deployment I use a Helm chart that I created so to implement the hacky solution I could just amend my custom helm chart and redeploy it as needed. The code snippets below have been extracted from my custom helm chart.
Volumes This solution relies heavily on the use of Kubernetes volumes to copy, share and backup the required files.
volumes: - name: mealie-data persistentVolumeClaim: claimName: {{ .Values.volumes.claimName }} - name: mealie-working emptyDir: {} - name: backup-script configMap: name: {{ include "mealie.fullname" . }}-backup-script # Make the script executable defaultMode: 0777 Copying the files to the emptyDir We use busybox to copy the files from the PVC to the emptyDir, which actually resides on the machine hosting the pod.
initContainers: - name: init image: "busybox:1.36" imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /source name: mealie-data - mountPath: /target name: mealie-working command: ['sh', '-c', 'echo "The init container is running!" && cp -r /source/* /target/'] Map the emptyDir to the application containers: - name: {{ .Chart.Name }} securityContext: {{- toYaml .Values.securityContext | nindent 12 }} image: "{{ .Values.mealie.repository }}:{{ .Values.mealie.tag | default .Chart.AppVersion }}" imagePullPolicy: {{ .Values.mealie.pullPolicy }} ports: - name: http containerPort: {{ .Values.service.port }} protocol: TCP volumeMounts: # Map the emptyDir to the application (which now contains the files from the PVC) - mountPath: /app/data name: mealie-working Create the backup image This solution uses inotifywait to watch for file changes and rsync to copy the changes back to the source PVC. I couldn’t find a trusted lightweight container which had these tools, so I decided to create my own using the Dockerfile below.
# # To build: # docker build -f .\\Dockerfile -t [IMAGE_REGISTRY:PORT/]proof-of-concept/alpine-inotify . # To publish: # docker push [IMAGE_REGISTRY:PORT/]proof-of-concept/alpine-inotify # FROM alpine:3.18.4 RUN /bin/sh -c set -eux; apk update; apk add inotify-tools rsync ; rm -rf /var/cache/apk/* Backup script To make the solution a bit more portable I decided to use an external script rather than embed the script within the proof-of-concept/alpine-inotify image.
I embedded the script within a Kubernetes ConfigMap so that it formed part of the Kubernetes objects that were created when my custom helm chart was deployed.
# # Descriptions: Contains the backup script to watch for file changes in the /source directory # and run rsync to mirror the changes to the /target directory. # --- apiVersion: v1 kind: ConfigMap metadata: name: {{ include "mealie.fullname" . }}-backup-script labels: {{- include "mealie.labels" . | nindent 4 }} data: # Backup script backup.sh: |- #!/bin/bash -x echo "Waiting for {{ .Values.backup.startUpWaitSeconds }} seconds for application to startup..." sleep {{ .Values.backup.startUpWaitSeconds }}s echo "Backup script running..." while true; do echo "" echo "Waiting for new file changes..." echo "" FILE_CHANGES=$(inotifywait -r -e modify -e create -e delete -e move -q --format '%e %w%f' /source/ ) echo $(echo $FILE_CHANGES | awk '{ print "Detected " $1 " event(s) on file: " $2 " at: "}') $(date) echo "" # Sleep a little to let file changes settle/finish sleep {{ .Values.backup.sleepValue }}{{ .Values.backup.sleepUnits }} # Backup echo "Backing up files..." # -a = -rlptgoD, p = set permissions, g = set group on destination, o = preserve owner rsync -rltDvz --del --force --ignore-errors /source/ /backup/ done Backup container Lastly we mapped the script within the ConfigMap into the backup container and called it.
- name: backup image: "proof-of-concept/alpine-inotify:latest" imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /source name: mealie-working - mountPath: /backup name: mealie-data - mountPath: /backup.sh name: backup-script subPath: backup.sh command: [ 'sh', './backup.sh' ] Reflection on solution I will admit this solution is a bit hacky as it relies upon the rsync file copy working correctly and not corrupting the SQLite database or any of the other files needed by the application. But given this is just for my own use and I have backups of the Mealie data I am not too worried if that happens.
Getting this solution to work correctly took sometime, but did allow me to play with Alpine linux and some linux tools that I had not used before. A good learning experience!
`,description:"",tags:["db","sqlite","kubernetes","mealie","inotifywait","rsync"],title:"Hack for SQLite use on Kubernetes",uri:"/posts/2023-10-22-kubernetes-sqlite-hack/index.html"},{content:"",description:"",tags:null,title:"hardware",uri:"/tags/hardware/index.html"},{content:`This section contains pages and links to some of the hardware that I have come across.
Pages Broadlink (Black Bean) DevicesThe Broadlink (RM Mini) black bean is a small universal Infra Red (IR) remote control which connects to and receives commands over WiFi.
Raspberry PiThe Raspberry Pi is a series of small single-board computers (SBCs) developed in the United Kingdom by the Raspberry Pi Foundation in association with Broadcom.
`,description:"",tags:null,title:"Hardware",uri:"/hardware/index.html"},{content:"",description:"",tags:null,title:"head",uri:"/tags/head/index.html"},{content:"",description:"",tags:null,title:"health",uri:"/tags/health/index.html"},{content:"",description:"",tags:null,title:"helm",uri:"/tags/helm/index.html"},{content:`Helm helps you manage Kubernetes applications — Helm Charts help you define, install, and upgrade even the most complex Kubernetes application.
Handy Commands Command Description helm version Print the client version information helm ls List the deployed helm charts helm create <chart_name> Create a helm chart helm upgrade -i my <deployment_name> .\\<folder>\\ Upgrade, or install, the helm chart from folder into the cluster with the name deployment_name helm get values <RELEASE_NAME> -a Download all od the computed values for a given release helm uninstall <deployment_name> Remove/uninstall the deployment_name from the cluster Helm Charts Flow control See: https://helm.sh/docs/chart_template_guide/control_structures/
If/Else Example:
volumes: - name: influxdb-data {{- if not .Values.volumes.influxdb.claimName }} emptyDir: {} {{- else }} persistentVolumeClaim: claimName: {{ .Values.volumes.influxdb.claimName }} {{- end }} `,description:"",tags:["container","linux","development","kubernetes","helm","yaml"],title:"Helm",uri:"/tools/kubernetes/helm/index.html"},{content:`Eloquent Javascript A free online book which is updated regularly available at: eloquentjavascript.net
You Don’t Know JS (yet) A series of short helpful books on Javascript available at: github.com/getify/You-Dont-Know-JS
Mozilla Developer Network One of the finest online Javascript resources https://developers.mozilla.org/en-US/docs/Web/JavaScript/Reference
`,description:"",tags:["javascript"],title:"Helpful resources",uri:"/languages/javascript/resources/index.html"},{content:`Hoverfly is a tool written in go which can be used to simulate API.
Hoverfly is distributed as a Docker container and can also be used via a native Java (and JUnit) library.
Handy Links Baeldung - Introduction to Hoverfly in Java `,description:"",tags:["container","linux","development","mocking","simulations"],title:"Hoverfly",uri:"/tools/hoverfly/index.html"},{content:"",description:"",tags:null,title:"html",uri:"/tags/html/index.html"},{content:`The HyperText Markup Language, or HTML is the standard markup language for documents designed to be displayed in a web browser.
It can be assisted by technologies such as Cascading Style Sheets (CSS) and scripting languages such as JavaScript.
`,description:"",tags:["html"],title:"HTML",uri:"/languages/html/index.html"},{content:"",description:"",tags:null,title:"http",uri:"/tags/http/index.html"},{content:"",description:"",tags:null,title:"httpd",uri:"/tags/httpd/index.html"},{content:`Hugo is a very fast Static Site Generator (SSG) built with Go.
Installation (Windows) Download the binaries from the Hugo release page, typically you will want the hugo_extended_<VERSION>_windows-amd64.zip file.
Unzip the folder to somewhere on your computer, then add the directory to your PATH.
You should then be able to get the version using the console command:
hugo version # Returns hugo v0.120.2-9c2b2414d231ec1bdaf3e3a030bf148a45c7aa17+extended windows/amd64 BuildDate=2023-10-31T16:27:18Z VendorInfo=gohugoio Running Hugo (locally) From the root directory of the project run:
hugo serve -D --disableFastRender Using Hugo in Docker WARNING: Changes to the pages will not be detected when running Hugo in Docker on a Windows host.
Do to a limitation with Docker on Windows which prevents Windows file systems changes from propagating into the linux file system within the container, Hugo will be unable to detect file systems changes and rebuild the site.
Hugo publishes a docker image. Using it is easy, you just need to make sure to:
add the volume where the Hugo site files open a port to be able to view the site To serve the site using docker run:
docker run --rm -it --name hugo \` -v "$($pwd)\\quickstart:/src" \` -p 1313:1313 \` [$DOCKER_REG/]klakegg/hugo:0.101.0-ext server -D Hosting on Github pages See this hugo page for information.
Handy Commands Command Description hugo Builds/compiles the website into the ./public folder hugo serve -D --disableFastRender Serves the Hugo site, watching for changes and rebuilding as required hugo new posts/2023-06-17-new-blog-post.md Creates a new blog post using the appropriate archetype `,description:"",tags:["go","github_pages","hugo"],title:"Hugo",uri:"/tools/hugo/index.html"},{content:"",description:"",tags:null,title:"image",uri:"/tags/image/index.html"},{content:"",description:"",tags:null,title:"image_generation",uri:"/tags/image_generation/index.html"},{content:`This page contains information, syntax, and simple code examples, about the use of the Immutables library.
Java annotation processors to generate simple, safe and consistent value objects. Do not repeat yourself, try Immutables, the most comprehensive tool in this field!
`,description:"",tags:["java"],title:"Immutables",uri:"/languages/java/immutables/index.html"},{content:`This page contains information, and simple code examples, about the use of the JavaScript imports.
Introducing Imports JavaScript Imports come in several flavours (read syntaxes) that have been introduced as the language has evolved (ES5 -> ES6 etc.). The different syntaxes also provide different functionality.
ES6 Imports In ES6 the import syntax is a declarative import syntax, it does not execute any functions. So if you want to call a function you’ll need two lines:
import debugModule from 'debug'; const debug = debugModule('http'); `,description:"",tags:["javascript"],title:"Imports",uri:"/languages/javascript/imports/index.html"},{content:"",description:"",tags:null,title:"infra_red",uri:"/tags/infra_red/index.html"},{content:"",description:"",tags:null,title:"ingress",uri:"/tags/ingress/index.html"},{content:"",description:"",tags:null,title:"inotifywait",uri:"/tags/inotifywait/index.html"},{content:`inotifywait a linux tool that waits for changes to files using inotify
Watch Below is a single line command which will print out the details of the first file for the give events.
inotifywait -r -e modify -e create -e delete -e move -q --format '%e %w%f' /source/ \\ | awk '{ print "Detected " $1 " event(s) on file: " $2 }') `,description:"",tags:["unix","linux","files"],title:"inotifywait",uri:"/operating-systems/linux/tools/inotifywait/index.html"},{content:"",description:"",tags:null,title:"Inspiron",uri:"/tags/inspiron/index.html"},{content:`IntelliJ IDEA is an integrated development environment written in Java for developing computer software. It is developed by JetBrains, and is available as an Apache 2 Licensed community edition, and in a proprietary commercial edition. Both can be used for commercial development.
Plugins Docker Connecting to docker engine on another pc
`,description:"",tags:["development","java"],title:"IntelliJ",uri:"/tools/intellij/index.html"},{content:`This page captures some the interesting tech that I have heard about, but have not had a chance to investigate/use.
Tools Containerisation & Kubernetes Tekton - A Kubernetes-native open-source framework for building continuous integration and delivery (CI/CD) pipelines to build, test, and deploy software. Skaffold - a command line tool that facilitates continuous development for Kubernetes applications. Buildpacks - create application images without the need for Dockerfiles Repository managers Reposilite - Lightweight and easy-to-use repository manager for Maven based artifacts in JVM ecosystem. Gaming Steam on Raspberry Pi `,description:"",tags:["tools"],title:"Interesting...",uri:"/to_research/index.html"},{content:`Inventories organize managed nodes in centralized files that provide Ansible with system information and network locations.
Using an inventory file, Ansible can manage a large number of hosts with a single command.
To communicate with the hosts you must also ensure that your public SSH key is added to the authorized_keys file on each host.
Make sure you have:
Got a ssh key (ls -la ~/.ssh/id_rsa) or generated one with ssh-keygen. Added it to the host with ssh-copy-id <USER_ID_ON_TARGET>@<HOSTNAME> Inventory file The inventory file can be a ini or yaml file and can list the hosts in logical groups.
Connecting to the host with a different username You can use the behavioral inventory parameter ansible_user to connect to the host with a different username. For example:
my_hosts: hosts: my_r_pi: ansible_user: pi And then used with the command:
$ ansible my_hosts -m ping -i inventory.yaml my_r_pi | SUCCESS => { "ansible_facts": { "discovered_interpreter_python": "/usr/bin/python3" }, "changed": false, "ping": "pong" } Host and Group variables You can use variables within the main inventory file, however storing them separately can help with organising your variables.
Variable files must use YAML syntax. Valid file extensions include ‘.yml’, ‘.yaml’, ‘.json’, or no file extension.
Ansible loads host and group variable files by searching paths relative to the inventory file or the playbook file.
You can also create directories named after your groups or hosts. Ansible will read all the files in these directories in lexicographical order.
Variables are merged in the following order/ precedence is (from lowest to highest):
all group (because it is the ‘parent’ of all other groups) parent group child group host Vaulted variables You can combine the use of variables with vaulted variables as the example used in the Keep vaulted variables safely visible section of the ansible documentation.
So the folder structure might be:
inventory.yaml - uses the variable "{{ my_password }}" group_vars/ all/ - applies to all groups vars.yaml - contains my_password: "{{ vault_my_password }}" vault.yaml - contains vault_my_password: !vault | <ENCRYPTED_VARIABLE_DETAILS> `,description:"",tags:["ansible","automation"],title:"Inventory",uri:"/tools/ansible/inventory/index.html"},{content:"",description:"",tags:null,title:"ip_address",uri:"/tags/ip_address/index.html"},{content:"",description:"",tags:null,title:"java",uri:"/tags/java/index.html"},{content:`Java is a general-purpose programming language that is class-based, object-oriented, and designed to have as few implementation dependencies as possible. It is intended to let application developers write once, run anywhere, meaning that compiled Java code can run on all platforms that support Java without the need for recompilation
Java Virtual Machine (JVM) options There is a bunch of options you can add to the command line when run Java which can be very handy. Below is some of the more common or handy option which I have come across, or have a look at this handy cheat sheet for more.
Xms and Xmx (Memory allocation) The flag Xmx specifies the maximum memory allocation pool for a Java Virtual Machine (JVM), while Xms specifies the initial memory allocation pool.
This means that your JVM will be started with Xms amount of memory and will be able to use a maximum of Xmx amount of memory. For example, starting a JVM like below will start it with 256 MB of memory and will allow the process to use up to 2048 MB of memory:
java -Xms256m -Xmx2048m The memory flag can also be specified in different sizes, such as kilobytes, megabytes, and so on.
-Xmx1024k -Xmx512m -Xmx8g -Dproperty (System properties) You can set a Java system property by using the -D<property> syntax, for example:
# Sets the 'name' system property to 'HelloWorld' -Dname=HelloWorld # The property can be retrieved at runtime using System.getProperty("name") //HelloWorld Sub Pages (Apache) Tomcat AssertJ Class Collections Concurrency Immutables JavaFX JPA NIO2 Record Spring Boot `,description:"",tags:["java","jvm","properties"],title:"Java",uri:"/languages/java/index.html"},{content:`JavaFX is a software platform for creating and delivering desktop applications, as well as rich Internet applications (RIAs) that can run across a wide variety of devices. JavaFX is intended to replace Swing as the standard GUI library for Java SE.
With the release of JDK 11 in 2018, Oracle has made JavaFX part of the OpenJDK under the OpenJFX project.
OpenJFX is a collaborative effort by many individuals and companies with the goal of producing a modern, efficient, and fully featured toolkit for developing rich client applications.
Input Events Key typed vs Key pressed and key released “Key typed” events are higher-level and generally do not depend on the platform or keyboard layout. They are generated when a Unicode character is entered, and are the preferred way to find out about character input. In the simplest case, a key typed event is produced by a single key press (e.g., ‘a’). Often, however, characters are produced by series of key presses (e.g., SHIFT + ‘a’).
“Key pressed” and “key released” events are lower-level and depend on the platform and keyboard layout.
For more information see the Key Event JavaDoc.
Binding Simple primitive Properties When using Simple primitive properties such as SimpleIntegerProperty you will find that there is no implementation of StringConverter that works with these.
If you are doing a one way (read only) binding then you can use \`Bindings.convert(ObservableValue observableValue).
Example:
private final SimpleIntegerProperty indicesLength = new SimpleIntegerProperty(); private Label indiciesCountLabel = new Label(); indiciesCountLabel.textProperty().bind(Bindings.convert(indicesLength)); indicesLength.set(23); ListView Changing item display text @FXML private ListView<Index> indiciesListView; @FXML public void initialize() { // Setup bindings indiciesListView.setCellFactory(param -> new IndexListCell()); } /** * Format the display of the Index list */ private static class IndexListCell extends ListCell<Index> { @Override public void updateItem(Index index, boolean empty) { super.updateItem(index, empty); if (index != null) { setText(String.format("%s - %s", index.indexCode(), index.fullName())); } } } TableView (Re)Applying Sorting after an update If you change the underlying ObservableList, then you can reapply the (column) sorting that the user may have done by calling tableView.sort().
A more complete example can be seen below:
fxBeans.clear(); fxBeans.addAll(beans.stream() .map( bean -> new TableRowFxBean(bean)).collect(Collectors.toList())); // Restore Sorting, if applied tableView.sort(); `,description:"",tags:["java"],title:"JavaFX",uri:"/languages/java/javafx/index.html"},{content:"",description:"",tags:null,title:"javascript",uri:"/tags/javascript/index.html"},{content:`Javascript is a modern scripting language that is used in many responsive web applications.
Javascript is:
Loosely Typed Object-orientated Sub Pages Arrays Basic Syntax Classes Helpful resources Imports Jest Libraries Node.js React Tools UI Components Versions and Transpilers `,description:"",tags:["javascript"],title:"JavaScript",uri:"/languages/javascript/index.html"},{content:"",description:"",tags:null,title:"jaxb",uri:"/tags/jaxb/index.html"},{content:`Jekyll is a Ruby tool which can generate static websites and blogs using plain text files.
Handy Commands Command Description bundle exec jekyll build Build the Jekyll site bundle exec jekyll build --profile Build the site and profile the build bundle exec jekyll serve Run the Jekyll site bundle update github-pages Update a plugin (example github-pages) Configuration ( _config.yml) baseurl Use baseurl when you are building a site that doesn’t sit at the root of the domain. For example:
‘http://jekyll.github.io/example'
In this case you would need to set baseurl: /example in the _config.yml file otherwise the automatically generated links will not include the /example in the url path.
For more info see Clearing Up Confusion Around baseurl – Again
Links (to other site pages) Links to other pages in Jekyll follow the basic markdown format, but with the use of a little liquid to insert the url details.
If you do not use the baseurl configuration mentioned above, then use:
[Link to a document]({% link _tools/jekyll.md %}) If your site makes use of the baseurl configuration you need to prepend this to your link like this:
[JavaScript]({{site.baseurl}}{% link _languages/javascript.md %}) `,description:"",tags:["jekyll","ruby","github_pages"],title:"Jekyll",uri:"/tools/jekyll/index.html"},{content:"",description:"",tags:["ci_cd","development"],title:"Jenkins",uri:"/tools/jenkins/index.html"},{content:`Jest is a JavaScript Testing Framework with a focus on simplicity. It is compatible with many frameworks (including node) and TypeScript.
Mocks You can get Jest to create an automatic mock for a module simply by adding jest.mock('./moduleToMock') to your test module/file.
With automatic mocks Jest replaces the ES6 class with a mock constructor, and replaces all of its methods with mock functions that always return undefined.
See the Jest mocks page for more details.
Manual Mocks Manual mocks are used to stub out functionality with mock data. For example, instead of accessing a remote resource like a website or a database, you might want to create a manual mock that allows you to use fake data.
For more details see the Jest manual mock page.
Mock Folder Structure . ├── config ├── __mocks__ │ └── fs.js (For mocking node modules) ├── models │ ├── __mocks__ │ │ └── user.js │ └── user.js ├── node_modules └── views `,description:"",tags:["javascript","testing","frameworks"],title:"Jest",uri:"/languages/javascript/jest/index.html"},{content:`Java Persistence API (JPA) This page contains information, syntax, and simple code examples, about the use of the Java Persistence API (JPA).
Case-insensitive like matching To make your query case-insensitive, convert both your keyword and the compared field to lower case:
query.where( builder.or( builder.like( builder.lower( root.get( type.getDeclaredSingularAttribute("username", String.class) ) ), "%" + keyword.toLowerCase() + "%" ) ); ''' `,description:"",tags:["java"],title:"JPA",uri:"/languages/java/jpa/index.html"},{content:"",description:"",tags:null,title:"jvm",uri:"/tags/jvm/index.html"},{content:"",description:"",tags:null,title:"k3s",uri:"/tags/k3s/index.html"},{content:`K3s - Lightweight Kubernetes. Easy to install, half the memory, all in a binary of less than 100 MB.
Cluster Access Accessing the Cluster from Outside with kubectl Copy /etc/rancher/k3s/k3s.yaml on your machine located outside the cluster as ~/.kube/config. Then replace the value of the server field with the IP or name of your K3s server. kubectl can now manage your K3s cluster.
Certificates By default, certificates in K3s expire in 12 months. If the certificates are expired or have fewer than 90 days remaining before they expire, the certificates are rotated when K3s is restarted.
Note: If the certificates have expired you will not be able to access the cluster with \`kubeclt\` For more see Certificate Rotation.
Labeling Nodes sudo k3s kubectl label node pi-server arch=arm64 To view the current labels for a node use the following command:
sudo k3s kubectl label --list node <NODE_NAME> # For example sudo k3s kubectl label --list node ht-pc Enable the use of NFS PV If you want to use NFS backed Persistent Volumes (PV) you will need to install nfs-common on the k3s agent/node.
sudo apt-get update && sudo apt-get install -y nfs-common Changing the k3s service When installed as a service, the service file is located in: /etc/systemd/system/k3s.service
You can alter the ExecStart= entry if you need to change the startup arguments.
Just remember to reload the systemctl daemon and restart the service so that the settings are applied:
sudo systemctl daemon-reload sudo systemctl restart k3s # Check the status sudo systemctl status k3s Changing IP address of the server On the worker nodes edit the sudo nano /etc/systemd/system/k3s-agent.service.env file to change the IP address of the server.
`,description:"",tags:["container","linux","development","kubernetes","k3s"],title:"K3s",uri:"/tools/kubernetes/k3s/index.html"},{content:`A project to set up a small scale multi worker Kubernetes instance at home (sometimes called a ‘home lab’).
Why There are two main drivers for this project:
Enable me to host resilient services/containers (using the hardware I have lying around) Learn more about Kubernetes Why not Docker (or Docker Swarm) It appears that some of more advanced features (such as self healing containers, security, network storage) is a little beyond the capabilities of Docker (swarm).
Kubernetes also seems to be more of an industry standard and (similar technologies) are used by most cloud providers.
Lastly for some of the more advance features to work in Docker I needed to run it as root which is a security concern.
Containers Services to host Artifactory (or SonarNexus) CI/CD Pipeline (Jenkins) 3Personal Projects: DS Photo service/site Others….? Steps Install OS I am using a 8GB Raspberry Pi 4 as the host for the k3s server.
First I installed the latest version of the Raspberry Pi 64-bit OS lite using the Raspberry Pi Imager. Using the advanced options in the imager I was able to set the hostname, password, and enable SSH as the Pi is running headless in my closet.
The Raspberry OS I installed was based on Debian version 11, you can check which OS version you have with the command:
cat /etc/os-release Install K3s Even though I wasn’t using buster I needed to enable cgroups as K3S needs cgroups to start the systemd service.
cgroupscan be enabled by appending cgroup_memory=1 cgroup_enable=memory to /boot/cmdline.txt.
For more info on this see Enabling cgroups for Raspbian Buster.
The actual installation of K3s was very easy, I just ran:
curl -sfL https://get.k3s.io | sh - For more info on this see the k3s install doco.
Install/Setup an Agent for an existing cluster The value to use for K3S_TOKEN is stored at /var/lib/rancher/k3s/server/node-token on your server node.
curl -sfL https://get.k3s.io | K3S_URL=https://<IP_ADDRESS_OF_SERVER>:6443 K3S_TOKEN=$K3S_TOKEN sh - # For example curl -sfL https://get.k3s.io | K3S_URL=https://192.168.0.180:6443 K3S_TOKEN=K102c547b44<truncated>e06ce9b06442a sh - Labeling Nodes sudo k3s kubectl label node pi-server arch=arm64 To view the current labels for a node use the following command:
sudo k3s kubectl label --list node <NODE_NAME> # For example sudo k3s kubectl label --list node ht-pc IDE/Tool access to the Cluster Copy /etc/rancher/k3s/k3s.yaml on your server machine located outside the cluster as ~/.kube/config. Then replace “localhost” with the IP or name of your K3s server. kubectl can now manage your K3s cluster.
For more information.
Test with a ‘Hello World’ container/service The steps below walk you through the (manual) creation of a simple ‘hello world’ pod
sudo k3s kubectl run my-httpd --image=httpd:2.4 # You can find the ip address of this running pod with: sudo k3s kubectl get pods my-httpd -o wide # NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES # my-httpd 1/1 Running 0 8m27s 10.42.1.5 ht-pc <none> <none> # You can see the pod in action by using curl curl 10.42.1.5 #<html><body><h1>It works!</h1></body></html> # And remove the pod with sudo k3s kubectl delete pod my-httpd Enable the use of NFS persistent volumes (i.e. NAS shares) Install nfs-common on all the k3s nodes.
sudo apt-get update && sudo apt-get install -y nfs-common K3s bash aliases My experience is the there is a lot of use of the command line so setting up some bash aliases will make your life a lot easier and save on keystrokes for common commands.
ssh <user>@<server> # Create edit the .bash_aliases file nano ~/.bash_aliases # add the following line alias k='sudo k3s kubectl' # Write out the file (Ctrl + O), and exit (Ctrl + X) # Load the aliases file, so that you don't need to log out of the ssh session. . ~/.bash_aliases `,description:"",tags:["raspberry_pi","kubernetes","linux","k3s","project","docker"],title:"K3s on Raspberry Pi",uri:"/posts/2022-03-05-k3s-on-raspbery-pi/index.html"},{content:`This post contains some details on how to customise the k3s (default) traefik ingress controller.
K3s comes with a pre-configured traefik ingress controller which is deployed as a part of the k3s installation/setup. By default traefik listens for changes in the kubernetes services and ingresses and will use the information, along with any annotations, to configure itself accordingly.
To add/alter this default configuration you need to use apply a HelmChartConfig Custom Resource Definition (CRD). For more information see the Customizing Packaged Components with HelmChartConfig K3s page.
Applying your HelmChartConfig To apply your custom HelmChartConfig, which is just a yaml file, use the following command:
sudo k3s kubectl apply -f yourHelmChartConfig.yaml This will cause the K3s cluster to rebuild the details of the traefik deployment, then update the cluster with the new version of the traefik deployment, which includes your customisations.
My desired customisations There were a number of things that I wanted to customise for my Traefik instance which included:
Using a Letscrypt certificate (via acme and duckdns) I wanted to persist these between pods/containers so that I did not risk hitting any rate limits. Setup extra entry points/posts so that I easily reach services inside my k3s cluster Be able to access the Traefik dashboard others… Setup K3s bash aliases If you haven’t done so already setting up some aliases will make life easier and save on keystrokes for common commands.
ssh <user>@<server> # Create edit the .bash_aliases file nano ~/.bash_aliases # add the following line alias k='sudo k3s kubectl' alias kga='sudo k3s kubectl get -A' # Write out the file (Ctrl + O), and exit (Ctrl + X) # Load the aliases file, so that you don't need to log out of the ssh session. . ~/.bash_aliases Letscrypt Certificate Storage I had quite a number of issues trying to get Traefik to store the Letscrypt certificates. Originally I wanted these to be stored on my NAS and accessed via NFS with a kubernetes persistent volume. Unfortunately Traefik requires that the acme.json has file permissions of 600, which I was not able to achieve with my NAS-NFS-PV setup.
In the end I used a directory on the k3s server and setup a Persistent Volume (PV) which used nodeAffinity to make sure that it was always hosted on the same node.
StorageClass The local section of the kubernetes volume doco advised setting up a new StorageClass for this.
# A custom StorageClass for use with local (node) persistent storage --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: local-persistent-storage provisioner: kubernetes.io/no-provisioner reclaimPolicy: Retain allowVolumeExpansion: true volumeBindingMode: WaitForFirstConsumer Create local directory On my server I created a new directory to house my certificates:
sudo mkdir -p /var/local/k3s/traefik PersistentVolume Then I setup a PersistentVolume volume to access this local directory using nodeAffinity to make sure that it was always bound to the right node.
# For the persistent storage of Traefik data (certs etc.) on the local disc of the pi-server node # # Note: Due to the file permission restrictions around the acme.json (letsencrypt certs) file # I was not able to get Traefik to use the NAS via an NFS PV. --- apiVersion: v1 kind: PersistentVolume metadata: name: traefik-local spec: capacity: storage: 100Mi # If not set defaults to Filesystem volumeMode: Filesystem accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain storageClassName: local-persistent-storage local: # Note: this directory needs to exist on the node selected from the expression below path: /var/local/k3s/traefik nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - pi-server PersistentVolumeClaim Next I created a PersistentVolumeClaim which would bind the Traefik volume in the deployment/pod to the PV.
# Needed to support the storage of Traefik data such as letsencrypt certs # # Note: as the 'local-persistent-storage' has volumeBindingMode: WaitForFirstConsumer # This PVC will not bind until a container starts to use it. --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: traefik-local namespace: kube-system spec: # Must match the storageClassName in the PersistentVolume storageClassName: local-persistent-storage # The access modes must match those in a PersistentVolume for it to get Bound accessModes: - ReadWriteMany resources: requests: storage: 100Mi HelmChartConfig Now the persistence has been setup you can now refer to them in your HelmChartConfig which contains all of the extra configuration items that you want for your Traefik instance.
# This configures the built in k3s traefik instance with my additional bits --- apiVersion: helm.cattle.io/v1 kind: HelmChartConfig metadata: name: traefik namespace: kube-system spec: valuesContent: |- # Configure Traefik static configuration # Additional arguments to be passed at Traefik's binary # All available options available on https://docs.traefik.io/reference/static-configuration/cli/ additionalArguments: - "--api.insecure=true" #- "--api.dashboard=true" # This is the letsencrypt staging server, use this in testing to avoid rate limits - "--certificatesresolvers.letsencrypt--duckdns-resolver.acme.caServer=https://acme-staging-v02.api.letsencrypt.org/directory" - "--certificatesresolvers.letsencrypt--duckdns-resolver.acme.dnsChallenge=true" - "--certificatesresolvers.letsencrypt--duckdns-resolver.acme.dnsChallenge.provider=duckdns" - "--certificatesresolvers.letsencrypt--duckdns-resolver.acme.email=<YOUR_EMAIL_ADDRESS>" - "--certificatesresolvers.letsencrypt--duckdns-resolver.acme.storage=/data/letsencrypt/acme.json" # Environment variables to be passed to Traefik's binary # Ideally this should be done via kubernetes secrets! env: - name: DUCKDNS_TOKEN value: "<YOUR_DUCK_DNS_TOKEN>" image: name: traefik tag: v2.7 logs: general: level: DEBUG ports: oracle-db: port: 1521 expose: true exposedPort: 1521 protocol: TCP image-pull: port: 5000 expose: true exposedPort: 5000 protocol: TCP image-push: port: 5001 expose: true exposedPort: 5001 protocol: TCP persistence: enabled: true existingClaim: traefik-local After you apply this to your k3s cluster sudo k3s kubectl apply -f HelmChartConfig.yaml, the cluster will re-create the Traefik deployment and start Traefik with the new updated configuration.
(optional) Exposing the Traefik dashboard Exposing the Traefik dashboard gives you a user friendly way to visualize what is going on in the Traefik system.
Dashboard Service First create a Service to expose the ports from the Traefik pod/instance.
# Exposes the traefik (admin) web ui to the cluster --- kind: Service apiVersion: v1 metadata: name: traefik-dashboard namespace: kube-system labels: app.kubernetes.io/instance: traefik app.kubernetes.io/name: traefik-dashboard spec: type: NodePort ports: - name: traefik port: 9000 targetPort: 9000 protocol: TCP selector: # These need to match the labels of the k3s traefik instance app.kubernetes.io/instance: traefik app.kubernetes.io/name: traefik Dashboard Ingress Lastly create an Ingress to expose the traefik dashboard.
# Exposes the Traefik dashboard --- kind: Ingress apiVersion: networking.k8s.io/v1 metadata: name: traefik-dashboard namespace: kube-system annotations: traefik.ingress.kubernetes.io/router.entrypoints: web spec: rules: - host: "<YOUR_DNS_ENTRY>" http: paths: - path: / pathType: Prefix backend: service: name: traefik-dashboard port: name: traefik Handy Commands # Display the dashboard service k describe -n kube-system svc traefik-dashboard Links and Further information Github Issue - Updating K3s Traefik Ingress Configuration The values and schema used in the Traefik helm chart How to expose the Traefik dashboard in k3s https://medium.com/kubernetes-tutorials/deploying-traefik-as-ingress-controller-for-your-kubernetes-cluster-b03a0672ae0c `,description:"",tags:["kubernetes","ingress","traefik","linux","k3s","project"],title:"K3s with customised Traefik Ingress",uri:"/posts/2022-04-08-k3s-with-customised-ingress/index.html"},{content:`Kubernetes provides a command line tool kubectl for communicating with a Kubernetes cluster’s control plane, using the Kubernetes API.
Configuration For configuration, kubectl looks for a file named config in the $HOME/.kube directory. You can specify other kubeconfig files by setting the KUBECONFIG environment variable or by setting the --kubeconfig flag.
kubectl Commands There are heaps of wonderful kubectl commands which can make life a little easier. See kubectl Cheat Sheet for an overview.
Get Command Description kubectl get pods --show-labels Show labels for all pods (or any other Kubernetes object that supports labelling) kubectl get po -w -l app.kubernetes.io/instance=test-monitoring Watch a specific resource(s) based on a label kubectl get po -l app.kubernetes.io/name=openhab --field-selector status.phase=Running Get a running pod based on a label kubectl get po -l app.kubernetes.io/name=openhab –field-selector status.phase=Running
Interactive shell into container sudo k3s kubectl exec -it <name-of-pod> -- /bin/bash # For example sudo k3s kubectl exec -it task-pv-pod -- /bin/bash Top (resource usage) # for the cluster nodes kubectl top node # for the pods kubectl top pod logs To view the logs for a pod use the commands below:
kubectl logs <pod_name> # For example kubectl logs traefik-ingress-7f4f6cd549-jkdjl -n kube-system Logs for a specific container To access logs from a specific container (such as Init Containers) Pass the Init Container name along with the Pod name to access its logs.
kubectl logs <pod-name> -c <init-container-2> # For example kubectl logs <pod-name> -c <init-container-2> kubectl logs $(kubectl get po | grep magicmirror | awk '{ print $1}') -c install-modules Custom columns Using the flag -o custom-columns=<header-name>:<field> will let you customize the output.
Example with resource name, under header NAME kubectl get pods -o custom-columns=NAME:metadata.name output
NAME myapp-5b77df6c48-dbvnh myapp-64d5985fdb-mcgcn httpd-9497b648f-9vtbl Omit headers The proper solution to omit the header line is by using the flag --no-headers
kubectl get pods -o custom-columns=NAME:metadata.name --no-headers Example output
myapp-5b77df6c48-dbvnh myapp-64d5985fdb-mcgcn httpd-9497b648f-9vtbl Forcefully delete Resources DANGER - Forcefully deleting objects can leave resources in the cluster dangling. Use with extreme caution!
Sometimes when you want to delete a particular resource in Kubernetes, it gets stuck in a Terminating phase. This is typically because the object’s finalizer is no longer in the cluster. This invalid state can come as a result of using tools like Helm which creates custom resources during installation and does not edit or remove them during an uninstall.
To force delete, you take the following steps:
Edit the Object e.g. kubectl edit pod pod-name or kubectl edit customresource/name Remove delete the custom finalizers. If the finalizer has only a “kubernetes” finalizer then you can ignore it as it will be recreated if you remove it Delete the object kubectl delete customresource/name For more information see:
https://github.com/odytrice/kubernetes/blob/master/force-delete.md Other Handy Commands Command Description kubectl rollout restart deployment <deployment-name> Restart all of the pods in a deployment kubectl rollout restart statefulset <deployment-name> Restart all of the pods in a statefulset kubectl scale deployment gogs --replicas=0 Scale the pods to 0 kubectl scale deployment magicmirror --replicas=0 && kubectl scale deployment magicmirror --replicas=1 Restart a deployment. Useful if you need to reload data from a ConfigMap or similar) kubectl logs $(k get po | grep magicmirror | awk '{ print $1}') -c install-modules Show the logs for a specific (init) container kubectl drain --ignore-daemonsets <node> Drain a node in preparation for a maintenance activity kubectl uncordon <node> Tell Kubernetes that it can resume scheduling new pods onto the node (post maintenance) kubectl cp my-pod:my-file my-file Copy a file from a pod to the host `,description:"",tags:["container","development","kubernetes","kubectl"],title:"kubeclt",uri:"/tools/kubernetes/kubectl/index.html"},{content:"",description:"",tags:null,title:"kubectl",uri:"/tags/kubectl/index.html"},{content:`This page and it’s children contain information about Kubernetes that I have discovered, and documented.
Kubernetes Pages HelmHelm helps you manage Kubernetes applications — Helm Charts help you define, install, and upgrade even the most complex Kubernetes application.
K3sK3s - Lightweight Kubernetes. Easy to install, half the memory, all in a binary of less than 100 MB.
kubecltKubernetes provides a command line tool kubectl for communicating with a Kubernetes cluster’s control plane, using the Kubernetes API.
Kubernetes ObjectsThis page contains information about Kubernetes objects.
Kubernetes overview and major components Control Pane components The control pane components are started with the container runtime (i.e. Docker, CRI-O )
Creating/Updating API Resources - Apply vs Create apply is preferred as this uses a declarative approach, this means that if you run it twice you will NOT get an error.
Namespaces default kube-system Used for the Kubernetes systems kube-public Available publicly, for use for things like container repositories
Volumes nfs is the only storage type that supports accessModes of ReadWriteMany.
Services (and access) ClusterIP only exposes the service(s) to the container.
NodePort is accessible from both within and outside the Kuberentes cluster.
Config maps Best practice is to inject your config map as a volume.
`,description:"",tags:["container","kubernetes"],title:"Kubernetes",uri:"/tools/kubernetes/index.html"},{content:`This page contains information about Kubernetes objects.
Namespaces default kube-system Used for the Kubernetes systems kube-public Available publicly, for use for things like container repositories
Volumes nfs is the only storage type that supports accessModes of ReadWriteMany.
Services (and access) ClusterIP only exposes the service(s) to the container.
NodePort is accessible from both within and outside the Kuberentes cluster.
Config maps Best practice is to inject your config map as a volume.
Workload Resources (Pods, Deployments, StatefulSets) Resource Limits You can apply resource limits to prevent containers from using all available resources on a node.
... containers: - name: gogs image: gogs/gogs:0.12.6 imagePullPolicy: IfNotPresent ... resources: limits: # Limit the memory use as it can get a little wild if left unbound memory: 512Mi cpu: 500m Liveness, Readiness and Startup Probes The kubelet uses liveness probes to know when to restart a container.
Caution: Liveness probes do not wait for readiness probes to succeed. If you want to wait before executing a liveness probe you should use initialDelaySeconds or a startupProbe.
The kubelet uses readiness probes to know when a container is ready to start accepting traffic. A Pod is considered ready when all of its containers are ready.
The kubelet uses startup probes to know when a container application has started. If such a probe is configured, it disables liveness and readiness checks until it succeeds, making sure those probes don’t interfere with the application startup. This can be used to adopt liveness checks on slow starting containers, avoiding them getting killed by the kubelet before they are up and running.
`,description:"",tags:["container","kubernetes","kubectl","yaml","manifest"],title:"Kubernetes Objects",uri:"/tools/kubernetes/objects/index.html"},{content:`This page contains some information about some handy or popular JavaScript libraries or frameworks.
Lodash Lodash is a popular library with methods which can help with type checking.
Importing Lodash The recommended way to import lodash is on a per function basis:
import isArray from 'lodash/isArray'; Because lodash holds all it’s functions in a single file, so rather than import the whole ’lodash’ library at 100k, it’s better to just import lodash’s has function which is maybe 2k.
For more information see this stackoverflow post.
Jest Jest is a common JavaScript testing framework.
Testing Errors Test Error is thrown // Function being tested const throwError = (): void { throw new Error('Something unexpected happened'); } test('Test that an Error should be thrown', () => { // Will fail the test if an Error is not thrown expect(throwError).toThrowError(); }); `,description:"",tags:["javascript"],title:"Libraries",uri:"/languages/javascript/libraries/index.html"},{content:"",description:"",tags:null,title:"linux",uri:"/tags/linux/index.html"},{content:`Linux is a family of open-source Unix-like operating systems based on the Linux kernel, an operating system kernel first released on September 17, 1991, by Linus Torvalds.
Linux is typically packaged as a Linux distribution, which includes the kernel and supporting system software and libraries, many of which are provided by the GNU Project.
cgroups cgroups is a Linux kernel feature that limits, accounts for, and isolates the resource usage of a collection of processes. Engineers at Google started the work on this feature in 2006 under the name “process containers”
Get a List of All Users using the /etc/passwd File Local user information is stored in the /etc/passwd file. Each line in this file represents login information for one user. To open the file you can either use cat or less:
less /etc/passwd Sub Pages & Topics Configuration (Linux) Environmental Variables (Linux) Files Linux Tools Monitoring Tools (Linux) sh - Shell Command Language System Information (Linux) systemd `,description:"",tags:["operating_system","linux"],title:"Linux",uri:"/operating-systems/linux/index.html"},{content:`This page contains links to the Linux (and Unix) specific tools that I have discovered, and documented, as a part of my development journey.
Tools awk grep inotifywait rsync sleep Unix VI text editor `,description:"",tags:["operating_system","linux","tools"],title:"Linux Tools",uri:"/operating-systems/linux/tools/index.html"},{content:"",description:"",tags:null,title:"list",uri:"/tags/list/index.html"},{content:`This page contains information and simple code examples about dealing with LOBs (CLOBs and BLOBs) in SQL statements.
CLOB/BLOB length To find out the length of a LOB object you can use the DBMS_LOB.getLength function:
Select up.*, DBMS_LOB.getLength(up.stage_layout) as "BLOB_LENGTH" from ui_properties up where up.stage_layout is not null order by "BLOB_LENGTH" desc; Converting BLOB to CLOB - No truncation Unfortunately there is no built in function that will do this conversion that does not have a length restriction on it. A workaround to this is to create a function which you can then use in your SQL statements:
Function create function clobfromblob(p_blob blob) return clob is l_clob clob; l_dest_offsset integer := 1; l_src_offsset integer := 1; l_lang_context integer := dbms_lob.default_lang_ctx; l_warning integer; begin if p_blob is null then return null; end if; dbms_lob.createTemporary(lob_loc => l_clob ,cache => false); dbms_lob.converttoclob(dest_lob => l_clob ,src_blob => p_blob ,amount => dbms_lob.lobmaxsize ,dest_offset => l_dest_offsset ,src_offset => l_src_offsset ,blob_csid => dbms_lob.default_csid ,lang_context => l_lang_context ,warning => l_warning); return l_clob; end; Function use select up.*, clobfromblob(up.stage_layout), DBMS_LOB.getLength(up.stage_layout) as "BLOB_LENGTH" from ui_properties cup where up.stage_layout is not null; `,description:"",tags:["database","sql","oracle","clob","blob"],title:"LOBs",uri:"/languages/sql/sql_lobs/index.html"},{content:"",description:"",tags:null,title:"loops",uri:"/tags/loops/index.html"},{content:"This page provides examples about the use of PowerShell loops.\nforeach - file You can you use combination of Get-ChildItem and the foreach loop to process a set of files:\n# Get all sql files... $files = Get-ChildItem $DataDir\\*.sql foreach ($file in $files) { Write-Output 'Processing sql script: ' $file.Fullname ## Do something } continue statement The continue statement allows you to skip the remaining part of the code, returning you to the beginning of the loop without exiting it.\nFor example:\nfor ($var = 1; $var -le 5; $var++) { if ($var -eq 3) {Continue} Write-Host The value of Var is: $var } Write-Host End of for loop. Results in:\nThe value of Var is: 1 The value of Var is: 2 The value of Var is: 4 The value of Var is: 5 End of for loop. ",description:"",tags:["scripting","windows","powershell","terminal","loops"],title:"Loops (PowerShell)",uri:"/languages/powershell/loops/index.html"},{content:"",description:"",tags:null,title:"manifest",uri:"/tags/manifest/index.html"},{content:"",description:"",tags:null,title:"mariadb",uri:"/tags/mariadb/index.html"},{content:"",description:"",tags:null,title:"markdown",uri:"/tags/markdown/index.html"},{content:`Markdown is a lightweight markup language with plain-text-formatting syntax. Its design allows it to be converted to many output formats.
Italics To italicise a word just wrap it in either a _ or *, for example:
With underscores _italics_ => italics With an astrix *italics* => italics
New line By default, Markdown will collapse lines separated but just a carriage return will be collapsed into one line.
To display the lines separately add two (2) spaces and a carriage return.
Collapsible Sections This gist post shows how you can use a collapsible section in markdown.
# A collapsible section with markdown <details> <summary>Click to expand!</summary> ## Heading 1. A numbered 2. list * With some * Sub bullets </details> Blockquotes To create a blockquote, add a > in front of a paragraph. Best practice is to leave a line before and after the block quote.
> Dorothy followed her through many of the beautiful rooms in her castle. The rendered output looks like this:
Dorothy followed her through many of the beautiful rooms in her castle.
`,description:"",tags:["markdown","jekyll","hugo"],title:"MarkDown",uri:"/languages/markdown/index.html"},{content:"",description:"",tags:null,title:"marshalling",uri:"/tags/marshalling/index.html"},{content:`This page contains information, syntax, and simple code examples, about un/marshalling XML in SpringBoot.
Using a JAXB runtime If you have existing POJOs which use Jakarta (nee JAXB) XML annotations you might want to consider using a JAXB runtime such as glassfish.
My experience is that even with a configured Jackson Jakarta annotation module (com.fasterxml.jackson.module:jackson-module-jakarta-xmlbind-annotations) the behaviour of each implementation is still different and the XML they produce can be quite different.
To use a JAXB runtime add the following dependencies to your pom.xml file:
<dependency> <groupId>jakarta.xml.bind</groupId> <artifactId>jakarta.xml.bind-api</artifactId> </dependency> <dependency> <groupId>org.glassfish.jaxb</groupId> <artifactId>jaxb-runtime</artifactId> </dependency> Then create a configuration which produces a Jaxb2Marshaller class, for example:
@Configuration public class JaxbConfiguration { @Bean public Jaxb2Marshaller jaxb2Marshaller(){ Jaxb2Marshaller marshaller = new Jaxb2Marshaller(); // Pretty print the output marshaller.setMarshallerProperties(Map.of(Marshaller.JAXB_FORMATTED_OUTPUT, true)); marshaller.setClassesToBeBound(User.class, AnotherXmlClass.class); return marshaller; } } Manually applying to RestTemplate In typical cases the creation of a Jaxb2Marshaller will get automatically applied to the creation of any RestTemplate instances by SpringBoot. If this is not working, or you need more control you can apply it manually, then you can do this as follows:
@Configuration public class ConfigUtil { /** * @return the RetTemplate. This method sets the Message Converter objects to the RestTemplate so that it uses the * Jaxb2Marshaller for XML un/marshalling */ @Bean(name = "restTemplate") public RestTemplate getRestTemplate(MarshallingHttpMessageConverter converter) { RestTemplate restTemplate = new RestTemplate(); restTemplate.setMessageConverters( List.of(converter, new FormHttpMessageConverter(), new StringHttpMessageConverter())); return restTemplate; } /** * @return MarshallingHttpMessageConverter object which is responsible for XML * marshalling and unMarshalling processes using Jaxb2Marshaller */ @Bean(name = "marshallingHttpMessageConverter") public MarshallingHttpMessageConverter getMarshallingHttpMessageConverter(Jaxb2Marshaller jaxb2Marshaller) { MarshallingHttpMessageConverter marshallingHttpMessageConverter = new MarshallingHttpMessageConverter(); marshallingHttpMessageConverter.setMarshaller(jaxb2Marshaller); marshallingHttpMessageConverter.setUnmarshaller(jaxb2Marshaller); return marshallingHttpMessageConverter; } @Bean(name = "jaxb2Marshaller") public Jaxb2Marshaller getJaxb2Marshaller() { Jaxb2Marshaller jaxb2Marshaller = new Jaxb2Marshaller(); // Add all XML classes to be bound jaxb2Marshaller.setClassesToBeBound(Customer.class); return jaxb2Marshaller; } } `,description:"",tags:["java","springboot","xml","serialisation","jaxb","marshalling","unmarshalling"],title:"Marshalling XML in SpringBoot",uri:"/languages/java/spring-boot/xml-marshalling/index.html"},{content:"",description:"",tags:null,title:"maven",uri:"/tags/maven/index.html"},{content:`Maven is a build automation tool used primarily for Java projects. Maven can also be used to build and manage projects written in C#, Ruby, Scala, and other languages. The Maven project is hosted by the Apache Software Foundation, where it was formerly part of the Jakarta Project.
Handy Commands Command Description mvn dependency:tree Print Dependency Tree mvn test-compile Compiles the test classes (but doesn’t run them) mvn archetype:generate Interactively generates/initialised a maven project (Re) Download project dependencies In Maven, you can use Apache Maven Dependency Plugin, goal dependency:purge-local-repository to remove the project dependencies from the local repository, and re-download it again.
mvn dependency:purge-local-repository Excluding a module from being published ... <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-deploy-plugin</artifactId> <configuration> <!-- Do not deploy this artifact --> <skip>true</skip> </configuration> </plugin> </plugins> </build> Publishing Artifacts You will need a distributionManagement entry in your projects pom.xml
<project ...> ... <distributionManagement> <snapshotRepository> <id>snapshots</id> <url>https://<YOUR_REPO_HOST>/repository/maven-snapshots</url> </snapshotRepository> </distributionManagement> ... </project> And you will need an entry in your ~/.m2/settings.xml
<settings ...> <servers> <server> <id>central</id> <username>USER_NAME</username> <password>PASSWORD</password> </server> </servers> </settings> Then you can publish/deploy the artefact with the command:
mvn clean deploy -Dmaven.test.skip=true For more information this baeldung article
Generating sources from (SOAP) WSDL This one took me a bit to figure out so recording it here for future use/recollection.
This solution is based on the Apache cxf-codegen-plugin which worked well for my case.
<project> ... <build> <plugins> <plugin> <groupId>org.apache.cxf</groupId> <artifactId>cxf-codegen-plugin</artifactId> <version>4.0.3</version> <executions> <execution> <id>generate-hello-world-sources</id> <configuration> <sourceRoot>\${project.build.dir}/generated-sources</sourceRoot> <wsdlOptions> <wsdl>\${project.baseDir}/src/main/resources/wsdl/HelloWorld.wsdl</wsdl> </wsdlOptions> </configuration> <goals> <goal>wsdl2java</goal> </goals> </execution> </executions> </plugin> </plugins> </build> </project> You can also generate toString() methods and more using xjc plugins, which need to be added as dependencies of the plugin.
Overriding Parent Maven Plugins If you are inheriting from a parent or using some defaults plugins you can override the version of the maven plugin(s) that are being used in the child project by adding an entry to the <build><pluginManagement> section of the pom.xml file.
<project> ... <build> <pluginManagemnt> <plugins> <!-- Overrides the version of the plugin defined the in the parent pom.xml file --> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <version>3.1.2</version> </plugin> </plugins> </pluginManagemnt> </build> </project> Troubleshooting JUnit 5 Tests not Running Under Maven If you run into an issue where your JUnit5 tests run within the IDE but don’t run in maven it may be due to the version of the maven-surefire-plugin or maven-failsafe-plugin that is being used. Older versions of these plugins do not work with JUnit 5.
The fix is to define a newer version (ideally the latest available) of these plugins in your projects pom.xml.
<project> ... <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <version>3.1.2</version> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-failsafe-plugin</artifactId> <version>3.1.2</version> </plugin> </plugins> </build> </project> Make sure you check Maven Central for the latest version of the plugins.
I stumbled across this DZone article which helped me solve my issue.
JDK8 sun.* classes missing in build??? If you are using any of the sun.* classes directly (such as sun.security.x509.CertAndKeyGen) in your project keep in mind that these fall outside the standard Java platform.
These are JDK implementation specific classes so may not be available in all flavours of Java. If they are available in the version you are using you can use the -XDignore.symbol.file compiler argument to allow access to these classes.
You can add this argument to the maven-compiler-plugin in your projects pom.xml so that it is applied automatically.
<project> ... <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>3.2</version> <configuration> <fork>true</fork> <compilerArgument>-XDignore.symbol.file</compilerArgument> </configuration> </plugin> </plugins> </build> </project> I came across this stakoverflow post which helped me solve my issue.
`,description:"",tags:["maven","java","development"],title:"Maven",uri:"/tools/maven/index.html"},{content:"",description:"",tags:null,title:"mealie",uri:"/tags/mealie/index.html"},{content:'This page documents my efforts to migrate my Gogs database from SQLite to MariaDB.\nI had been using the default sqlite DB for my configuration, however I started getting database locked errors. The best advice that I could find was to migrate to a production ready database like MySQL.\nI run my gogs instance on a k3s kubernetes cluster as a StatefulSet.\nAs such the instructions below run any gogs commands within the gogs container of the gogs-0 pod.\nBackup current database Open a terminal and run:\n#Get a terminal within the container $ sudo k3s kubectl exec -it gogs-0 -- /bin/bash # Switch to the `git` user (the one running the gogs container) $ su git # Create the directory where you want to create the backup and change to it $ mkdir /data/gogs/data/backup $ cd /data/gogs/data/backup # And backup the database $ /app/gogs/gogs backup --database-only 2023/06/22 23:39:10 [ INFO] Backup root directory: /tmp/gogs-backup-1214859535 2023/06/22 23:39:10 [ INFO] Packing backup files to: gogs-backup-20230622233910.zip 2023/06/22 23:39:11 [ INFO] Backup succeed! Archive is located at: gogs-backup-20230622233910.zip Create new Database After you have installed & setup MySQL/MariaDB, then run the mysql.sql script. This will create a database called gogs.\nCreate a gogs DB user I used phpMyAdmin to create the gogs database user:\nUser accounts –> Add user account Fill in the user details: Username: gogs Hostname: Any host Password: <GOGS_DB_USER_PASSWORD> Database for user account: Grant all privileges to gogs If you are creating the user via SQL you can use something like:\nGRANT USAGE ON *.* TO `gogs`@`%` IDENTIFIED BY PASSWORD \'<SOME_PASSWORD>\'; GRANT ALL PRIVILEGES ON `gogs`.* TO `gogs`@`%`; Update Database Configuration I found that the comments embedded in the gogs GitHub repo were the best source of information on how to configure the database.\nUpdate the /data/gogs/conf/app.ini config file with the database connection details:\n[database] TYPE = mysql HOST = <YOUR_MARIA_DB_SERVER>:3307 NAME = gogs USER = gogs PASSWORD = `<GOGS_DB_USER_PASSWORD>` Restore from backup #Get a terminal within the container $ sudo k3s kubectl exec -it gogs-0 -- /bin/bash # Switch to the `git` user (the one running the gogs container) $ su git # Change to the backup dir $ cd /data/gogs/data/backup # And restore the database $ /app/gogs/gogs restore --database-only --from="gogs-backup-20230622233910.zip" --config="/data/gogs/conf/app.ini" 2023/06/23 21:16:38 [ INFO] Restore backup from: gogs-backup-20230622233910.zip 2023/06/23 21:17:10 [ INFO] Restore succeed! Done! I restarted the Gogs kubernetes pod just to be safe, but after loggin in I found everything was as it had been.\nAdditionally, the database locked issue that I was encountering before no longer appears : )\nLinks How to backup, restore and migrate Gogs Installation instructions Gogs default configuration (with handy comments) ',description:"",tags:["gogs","db","sqlite","mariadb","git"],title:"Migrating Gogs from SQLite to MariaDB",uri:"/posts/2023-06-23-gogs-sqlite-to-mariadb-migration/index.html"},{content:"",description:"",tags:null,title:"mint",uri:"/tags/mint/index.html"},{content:"",description:"",tags:null,title:"mocking",uri:"/tags/mocking/index.html"},{content:`This page provides examples of how to use Hugo modules.
Upgrading a theme To upgrade an existing theme to a new version, use the following syntax: hugo mod get github.com/username/themeName@commitHash.
For example:
hugo mod get github.com/McShelby/hugo-theme-relearn@32a23dbf44006e7ac19604657b897b1f204753c4 `,description:"",tags:["go","hugo","theme"],title:"Modules",uri:"/tools/hugo/modules/index.html"},{content:"",description:"",tags:null,title:"monitoring",uri:"/tags/monitoring/index.html"},{content:`This page provides information about some linux specific monitoring tools.
Network Monitoring nload nload is a command-line tool that displays the network usage on the system. It belongs to the category of network monitoring tool in Linux that simply sum up all the network traffic on a network interface.
# Installation sudo apt-get install -y nload # Basic Usage nload `,description:"",tags:["linux","monitoring"],title:"Monitoring Tools (Linux)",uri:"/operating-systems/linux/monitoring/index.html"},{content:"",description:"",tags:null,title:"network",uri:"/tags/network/index.html"},{content:`The Open Systems Interconnection model (OSI model) is a conceptual model that ‘provides a common basis for the coordination of ISO standards development for the purpose of systems interconnection’.
In the OSI reference model, the communications between a computing system are split into seven different abstraction layers: Physical, Data Link, Network, Transport, Session, Presentation, and Application.
Below are some diagrams that I found useful in understanding this concept.
OSI Reference Model Layers TCP/IP Model vs. OSI Model TCP/IP vs. OSI: What’s the Difference Between the Two Models?
`,description:"",tags:["network","osi","concept","ip_address"],title:"Network Layers",uri:"/concepts/network_layers/index.html"},{content:"",description:"",tags:null,title:"nginx",uri:"/tags/nginx/index.html"},{content:`Nginx (pronounced “engine x” /ˌɛndʒɪnˈɛks/ EN-jin-EKS, stylized as NGINX) is a web server that can also be used as a reverse proxy, load balancer, mail proxy and HTTP cache.
The software was created by Igor Sysoev and publicly released in 2004. Nginx is free and open-source software.
Troubleshooting Links have wrong hostname or port While building a static site with Hugo which I was hosting with a Nginx Docker container I found that the relative links would be resolved by the browser to the wrong hostname or port.
After way too much investigation and searching I came across this article which gave me the clue that I needed.
Looking at the Nginx logs I could see that when I used a relative href link such as href="/blog" Nginx was returning with a http code of 301 (Moved Permanently) and redirect the browser to /blog/.
This is expected behaviour except that Ngnix was also returning the hostname (and port) as well, which does not work when running in a container, or behind a load balancer, or a TLS termination point (such as an HTTPS Ingress in Kubernetes).
The fix is easy I just needed to add absolute_redirect off; to the http server config as per the example below:
# In the /etc/nginx/conf.d/default.conf file server { listen 80; listen [::]:80; server_name localhost; # Make redirects issued by nginx relative. absolute_redirect off; ... } See the Nginx config doco for more details.
`,description:"",tags:["nginx","http"],title:"Nginx",uri:"/tools/nginx/index.html"},{content:`This page contains information, syntax, and simple code examples, about the use of Java’s NIO2 API.
The new API was introduced in Java 7 and provides methods that make working with files and filesystems much easier.
Current Directory The Paths API can be used to get the current working directory:
String workingDirectory = Paths.get("") .toAbsolutePath() .toString(); System.out.println("PWD: " + workingDirectory); `,description:"",tags:["java","development","files","paths"],title:"NIO2",uri:"/languages/java/nio_2/index.html"},{content:`Node.js is an open-source, cross-platform, JavaScript runtime environment that executes JavaScript code outside of a browser.
Node.js lets developers use JavaScript to write command line tools and for server-side scripting—running scripts server-side to produce dynamic web page content before the page is sent to the user’s web browser.
Consequently, Node.js represents a “JavaScript everywhere” paradigm, unifying web-application development around a single programming language, rather than different languages for server- and client-side scripts. Wikipedia
Node Versions and ES Support According to node.green version 10, or higher, covers 99% of ES2015 (ES6) support.
NPM (Node Package Manager) Note: Most of the commands below can be run in Global mode by appending -g or --global to the command.
install a package Project dependencies (packages) are installed by using the cli command npm install <package> or npm i <package>.
Install Dev Dependencies To install a library as a Dev package use the --save-dev at the end:
PS> npm i ts-jest --save-dev # Installs as a devDependencies in the package.json PS> npm i jest --save-dev Install a specific version By default npm will install the latest version of a package, you can override this behavior if you need to install a specific version by using the following cli syntax npm i <package>@<version>. If you want that exact version specified in your package.json dependencies add the --save --save-exact flags. for example:
# Removes the package and updates the package.json PS> npm i serverless-offline@5.12.1 --save-dev --save-exact remove a package Packages can be removed by using the cli command npm remove <package>. To also update the package.json file add the --save flag:
# Removes the package and updates the package.json PS> npm remove jest --save list packages installed The npm list command will print to stdout all the versions of packages that are installed, as well as their dependencies, in a tree-structure.
# Prints out all packages for the project PS> npm list # Prints out the version of the \`tuyapi\` package PS> npm list tuyapi # To see which global libraries are installed and where they're located PS> npm list -g # To see just the path run PS> npm list -g | head -1 outdated packages The npm outdated [<package> ...] command will check the registry to see if any (or, specific) installed packages are currently outdated.
/etc/openhab2/scripts/tuya-mqtt$ npm outdated Package Current Wanted Latest Location color-convert 1.9.3 1.9.3 2.0.1 tuya-mqtt mqtt 3.0.0 3.0.0 4.0.0 tuya-mqtt tuyapi 5.1.3 5.2.1 5.2.1 tuya-mqtt /etc/openhab2/scripts/tuya-mqtt$ update packages The npm update [-g] [<pkg>...] command will update all the packages listed to the latest version.
/etc/openhab2/scripts/tuya-mqtt$ npm update tuyapi + tuyapi@5.2.1 updated 3 packages and audited 357 packages in 10.155s /etc/openhab2/scripts/tuya-mqtt$ clean-install (ci) The npm clean-install or npm ci is similar to npm install, except it’s meant to be used in automated environments such as test platforms, continuous integration, and deployment – or any situation where you want to make sure you’re doing a clean install of your dependencies.
link This command is handy if you are actively working on a module and would like these changes reflected immediately within the containing project/app.
Firstly, it creates a global link to the folder containing the source code for the folder. Then you link the parent app/project to the global link so that it uses the source code folder, rather than the real npm module.
cd ~/projects/some-module # go into the package directory npm link # creates global link cd ~/projects/node-bloggy-app # go into some other package directory. npm link some-module # link-install the package `,description:"",tags:["javascript"],title:"Node.js",uri:"/languages/javascript/node_js/index.html"},{content:"",description:"",tags:null,title:"nodejs",uri:"/tags/nodejs/index.html"},{content:"",description:"",tags:null,title:"openhab",uri:"/tags/openhab/index.html"},{content:`Open Home Automation Bus (openHAB) is an open source home automation software written in Java. It is deployed on premises and connects to devices and services from different vendors.
Handy Links & Info Lost admin password openHAB console Rules The openHAB rule syntax is based on Xbase and as a result it is sharing many details with Xtend, which is built on top of Xbase as well.
Loops // The time between each command var Number loopTime = 1000 // Loop to Turn volume down var Number i = 0 while((i=i-1) >= iterations) { logInfo(logger_name, "Volume down") AV_Receiver_Volume_Down_Switch.sendCommand(ON) Thread::sleep(loopTime) } `,description:"",tags:["openhab","linux","java","automation"],title:"openHAB",uri:"/tools/openhab/index.html"},{content:`OpenSSL is a software library for applications that provide secure communications over computer networks against eavesdropping or need to identify the party at the other end.
It is widely used by Internet servers, including the majority of HTTPS websites.
Check if expired # PEM file openssl x509 -enddate -noout -in file.pem # Certifcate file openssl x509 -enddate -noout -in file.crt `,description:"",tags:["linux","ssl","certificates"],title:"OpenSSL",uri:"/tools/openssl/index.html"},{content:`This page contains information about the operating systems that I have come across throughout my career (read journey) as a software developer.
Pages AlpineAlpine Linux is a Linux distribution designed to be small, simple, and secure.
DebianThe Linux Debian operating system is used as the base for many other linux operating systems such as Ubuntu and the Raspberry Pi OS (formerly Raspbian).
LinuxLinux is a family of open-source Unix-like operating systems based on the Linux kernel, an operating system kernel first released on September 17, 1991, by Linus Torvalds.
Red Hat Enterprise Linux (RHEL)Red Hat Enterprise Linux is a commercial Linux distribution developed by Red Hat for the commercial market.
RetroPieInstalling/Enabling the Pixel Desktop The PIXEL (formerly LXDE) desktop environment was removed from the RetroPie image to keep it smaller. It can easily be installed from the RetroPie Setup Script (~/RetroPie-Setup/retropie_setup.sh) In Configuration / Tools » Raspbiantools » Install Pixel Desktop Environment After installation, it will be accessible from the ports menu of EmulationStation or can be called from the command line with startx For more details see where did the desktop go
UbuntuUbuntu is a Linux distribution based on Debian and composed mostly of free and open-source software.
WindowsMicrosoft Windows is a group of several proprietary graphical operating system families developed and marketed by Microsoft.
`,description:"",tags:null,title:"Operating Systems",uri:"/operating-systems/index.html"},{content:"",description:"",tags:null,title:"operating_system",uri:"/tags/operating_system/index.html"},{content:"",description:"",tags:null,title:"oracle",uri:"/tags/oracle/index.html"},{content:`Oracle XE (Express Edition) is the same powerful Oracle Database that enterprises rely on worldwide, packaged for simple download, ease-of-use, and a full-featured experience. You get an Oracle Database to use in any environment, plus the ability to embed and redistribute – all completely free!
It should be noted that oracle runs using a client-server model. This means that there are different files (and commands) specific to the client and server roles of an Oracle XE database.
Listener (Server Side) The listener is a separate database server process that runs locally on the database server or remotely.
When a client request comes, the listener first receives it. And then the listener establishes a connection between the client and the database instance.
Once the client is connected to the database instance successfully, the listener hands over the client connection to the server process.
If the listener stops running, you cannot connect to the Oracle Database anymore. However, all the existing connections will not be affected.
Listener commands lsnrctl services - checks the listener services that are running lsnrctl stat - prints the status of the listener lsnrctl - Starts the LSNRCTL interactive command line, more details can be found with help lsnrctl stop - Stops the listener service (needs to be run by the user who installed it, often admin/root) lsnrctl start - Starts the listener service listerer.ora This file is used by the listener to determine what DB services to expose and the details of the host and port.
They are typically located in the $ORACLE_HOME/network/admin/ folder.
Windows sometimes does not set the ORACLE_HOME variable, in this case have a look in the path variable to see if there is an entry there Take note that these are files are automatically generated as a part of the Oracle XE installation process and may need to be updated to change the HOST variable to something else (often to HOST = localhost for a localhost only connection, or the Hostname for a remote connection).
Client Side On the client side the connections that your pc uses are defined in the $ORACLE_HOME/network/admin/tnsnames.ora file.
To assist in diagnosing connection problems you can use the tnsping command, such as:
tnsping <HOSTNAME> tnsping <HOSTNAME> [<DATABASE_SID>] Another lower level (linux) diagnostic tool that can be used on the client side is telnet. With telnet you can determine if the server is listening on a given port:
telnet <HOSTNAME> <PORT>
`,description:"",tags:["oracle","database","sql"],title:"Oracle XE (Express Edition)",uri:"/tools/oracle-xe/index.html"},{content:`This post captures the steps that I went through to upgrade my Oracle XE 11g database, which I use for some of my development projects, to Oracle XE version 19c.
The process below was based on the instructions outlined here
Remove unwanted data/schemas sqlplus / AS SYSDBA SQL> DROP USER HR CASCADE; SQL> exit Exporting Data To export data from your 11.2 XE database, perform the following steps:
Create a dump folder, run the following command from your Windows command prompt: mkdir C:\\temp\\dump
Connect to the 11.2 XE database as user SYS using the SYSDBA privilege.
Create directory object DUMP_DIR and grant READ and WRITE privileges on the DUMP_DIR directory to the SYSTEM user.
sqlplus / AS SYSDBA SQL> CREATE DIRECTORY DUMP_DIR AS 'C:\\temp\\dump'; SQL> GRANT READ, WRITE ON DIRECTORY DUMP_DIR TO SYSTEM; SQL> exit Create a parameters file params.par for the export parameters, this avoids having to deal with escaping characters on the command line:
full=Y EXCLUDE=SCHEMA:"LIKE 'APEX_%'",SCHEMA:"LIKE 'FLOWS_%'" directory=DUMP_DIR dumpfile=DB11G.dmp logfile=expdpDB11G.log Export data from your 11.2 XE database to the dump folder.
expdp system/system_password # for example expdp system/password parfile=export_params.par Uninstall and Install new Version Uninstall Oracle Database XE 11.2 if installation of 18c XE is planned on the same system.
Install Oracle Database XE 18c.
Import the data (into 18c) To import data in your new 18c XE database, perform the following steps:
Connect to the 18c XE database as user SYS using the SYSDBA privilege.
Create directory object DUMP_DIR and grant READ and WRITE privileges on the directory to the SYSTEM user.
sqlplus / AS SYSDBA SQL> ALTER SESSION SET CONTAINER=XEPDB1; SQL> CREATE DIRECTORY DUMP_DIR AS 'C:\\temp\\dump'; SQL> GRANT READ, WRITE ON DIRECTORY DUMP_DIR TO SYSTEM; SQL> exit Update/create a file params.par for the import parameters, this avoids having to deal with escaping characters on the command line.
Note: the only change from the import parameter file is the logfile=impdpDB11G.log line
full=Y EXCLUDE=SCHEMA:"LIKE 'APEX_%'",SCHEMA:"LIKE 'FLOWS_%'" directory=DUMP_DIR dumpfile=DB11G.dmp logfile=impdpDB11G.log Import data to the 18c XE database from the dump folder.
impdp system/system_password@localhost:listnerport/xepdb1 parfile=params.par # For example impdp system/password@localhost:1521/xepdb1 parfile=params.par You can ignore the following errors:
ORA-39083: Object type TABLESPACE:“SYSAUX” failed to create with error ORA-31685: Object type USER:“SYS” failed due to insufficient privileges ORA-39083: Object type PROCACT_SYSTEM failed to create with error ORA-01917: user or role ‘APEX_040000’ does not exist ORA-31684 “already exists” errors Update connection details If your database connection used the database SID you will need to change this to use the Service Name of the pluggable database.
For example if your previous jdbc connection url was: jdbc:oracle:thin:@hostname:port:SID this will need to be updated to: jdbc:oracle:thin:@hostname:port/service
Identifying the Service name (if required) You can check the listener services that are running to identify the Service name you should be using with the command: lsnrctl services
Listener service failing to start (Windows) I encountered an issue on an older Windows PC which prevented the OracleOraDB18Home1TNSListener service to start automatically when the system was rebooted/restarted.
I believe this was due a default timeout that windows has for services, if this is exceeded (like when there are a lot of processes starting on an older PC) it prevents the service from starting correctly.
To fix this I changed the Startup type to Automatic (Delayed Start) which delays the start of the service (defaults to 120 seconds), to when there is less load on the PC.
`,description:"",tags:["oracle","database","sql"],title:"Oracle XE 11g to 19c Upgrade",uri:"/posts/2022-03-18-oracle-11g-to19c-upgrade/index.html"},{content:"",description:"",tags:null,title:"osi",uri:"/tags/osi/index.html"},{content:`This page provides examples of how to use the Hugo page syntax.
Front Matter The date field in the front matter is used by Hugo to determine if the post should be rendered in the output. If the value of date is in the future it will not be rendered.
Links Within markdown files you can use links as follows:
Github uses [git](http://example.org/tools/git/) `,description:"",tags:["go","hugo"],title:"Page syntax",uri:"/tools/hugo/syntax/index.html"},{content:"",description:"",tags:null,title:"password",uri:"/tags/password/index.html"},{content:"",description:"",tags:null,title:"paths",uri:"/tags/paths/index.html"},{content:"",description:"",tags:null,title:"permissions",uri:"/tags/permissions/index.html"},{content:`Pi-hole is a Linux network-level advertisement and Internet tracker blocking application which acts as a DNS sinkhole and optionally a DHCP server, intended for use on a private network. It is designed for low-power embedded devices with network capability, such as the Raspberry Pi, but can be installed on almost any Linux machine.[
Admin API There is an admin api that can be used by script or their-party app to control Pi-hole.
The admin api is available at: http://pi.hole/admin/api.php?<ADMIN_FUNTION>&auth=
The TOKEN (WEBPASSWORD) can be found here: /etc/pihole/setupVars.conf, as per the advice in this blog post.
Note 1: An incorrect, or missing, auth parameter in the request will result in a 200 response with an empty array [] in the response payload. Note 2: It looks like the WEBPASSWORD changes when a pihole update (pihole -up) is performed Temporary Disable Using the Admin API you can temporarily disable the ad blocking by using the command: http://pi.hole/admin/api.php?disable=&auth=
To re-enable the ad blocking you can use: http://pi.hole/admin/api.php?enable&auth=
`,description:"",tags:["linux","development","network"],title:"Pi-hole",uri:"/tools/pi-hole/index.html"},{content:"",description:"",tags:null,title:"plugin",uri:"/tags/plugin/index.html"},{content:"",description:"",tags:null,title:"power",uri:"/tags/power/index.html"},{content:`PowerShell is a task automation and configuration management framework from Microsoft, consisting of a command-line shell and associated scripting language.
Multi-line commands You can use the backtick character \` to run commands across multiple lines in PowerShell.
date; \` hostname Stream input (piped from another command) git config -l | Select-String -Pattern 'email' Open a file You can use the command Invoke-Item or the shortcut version of ii to open a file. For example:
ii index.html `,description:"",tags:["scripting","windows","powershell","terminal"],title:"PowerShell",uri:"/languages/powershell/index.html"},{content:'This page provides examples about the use of PowerShell file utilities.\nGet-Content You can use Get-Content (or the alias gc) to be able to view the contents of files in the console.\nThis is particularly useful when dealing with log files.\nGet-Content | Select (head & tail) By piping the results of Get-Content through select you can get the equivalent of bash’s head and tail commands.\n# Head gc log.txt | select -first 10 # Tail (insce PSv3) and much faster than pipe through select gc -Tail 10 log.txt # Tail piping through select gc log.txt | select -less 10 # Tail with follow Get-Content log.txt -Tail 10 -Wait Get-Content | Select-String (grep) By piping the results of Get-Content through Select-String you can get the equivalent of bash’s grep command.\n# Select 2 lines before and 3 after the match Get-Content log.txt | Select-String -Pattern "match" -Context 2,3 Get-FileHash The Get-FileHash can be used to calculate the hash value of a file (or string) using a specific hash algorithm.\nFor example:\nGet-FileHash C:\\Users\\user1\\Downloads\\7z2201-x64.exe -Algorithm SHA512 | Format-List Algorithm : SHA512 Hash : 965D43F06D104BF6707513C459F18AAF8B049F4A043643D720B184ED9F1BB6C929309C51C3991D5AAFF7B9D87031A7248EE32748965 21ABE955D0E49F901AC94 Path : C:\\Users\\user1\\Downloads\\7z2201-x64.exe ',description:"",tags:["scripting","windows","powershell","terminal","utilities","grep","tail","head","file","directory"],title:"PowerShell file utilities",uri:"/languages/powershell/file_utilities/index.html"},{content:`This page provides examples about the use of PowerShell’s network utilities.
Test-NetConnection The Test-NetConnection cmdlet displays diagnostic information for a connection. It supports ping test, TCP test, route tracing, and route selection diagnostics. Depending on the input parameters, the output can include the DNS lookup results, a list of IP interfaces, IPsec rules, route/source address selection results, and/or confirmation of connection establishment.
# Test if HTTP port is open Test-NetConnection google.com -CommonTCPPort "Http" # Or define a port number Test-NetConnection google.com -Port 80 ComputerName : google.com RemoteAddress : 142.250.204.14 RemotePort : 80 InterfaceAlias : Wi-Fi SourceAddress : 192.168.0.XXX TcpTestSucceeded : True Invoke-WebRequest (curl) The Invoke-WebRequest utility can be used to perform functions similar to curl.
Request with Header Invoke-WebRequest -Uri http://example.com -Headers @{ Host = "example.com" } StatusCode : 200 StatusDescription : OK Content : <!doctype html> <html> <head> <title>Example Domain</title> <meta charset="utf-8" /> <meta http-equiv="Content-type" content="text/html; charset=utf-8" /> <meta name="viewport" conten... RawContent : HTTP/1.1 200 OK Age: 338450 Vary: Accept-Encoding X-Cache: HIT Accept-Ranges: bytes Content-Length: 1256 Cache-Control: max-age=604800 Content-Type: text/html; charset=UTF-8 Date: Tue, 23 May ... Forms : {} Headers : {[Age, 338450], [Vary, Accept-Encoding], [X-Cache, HIT], [Accept-Ranges, bytes]...} Images : {} InputFields : {} Links : {@{innerHTML=More information...; innerText=More information...; outerHTML=<A href="https://www.iana.org/domains/example">More information...</A>; outerText=More information...; tagName=A; href=https://www.iana.org/domains/example}} ParsedHtml : mshtml.HTMLDocumentClass RawContentLength : 1256 View response (Content) If you want to see all the content of an Invoke-WebRequest response pipe it through the Select-Object command. For example:
Invoke-WebRequest -Uri http://example.com -Headers @{ Host = "example.com" } \` | Select-Object -Expand Content POST requests Invoke-WebRequest -Uri http://example.com -Method POST -ContentType "application/json" \` -Body '{ "familyName": "Surname", "givenName" : "Firstname" }' \` | Select-Object -Expand Content Open the downloaded file You can use the command Invoke-Item or the shortcut version of ii to open the file after it has been downloaded. For example:
Invoke-WebRequest -Uri http://example.com -Headers @{ Host = "example.com" } \` | Select-Object -Expand Content > index.html; ii index.html `,description:"",tags:["scripting","windows","powershell","terminal","utilities","network","curl"],title:"PowerShell network utilities",uri:"/languages/powershell/network_utilities/index.html"},{content:`Pages Cascading Style Sheets (CSS)Cascading Style Sheets (CSS) is a style sheet language used for describing the presentation of a document written in a markup language such as HTML or XML (including XML dialects such as SVG, MathML or XHTML).
Command Prompt (cmd.exe)cmd.exe is the default command-line interpreter for OS/2, eComStation, Windows, Windows CE, and the ReactOS operating systems.
GoGo is a statically typed, compiled high-level programming language designed at Google.
HTMLThe HyperText Markup Language, or HTML is the standard markup language for documents designed to be displayed in a web browser.
JavaJava is a general-purpose programming language that is class-based, object-oriented, and designed to have as few implementation dependencies as possible. It is intended to let application developers write once, run anywhere, meaning that compiled Java code can run on all platforms that support Java without the need for recompilation
JavaScriptJavascript is a modern scripting language that is used in many responsive web applications.
MarkDownMarkdown is a lightweight markup language with plain-text-formatting syntax. Its design allows it to be converted to many output formats.
PowerShellPowerShell is a task automation and configuration management framework from Microsoft, consisting of a command-line shell and associated scripting language.
PythonPython is a high-level, interpreted, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically-typed and garbage-collected. It supports multiple programming paradigms, including structured, object-oriented and functional programming.
RubyRuby is an interpreted, high-level, general-purpose programming language which supports multiple programming paradigms. It was designed with an emphasis on programming productivity and simplicity. In Ruby, everything is an object, including primitive data types.
Structured Query Language (SQL)SQL (pronounced “ess-que-el”) stands for Structured Query Language. SQL is a domain-specific language used in programming and designed for managing data held in a relational database management system, or for stream processing in a relational data stream management system.
TypeScriptTypeScript is effectively a method to bring Strong typing (and more) to the JavaScript Language.
WSDLWSDL is an XML notation for describing a web service.
XMLExtensible Markup Language (XML) is a markup language and file format for storing, transmitting, and reconstructing arbitrary data.
`,description:"",tags:null,title:"Programming Languages",uri:"/languages/index.html"},{content:"",description:"",tags:null,title:"project",uri:"/tags/project/index.html"},{content:"",description:"",tags:null,title:"properties",uri:"/tags/properties/index.html"},{content:"",description:"",tags:null,title:"putty",uri:"/tags/putty/index.html"},{content:"",description:"",tags:null,title:"python",uri:"/tags/python/index.html"},{content:`Python is a high-level, interpreted, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically-typed and garbage-collected. It supports multiple programming paradigms, including structured, object-oriented and functional programming.
Python IDEs Two popular IDEs for python are Visual Studio and PyCharm.
Personally I prefer PyCharm as it has a very similar layout and keybindings to Intellij.
Python data types Python has the following data types built-in by default, in these categories:
Text Type: str Numeric Types: int, float, complex Sequence Types: list, tuple, range Mapping Type: dict Set Types: set, frozenset Boolean Type: bool Binary Types: bytes, bytearray, memoryview None Type: NoneType Boolean to String print("Something: " + str(boolean_variable)) Program flow control If Else statements a = 200 b = 33 if b > a: print("b is greater than a") elif a == b: print("a and b are equal") else: print("a is greater than b") Sub Pages Design patternsThis page contains information and simple code examples about some useful design patterns in Python Singleton Pattern A Singleton pattern in python is a design pattern that allows you to create just one instance of a class, throughout the lifetime of a program. Using a singleton pattern has many benefits. A few of them are: To limit concurrent access to a shared resource. To create a global point of access for a resource.
File operationsThis page contains information and simple code examples about using Python to interact with the file system. File exists() The OS module in Python lets us interact with the operating system. This comes under Python’s standard utility modules and gives a portable way to use the dependent functions of the operating system. The exists() function in Python exists in the os.path module, which is a submodule of the python’s OS module and is used to check if a particular file exists or not.
`,description:"",tags:["python","development"],title:"Python",uri:"/languages/python/index.html"},{content:`The Raspberry Pi is a series of small single-board computers (SBCs) developed in the United Kingdom by the Raspberry Pi Foundation in association with Broadcom.
Pages Camera Module (Raspberry Pi)Adding a camera to a Raspberry Pi opens up a new category of potential Raspberry Pi projects. Popular projects include time-lapse photography, face recognition, DIY CCTV, pet and 3D printer monitors, car cams and more.
Reducing Power consumption on a Raspberry PiThis page contains information on how to reduce the power consumption of a Raspberry Pi.
`,description:"",tags:["raspberry_pi","linux"],title:"Raspberry Pi",uri:"/hardware/raspberry-pi/index.html"},{content:"",description:"",tags:null,title:"raspberry_pi",uri:"/tags/raspberry_pi/index.html"},{content:`Create a React App Use the comand npx create-react-app <app_name> to bootstrap a basic React app.
Example: npx create-react-app my-app
Components There are two basic way to create custom components in React:
Create a class which extends React.Component - this is an older style Use a funcitonal style of declaring the component - a newer style since React version 14 Funcitonal Components Accessing Props Props are still avilable in functional components, you just need to declare a variable for them.
Example:
const Child = (props) => { return ( <div /> ) } You can then access the props throught the component as follows:
Example 1:
const Child = (props) => { return ( <div style={{backgroundColor: props.eyeColor}}/> ) } Example 2:
export const PrivateRoute = (props) => { const keycloak = userKeyCloak(); return ( <div> <h1>Use is authenticated? {keycloak && keycloak.authenticated} </div> ) } Hooks useEffect hook The Effect Hook lets you perform side effects in React functional components. Examples of side effects include fetching data, setting up subsciptions and manually changing the DOM.
By using the useEffect hook you tell React that the component needs to do something after render.
The useEffect hook also takes a list of dependencies, which you can use to trigger the useEffect hook if their state changes.
`,description:"",tags:["javascript"],title:"React",uri:"/languages/javascript/react/index.html"},{content:`Records were introduced in JDK 14 and are a new type to assist with creating ‘data carrier’ objects.
Like an enum, a record is a restricted form of a class. It’s ideal for “plain data carriers,” classes that contain data not meant to be altered and only the most fundamental methods such as constructors and accessors.
Basics A record is an immutable data class which automatically has a:
private, final field for each piece of data getter for each field public constructor with a corresponding argument for each field equals method that returns true for objects of the same class when all fields match hashCode method that returns the same value when all fields match toString method that includes the name of the class and the name of each field and its corresponding value Using records can remove a lot of boilerplate code.
Declaration To create a TransactionDto record, we’ll use the record keyword:
record TransactionDto (String queueName, String transactionName) {} The equals, hashCode, and toString methods, as well as the private, final fields and public constructor, are generated by the Java compiler.
Records can be declared at the class level or within a method.
Constructor The constructor can be used in the same way as a class to instantiate objects from the record:
TransactionDto transaction = new TransactionDto("Queue 1", "My Transaction"); Links Java 14 Record Keyword - baeldung Record Classes - Oracle `,description:"",tags:["java","development"],title:"Record",uri:"/languages/java/record/index.html"},{content:`Red Hat Enterprise Linux is a commercial Linux distribution developed by Red Hat for the commercial market.
Red Hat Enterprise Linux is released in server versions for x86-64, Power ISA, ARM64, and IBM Z and a desktop version for x86-64. Fedora Linux and CentOS Stream serve as its upstream sources.
`,description:"",tags:["operating_system","linux","fedora"],title:"Red Hat Enterprise Linux (RHEL)",uri:"/operating-systems/red_hat/index.html"},{content:`This page contains information on how to reduce the power consumption of a Raspberry Pi.
This is particularly important when you are running your Raspberry Pi on a battery.
Links Raspberry Pi Zero - Conserve power and reduce draw to 80mA How to save Power on your Raspberry Pi Power Consumption Benchmarks `,description:"",tags:["raspberry_pi","power","battery"],title:"Reducing Power consumption on a Raspberry Pi",uri:"/hardware/raspberry-pi/reducing-power-consumption/index.html"},{content:"",description:"",tags:null,title:"remote",uri:"/tags/remote/index.html"},{content:`Installing/Enabling the Pixel Desktop The PIXEL (formerly LXDE) desktop environment was removed from the RetroPie image to keep it smaller.
It can easily be installed from the RetroPie Setup Script (~/RetroPie-Setup/retropie_setup.sh)
In Configuration / Tools » Raspbiantools » Install Pixel Desktop Environment
After installation, it will be accessible from the ports menu of EmulationStation or can be called from the command line with startx
For more details see where did the desktop go
Setting up Wiimotes Look at the instructions in the RetroPie docs
`,description:"",tags:["raspberry_pi","operating_system","linux","games","wii"],title:"RetroPie",uri:"/operating-systems/retropie/index.html"},{content:"",description:"",tags:null,title:"rsync",uri:"/tags/rsync/index.html"},{content:`rsync is a fast, versatile, remote (and local) file-copying tool for linux.
Flags rsync has heaps of flags. Below are some of the ones that I have needed for my work:
-a = -rlptgoD -p = Set permissions on destination files -g = Set group on destination -o = Preserve owner Examples Copy to mapped NFS drive # Flags picked to avoid any 'Operations Not Permitted' errors rsync -rltDvz --del --force --ignore-errors /source/ /backup/ Links 16 Rsync Command Examples for Efficient File Synchronization Running rsync as root: Operations Not Permitted `,description:"",tags:["unix","linux","files"],title:"rsync",uri:"/operating-systems/linux/tools/rsync/index.html"},{content:"",description:"",tags:null,title:"ruby",uri:"/tags/ruby/index.html"},{content:`Ruby is an interpreted, high-level, general-purpose programming language which supports multiple programming paradigms. It was designed with an emphasis on programming productivity and simplicity. In Ruby, everything is an object, including primitive data types.
Ruby bundler Bundler provides a consistent environment for Ruby projects by tracking and installing the exact gems and versions that are needed.
The Gemfile.lock contains the version information for each of the gems used in the Ruby project.
Updating gems To update a specific gem use bundle update <GEM_NAME>
For example: bundle update github-pages
Use bundle info [gemname] to see where a bundled gem is installed.
Automatic Update Warning this can break your project! If you run bundle update --all, which will ignore the Gemfile.lock, it resolve all the dependencies (specified in the Gemfile) again. Keep in mind that this process can result in a significantly different set of gems, based on the requirements of new gems that the gem authors released since the last time you ran bundle update --all.
`,description:"",tags:["ruby","jekyll"],title:"Ruby",uri:"/languages/ruby/index.html"},{content:"",description:"",tags:null,title:"run",uri:"/tags/run/index.html"},{content:`This page documents my efforts to get Stable Diffusion running on my old Dell Inspiron 7537 laptop.
TL;DR The CUDA capabilities of the GeForce GT750M graphics card is only 3.0. Recent versions of PyTorch, which is required by Stable Diffusion, requires at least 3.5 to be able to run.
There may be a possible to build an old version of PyTorch which can use a 3.0 capability, but this looks to be very involved.
An alternate approach is to use only the laptops CPU to be able to generate the images.
Dell Inspiron 7537 Specs The core specifications of the Dell Inspiron 7537 (circa ~2013) which I am using are:
CPU: 4th Gen Intel Core i7-4510U processor (4M Cache up to 3.1 GHz) RAM: 16 GB Dual Channel DDR3L 1600MHz (8GB x 2) HDD: Current:Samsung SSD 850 EVO 500GB Graphics: Intel HD Graphics 4400 NVIDIA GeForge GT750M 2GB GDDR5MOD,INFO,GNRC OS: Linux Mint 21.1 (Ubuntu jammy, debian) Stable Diffusion web UI The Stable Diffusion web UI is a browser interface based on Gradio library for Stable Diffusion.
It has heaps of features and the browser interface makes it easy to run on a remote PC.
Installation The Dell laptop currently has Linux Mint (Debian-based) installed on it (due to some other projects I am working on) so we need to use the linux installation instructions.
Namely:
Install the dependencies: sudo apt update && sudo apt upgrade -y # Debian-based: sudo apt install wget git python3 python3-venv Create (and navigate to) the directory to install the webui into: # Create directory mkdir sd-webui && cd sd-webui Install using: # Install bash <(wget -qO- https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh) ################################################################ Install script for stable-diffusion + Web UI Tested on Debian 11 (Bullseye) ################################################################ ... Wait. The script will download a lot of python dependencies which will take some time ~10 - 15 minutes depending on your internet connection. Running Stable Diffusion The webui is run with the script ./webui.sh
cd stable-diffusion-webui ./webui.sh If you need to add/use any command line flags you can use the ./webui-user.sh script which has ready-made placeholders and comments for these.
Note: The webui.sh script needs to be run from the directory where is resides. Running the script from another directory will cause issues when loading any COMMANDLINE_ARGS
First run - fixing errors and warnings! Installing TCMalloc When starting the webui.sh if you see the message:
Cannot locate TCMalloc (improves CPU memory usage) You can install google-perftools to make this message go away sudo apt install --no-install-recommends google-perftools. See this github issue for more details.
RuntimeError: Torch is not able to use GPU;… On first the first run I got the following error:
RuntimeError: Torch is not able to use GPU; add --skip-torch-cuda-test to COMMANDLINE_ARGS variable to disable this check After heaps of investigation I found out that the CUDA capabilities of the GeForce GT750M graphics card is only 3.0. PyTorch, which is required by Stable Diffusion, requires a CUDA capability of at least 3.5 to be able to run.
CUDA (Compute Unified Device Architecture) is a parallel computing platform and application programming interface (API) that allows software to use certain types of graphics processing units (GPUs) for general purpose processing, an approach called general-purpose computing on GPUs (GPGPU). CUDA is a software layer that gives direct access to the GPU’s virtual instruction set and parallel computational elements, for the execution of compute kernels.
So we can configure Stable Diffusion to use only the CPU by adding some arguments to the ./webui-user.sh file:
export COMMANDLINE_ARGS="--precision full --no-half --use-cpu all --skip-torch-cuda-test --listen" export CUDA_VISIBLE_DEVICES="" This will run everything on the CPU and use the system memory, which is much slower than running on the GPU and VRAM. See below for information on testing and performance.
Testing & Performance The stable diffusion wiki has some information on optimization, unfortunately I believe much of these settings do not apply to a CPU only configuration.
Testing configuration:
Model: openjourney-v4.ckpt Prompt: glass of water on a table Sampling Steps: 20 For brevity the following required COMMANDLINE_ARGS have not been included in the table below --precision full --no-half --use-cpu all --skip-torch-cuda-test
COMMANDLINE_ARGS First run Second Run Third run Default args 19.59s/it (06:31 min/image) 19.20s/it (06:26 min/image) 19.19s/it (06:26 min/image) Default args + --opt-sdp-attention 18.70s/it (06:17 min/image) 18.68s/it (06:16 min/image) 18.69s/it (06:16 min/image) Default args + --opt-sdp-attention --opt-channelslast 19.13s/it (06:31 min/image) 19.16s/it (06:30 min/image) 19.52s/it (06:30 min/image) `,description:"",tags:["dell","Inspiron","linux","mint","stable_diffusion","image_generation","ai"],title:"Running Stable Diffusion on low resource PC (Dell Inspiron 7537)",uri:"/posts/2023-06-17-stable-diffusion-on-low-resource-pc/index.html"},{content:"",description:"",tags:null,title:"schedule",uri:"/tags/schedule/index.html"},{content:"",description:"",tags:null,title:"security",uri:"/tags/security/index.html"},{content:`This page contains information, and simple code examples, about SQL select scripts.
Count The SQL COUNT function is used to count the number of rows returned in a SELECT statement.
The syntax for the COUNT function in SQL is:
SELECT COUNT(aggregate_expression) FROM tables [WHERE conditions] Simple example:
SELECT COUNT(*) FROM Company; `,description:"",tags:["database","sql","oracle"],title:"Select Basics",uri:"/languages/sql/sql_select_basics/index.html"},{content:"",description:"",tags:null,title:"serialisation",uri:"/tags/serialisation/index.html"},{content:`Serverless is a framework which assist with building and running serverless functions (i.e. lambda).
View Debug Logs If something goes wrong when you are trying to run serverless it will advise
For debugging logs, run again after setting the "SLS_DEBUG=*" environment variable.
I found it a little hard to find out how to do this on Windows, it turns out you need to run the following command in powershell
$env:SLS_DEBUG="*" Environmental Variables You can set serverless environmental variables in the serverless.yml file in two places, in the functions section or the globally in the provider section.
Function Environment variables service: service-name provider: name: aws stage: dev functions: hello: handler: handler.hello environment: SYSTEM_URL: http://example.com/api/v1 Global Environmental variables provider: name: aws stage: dev environment: SYSTEM_ID: jdoe functions: hello: handler: handler.hello environment: SYSTEM_URL: http://example.com/api/v1 `,description:"",tags:["nodejs"],title:"Serverless",uri:"/tools/serverless/index.html"},{content:"",description:"",tags:null,title:"services",uri:"/tags/services/index.html"},{content:"",description:"",tags:null,title:"sh",uri:"/tags/sh/index.html"},{content:`sh (or the Shell Command Language) is a programming language described by the POSIX standard. It has many implementations (ksh88, dash, …). bash can also be considered an implementation of sh.
Because sh is a specification, not an implementation, /bin/sh is a symlink (or a hard link) to an actual implementation on most POSIX systems.
What is bash bash started as an sh-compatible implementation (although it predates the POSIX standard by a few years), but as time passed it has acquired many extensions. Many of these extensions may change the behavior of valid POSIX shell scripts, so by itself bash is not a valid POSIX shell. Rather, it is a dialect of the POSIX shell language.
Command line Shortcuts (and helpers) Jump to a word Ctrl + E - go to the end of the line
Ctrl + A - go to the start of the line
Alt + left - go back one word
Alt + right - go right one word
Ctrl + W - delete the last word
Search command history (reverse-i-search) You can search the command line history by using Ctrl + R then entering the search term.
From this prompt Ctrl + R will cycle backwards and Ctrl + S (if supported) will cycle forwards.
History history will display the command line history.
Find find search for files in a directory hierarchy
-newerXY <reference> Compares the timestamp of the current file with reference. The reference argument is normally the name of a file (and one of its timestamps is used for the comparison) but it may also be a string describing an absolute time. X and Y are placeholders for other letters, and these letters select which time belonging to how reference is used for the comparison. a The access time of the file reference B The birth time of the file reference c The inode status change time of reference m The modification time of the file reference t reference is interpreted directly as a time Example Find all of the xml files in that have a modification time newer than 2022-03-08
find -type f -name *.xml -newermt 2022-03-08 Mount an NFS share The information below came from here.
Firstly make sure that you have installed nfs-utils for your OS, for debian: sudo apt install nfs-common
Create the directory on the local machine where the remote file system will be mounted sudo mkdir /media/nfs
For a temporary mount you can just run the mount command:
sudo mount <hostname_or_ip_address>:/volume1/Virtual_Machines/docker/volumes /mnt/docker-volumes To remove the mount run: sudo umount /mnt/docker-volumes.
Mount at startup To mount the remote file system at startup you need to add a line to the /etc/fstab file
sudo nano /etc/fstab Add the following line to the file, replacing remote.server:/dir with the NFS server IP address or hostname and the exported directory:
# <file system> <dir> <type> <options> <dump> <pass> remote.server:/dir /media/nfs nfs defaults 0 0 Mount the NFS share by running the following command: sudo mount /media/nfs
Mount a CIFS share Install cifs-utils: sudo apt-get install cifs-utils Create the directory: mkdir /mnt/cifs Create a file to contain the credenials: nano ~/.smbcredentials Insert the details of the credientials: username=<your_username> password=<your_password> Now edit the fstab (so that it mounts on boot): sudo nano /etc/fstab Add the details of your mount: //<hostname>/shared/folder /mnt/cifs cifs credentials=/home/<user>/.smbcredentials,rw,file_mode=0777,dir_mode=0777,addr=<HOST_IP_ADDRESS>,nounix,noserverino,vers=2.0 0 0 Mount the share: mount -a Note the version vers may be different depending on the capabilities of the cifs server.
These instructions were based on the information here.
Create a new user (and add them to a group) # Create user $ sudo useradd temp_user # Set password $ sudo passwd temp_user # Add the user to a group $ sudo usermod -a -G groupname temp_user Delete a user sudo userdel temp_user Change a users UID # View the current UID $ id some_user uid=1034(some_user) gid=100(users) ... # Change the UID sudo usermod -u 1024 some_user Group ID To find out the GID (Group ID) for a group name use:
getent group 124 # mysql:x:124: getent group mysql # mysql:x:124: nohup The nohup command executes another program specified as its argument and ignores all SIGHUP (hangup) signals. SIGHUP is a signal that is sent to a process when its controlling terminal is closed.
To run a command in the background using the nohup command, type:
$ nohup <command> & nohup: ignoring input and appending output to 'nohup.out' If you log out or close the terminal, the process is not terminated.
curl POST Request The general form of the curl command for making a POST request is as follows: curl -X POST [options] [URL]
The -X option specifies which HTTP request method will be used when communicating with the remote server. Exit Code In linux a 0 exit status means the command was successful without any errors. A non-zero (1-255 values) exit status means command was a failure.
To return the last exit code use the command:
echo $? Flow Control Loops You can create a simple single line using while true; do foo; sleep 2; done
# Loop to keep printing a console output while true; do echo "Still alive at: $(date)"; sleep 60; done Ignoring the exit code of a command To ignore errors, you can use the command || true construct. This construct allows you to execute a command, and if it returns a non-zero exit status, the command following the || operator (in this case, true) will be executed instead.
# Create a group, ignoring any errors if a group with the same ID already exists addgroup -g $PGID -S backup-group || true `,description:"",tags:["sh","bash","linux","terminal"],title:"sh - Shell Command Language",uri:"/operating-systems/linux/sh/index.html"},{content:"",description:"",tags:null,title:"simulations",uri:"/tags/simulations/index.html"},{content:`In computing, sleep is a command in Unix, Unix-like and other operating systems that suspends program execution for a specified time.
Basic use sleep <NUMBER_OF_SECONDS> # For example, sleep for 5 seconds sleep 5 With time units sleep <VALUE>[TIME_UNITS] # For example, sleep for 30 minutes sleep 30m The time units used by the sleep command are:
s for seconds (the default). m for minutes. h for hours. d for days. `,description:"",tags:["unix","linux"],title:"sleep",uri:"/operating-systems/linux/tools/sleep/index.html"},{content:"",description:"",tags:null,title:"soap",uri:"/tags/soap/index.html"},{content:`Simple Object Access Protocol (SOAP) is a messaging protocol specification for exchanging structured information in the implementation of web services in computer networks.
It uses XML Information Set for its message format, and relies on application layer protocols, most often Hypertext Transfer Protocol (HTTP), although some legacy systems communicate over Simple Mail Transfer Protocol (SMTP), for message negotiation and transmission.
Identify SOAP Version Using Namespaces You can identify the SOAP version by looking at the namespaces in the message:
SOAP v1.1: http://schemas.xmlsoap.org/soap/envelope/ SOAP v1.2: http://www.w3.org/2003/05/soap-envelope Below is the sample xml message which has the envelope http://schemas.xmlsoap.org/soap/envelope/ which is SOAP v1.1.
<?xml version="1.0" encoding="UTF-8"?> <soapenv:Envelope xmlns:soapenv="http://schemas.xmlsoap.org/soap/envelope/" xmlns:xsd="http://www.w3.org/2001/XMLSchema" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"> <soapenv:Body> <listUsers xmlns="http://examples.com/" /> </soapenv:Body> </soapenv:Envelope> SOAP Action Header Typically, SOAP 1.1 will have the content type as text/xml. Below is the sample header information.
POST /YourService HTTP/1.1 Content-Type: text/xml; charset="utf-8" Content-Length: xxx SOAPAction: "urn:uuid:youraction" But SOAP 1.2 will have the content type as application/soap+xml.
`,description:"",tags:["xml","soap"],title:"SOAP",uri:"/languages/xml/soap/index.html"},{content:`The index page for about the use of Spring Boot.
Sub Pages & Topics Marshalling XML in SpringBoot SpringBoot Actuators SpringBoot Maven plugin SpringBoot Properties SpringBoot Security SpringBoot Soap WS SpringBoot Task Scheduling SpringBoot Web Services Testing in SpringBoot `,description:"",tags:["java","springboot"],title:"Spring Boot",uri:"/languages/java/spring-boot/index.html"},{content:"",description:"",tags:null,title:"springboot",uri:"/tags/springboot/index.html"},{content:`This page contains information, syntax, and simple code examples, about Springboots’ actuator.
Springboots’ actuator adds production-ready features to our application. When you add the actuator dependency functions such as monitoring, gathering metrics, understanding traffic, or the state of ths database becomes much much easier.
Adding Actuator To enable Spring Boot Actuator, just need add the spring-boot-actuator dependency to your project.
<dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-actuator</artifactId> </dependency> Endpoints There are a heap of endpoints available, but by default only the /health endpoint is exposed.
For the actuator endpoints you can:
Enable/disable them - which removes them from the application context Expose/hide them over HTTP or JMX Secure them - only the /health endpoint is unsecured over HTTP by default. Exposing If you deploy applications behind a firewall, it may be easier to expose all your actuator endpoints can be accessed without requiring authentication. You can do so by changing the management.endpoints.web.exposure.include property, as follows:
management.endpoints.web.exposure.include=* Security If Spring Security is present, you would need to also need add custom security configuration that allows unauthenticated access to the endpoints. You can do this by adding an entry for /actuator/** for example:
@Bean public SecurityWebFilterChain securityWebFilterChain(ServerHttpSecurity http) { return http.authorizeExchange() .pathMatchers("/actuator/**").permitAll() .anyExchange().authenticated() .and().build(); } Links Baeldung: Spring Boot Actuator Spring: Production-ready Features `,description:"",tags:["java","springboot","actuator","webservices","health","monitoring"],title:"SpringBoot Actuators",uri:"/languages/java/spring-boot/actuator/index.html"},{content:"This page contains information, syntax, and simple code examples, about using the spring-boot-maven-plugin.\nCreating runnable ‘fat’ jar ... <build> <plugins> <plugin> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-maven-plugin</artifactId> <version>${springboot.version}</version> <!-- Create a runnable fat jar --> <executions> <execution> <goals> <goal>repackage</goal> </goals> </execution> </executions> </plugin> </plugins> </build> ",description:"",tags:["java","springboot","maven","plugin"],title:"SpringBoot Maven plugin",uri:"/languages/java/spring-boot/maven-integration/index.html"},{content:`This page contains information, syntax, and simple code examples, about the use of properties in SpringBoot.
Adding Sources You can add property sources to SpringBoot configuration files as follows:
@Configuration @PropertySource("classpath:foo.properties") public class PropertiesWithJavaConfig { //... } Importing additional files You can also add additional configuration files using command line properties and form existing files. You can use the spring.config.import property within the application.properties or application.yml file to easily include additional files, which as the following features:
Can add several files or directories the files can be loaded either from the classpath or from an external directory indicating if the startup process should fail if a file is not found, or if it’s an optional file importing extensionless files For example:
spring.config.import=classpath:additional-application.properties, classpath:additional-application[.yml], optional:file:./external.properties, classpath:additional-application-properties/ Supplying runtime property files You can select *.properties files (including files not on the classpath) to use at application runtime by adding the spring.config.location environment property (a comma-separated list of directory locations, or file paths) to the arguments when launching the application.
# For specific files on the classpath $ java -jar myproject.jar --spring.config.location=classpath:/default.properties,classpath:/override.properties # or for an external set of file $ java -jar app.jar --spring.config.location=config/*/ Property Resolution Spring’s PropertySourcesPlaceholderConfigurer class resolves \${…} placeholders within bean definition property values and @Value annotations.
If you are having issues where the \${…} is not being substituted with the property value, make sure
PropertySourcesPlaceholderConfigurer is in the Spring context.
Using/Injecting Properties You can get the value of the property injected in your class variables using the @Value annotation for example:
@Value( "\${jdbc.url:aDefaultUrl}" ) private String jdbcUrl; You can also inject an array or list as per the following examples:
arrayOfStrings=Baeldung,dot,com // As an array @Value("\${arrayOfStrings}") private String[] arrayOfStrings; // As a List @Value("#{'\${arrayOfStrings}'.split(',')}") private List<String> listOfStrings; Default Values By default if the property key is not defined Spring will throw an exception. You can change this behaviour so that a default value is returned if the property is not set.
// Default value of "DEFAULT_VALUE" @Value("\${some.property:DEFAULT_VALUE}") private String propertyWithDefaultValue; // You can even set the value to null using SPEL // Defaults to null if not set by a property @Value("\${another.property:#{null}}") private String defaultsToNullProperty; Links For more information see:
Properties with Spring and Spring Boot Inject Arrays and Lists From Spring Properties Files Spring Expression Language (SpEL) `,description:"",tags:["java","springboot","properties"],title:"SpringBoot Properties",uri:"/languages/java/spring-boot/properties/index.html"},{content:`This page contains information, syntax, and simple code examples, about security in SpringBoot.
Spring Security <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-security</artifactId> </dependency> Configuration When Spring Security is added to a project, it will disable access to all the APIs by default. You will need to configure Spring Security to allow access to the APIs you want to expose.
@Configuration public class SecurityConfiguration { @Bean public SecurityFilterChain filterChain(HttpSecurity http) throws Exception { // Allow open access to any APIs residing under /<app_context>/rest/** return http.authorizeRequests() .antMatchers("/rest/**") .permitAll() .and() .build(); } } For more information about configuring security for different URLs see this Baeldung article.
Alternatively if the application is behind a firewall you may prefer to have a configuration that allow unauthenticated access to all the endpoints, for example:
import org.springframework.boot.actuate.autoconfigure.security.servlet.EndpointRequest; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.security.config.annotation.web.builders.HttpSecurity; import org.springframework.security.web.SecurityFilterChain; @Configuration(proxyBeanMethods = false) public class MySecurityConfiguration { @Bean public SecurityFilterChain securityFilterChain(HttpSecurity http) throws Exception { http.securityMatcher(EndpointRequest.toAnyEndpoint()); http.authorizeHttpRequests((requests) -> requests.anyRequest().permitAll()); return http.build(); } } Method Security Spring Security supports authorization semantics at the method level. Link to intro/overview
Make sure you have the spring-security-config dependency in your projects classpath:
<dependency> <groupId>org.springframework.security</groupId> <artifactId>spring-security-config</artifactId> </dependency> Next, to use the security annotations we need to enable global Method Security:
@Configuration @EnableGlobalMethodSecurity( prePostEnabled = true, securedEnabled = true, jsr250Enabled = true) public class MethodSecurityConfig extends GlobalMethodSecurityConfiguration { } `,description:"",tags:["java","springboot","security"],title:"SpringBoot Security",uri:"/languages/java/spring-boot/security/index.html"},{content:`This page contains information, syntax, and simple code examples, about creating (and using) SOAP ws services in SpringBoot.
Servlet Registration To be able to register (and expose) the servlet you will need to create a ServletRegistrationBean.
@EnableWs @Configuration public class CustomWsConfigurerAdapter extends WsConfigurerAdapter { @Bean(name = "helloServletRegistration") public ServletRegistrationBean<MessaheDisplatcherServlet> helloServletRegistration(ApplicationContext applicationContext) { MessageDisplatcherServlet servlet = new MessageDisplatcherServlet(); servlet.setApplicationContext(applicationContext); servlet.setTransformWsdlLocations(true); servlet.setTransformSchemaLocations(true); // Log the request details servlet.setEnableLoggingRequestDetails(true); return new ServletRegistrationBean<>(servlet, "/ws/*"); } } Exposing the WSDL To expose the WSDL file within your application you will need to create a bean as follows:
@EnableWs @Configuration public class CustomWsConfigurerAdapter extends WsConfigurerAdapter { /** * Publishes the WSDL to the path as defined by the bean name * e.g. <a href="http://localhost:8080/ws/helloworld.wsdl">http://localhost:8080/ws/helloworld.wsdl</a> */ @Bean(name = "helloworld") public Wsdl11Defintion helloWoldWsdlDefinition() { SimpleWsdl11Defintition wsdl11Definition = new SimpleWsdl11Defintition(); wsdl11Definition.setWsdl(new ClassPathResource("/wsdl/HelloWorld/WelloWorld.wsdl")); return wsdl11Definition; } } Exposing an external XSD schema If you WSDL imports a local schema, this can be published as well using:
@EnableWs @Configuration public class CustomWsConfigurerAdapter extends WsConfigurerAdapter { /** * Publishes the XSD to the path as defined by the bean name * e.g. <a href="http://localhost:8080/ws/schema.microsoft.com.Serialization.xsd">http://localhost:8080/ws/schema.microsoft.com.Serialization.xsd</a> */ @Bean(name = "schema.microsoft.com.Serialization") public SimpleXsdSchema serializationSchema() { return new SimpleXsdSchema(new ClassPathResource("/wsdl/HelloWorld/Serialization.xsd")); } } SOAP v1.2 Spring defaults to SOAP version 1.1 to be able to use SOAP v1.2 you need to configure the SaajSoapMessageFactory.
@EnableWs @Configuration public class CustomWsConfigurerAdapter extends WsConfigurerAdapter{ /** * Configure Spring-WS to use SOAP v1.2, rather than the default SOAP 1.1. * <br> */ @Bean(name = "soap12MessageFactory") public SaajSoapMessageFactory soapMessageFactory(){ SaajSoapMessageFactory messageFactory = new SaajSoapMessageFactory(); messageFactory.setSoapVersion(SoapVersion.SOAP_12); return messageFactory; } /** * Then use the bean name to set the message factory of your servlet */ @Bean(name = "helloServletRegistration") public ServletRegistrationBean<MessaheDisplatcherServlet> helloServletRegistration(ApplicationContext applicationContext){ MessageDisplatcherServlet servlet = new MessageDisplatcherServlet(); servlet.setApplicationContext(applicationContext); servlet.setTransformWsdlLocations(true); servlet.setTransformSchemaLocations(true); // Log the request details servlet.setEnableLoggingRequestDetails(true); // Use a specific message factory bean configured for use with SOAP v1.2 servlet.setMessageFactoryBeanName("soap12MessageFactory"); return new ServletRegistrationBean<>(servlet, "/ws/*"); } } SOAP Security SOAP security is applied in Spring by adding an Interceptor to the EndpointMapping on the server side, or the WebServiceGatewaySupport on the client.
Server side @Configuration public class SoapWsSecurityConfiguration { @Bean public CryptoFactoryBean severCryptoFactoryBean() throws Exception { CryptoFactoryBean factoryBean = new CryptoFactoryBean(); // Set the bean to use the servers trust store factoryBean.setKeyStoreLocation(new ClassPathResource(serverTruststoreLocation)); factoryBean.setKeyStorePassword(serverTruststorePassword); factoryBean.setKeyStoreType(serverTruststoreType); return factoryBean; } @Bean("serverWsSecurityInterceptor") public Wss4jSecurityInterceptor soapWsSecurityInterceptor() throws Exception { Wss4jSecurityInterceptor interceptor = new SoapWsSecurityConfiguration(); // Validate the Timestamp and Signature of the request interceptor.setValidationActions("Timestamp Signature"); interceptor.setValidationTimeToLive(300); interceptor.setValidationSignauteCrypto(severCryptoFactoryBean().getObject()); // Add Timestamp to the SOAP security header of the response interceptor.setSecurementAction("Timestamp"); // Add a digest to the signature interceptor.setSecurementSignatureDigestAlgorithm(WSS4JConstants.SHA1); return interceptor; } } @Configuration public class CustomWsConfigurationSupport extends WsConfigurationSupport { @Autowired @Qualifier("serverWsSecurityInterceptor") private Wss4jSecurityInterceptor wss4jSecurityInterceptor; /** * Apply the Wss4jSecurityInterceptor to the list of interceptors used by the Endpoints in the application */ protected void addInterceptors(List<EndpointInterceptors> interceptors){ interceptors.add(wss4jSecurityInterceptor); } } Client side @Configuration public class ClientSoapWsSecurityConfiguration { @Bean public CryptoFactoryBean clientCryptoFactoryBean() throws Exception { CryptoFactoryBean factoryBean = new CryptoFactoryBean(); // Set the bean to use the clients keystore factoryBean.setKeyStoreLocation(new ClassPathResource(clientKeyStoreLocation)); factoryBean.setKeyStorePassword(clientKeyStorePassword); factoryBean.setKeyStoreType(clientKeyStoreType); return factoryBean; } @Bean("clientWsSecurityInterceptor") public Wss4jSecurityInterceptor soapWsSecurityInterceptor() throws Exception { Wss4jSecurityInterceptor interceptor = new SoapWsSecurityConfiguration(); /* The security configuration of the outgoing request messages */ // Needs to match what the server is configured for interceptor.setSecurementAction("Timestamp Signature"); // Set the Keystore which contains the certificate for message signing interceptor.setSecurementSignatureCrypto(clientCryptoFactoryBean().getObject()); // The alias of the client certificate to use for the signing interceptor.setSecurementSignatureUser(clientCertificateAlias); // Potential values for the SecurementSignatureKeyIdentifier come from org.apache.wss4j.dom.handler.WSHandlerConstants.WSHandlerConstants.keyIdentifier interceptor.setSecurementSignatureKeyIdentifier("DirectReference"); interceptor.setSecurementSignatureAlgorithm(WSS4JConstants.RSA_SHA256); interceptor.setSecurementSignatureDigestAlgorithm(WSS4JConstants.SHA1); /* The security configuration to validate the servers response message. */ // Validate the Timestamp of the servers response interceptor.setValidationActions("Timestamp"); interceptor.setValidationTimeToLive(300); return interceptor; } } @Configuration public class ClientConfiguration { @Bean public MyClient client(Jaxb2Marshaller jaxb2Marshaller, @Qualifier("soap12MessageFactory") SaajSoapMessageFactory messageFactory, @Qualifier("clientWsSecurityInterceptor") Wss4jSecurityInterceptor wss4jSecurityInterceptor){ // MyClient extends Springs WebServiceGatewaySupport MyClient client = new MyClient(); client.setMessageFactory(messageFactory); // Add the Wss4jSecurityInterceptor to the client client.setInterceptors( new ClientInterceptor[]{ wss4jSecurityInterceptor } ); client.setMarshaller(jaxb2Marshaller); client.setUnmarshaller(jaxb2Marshaller); return client; } } Links https://www.jetbrains.com/help/idea/generating-xml-schema-from-instance-document.html https://cxf.apache.org/docs/maven-cxf-codegen-plugin-wsdl-to-java.html https://codenotfound.com/spring-ws-example.html https://spring.io/guides/gs/producing-web-service/ `,description:"",tags:["java","springboot","maven","plugin","webservice","soap","wsdl","xsd"],title:"SpringBoot Soap WS",uri:"/languages/java/spring-boot/soap-ws/index.html"},{content:`This page contains information, syntax, and simple code examples, about Springs’ Task scheduling.
Basic Example First make sure to add the @EnableScheduling annotation to one of your configuration classes:
@Configuration @EnableScheduling public class MySpringConfig{ ... } @Component public class ScheduledTasks { private static final Logger log = LoggerFactory.getLogger(ScheduledTasks.class); private static final SimpleDateFormat dateFormat = new SimpleDateFormat("HH:mm:ss"); @Scheduled(fixedRate = 5000) public void reportCurrentTime() { log.info("The time is now {}", dateFormat.format(new Date())); } } Troubleshooting My Scheduled Task runs twice Make sure that you do not use the @Scheduled annotation within a configuration class (such as one annotated with @Configuration). See the note below from the spring docs:
Make sure that you are not initializing multiple instances of the same @Scheduled annotation class at runtime, unless you do want to schedule callbacks to each such instance. Related to this, make sure that you do not use @Configurable on bean classes which are annotated with @Scheduled and registered as regular Spring beans with the container: You would get double initialization otherwise, once through the container and once through the @Configurable aspect, with the consequence of each @Scheduled method being invoked twice.
Links Baeldung: A Guide to the Spring Task Scheduler Baeldung: The @Scheduled Annotation in Spring Spring: Scheduling Tasks `,description:"",tags:["java","springboot","actuator","webservices","health","monitoring","schedule"],title:"SpringBoot Task Scheduling",uri:"/languages/java/spring-boot/scheduler/index.html"},{content:`This page contains information, syntax, and simple code examples, about Web Services in SpringBoot.
JAX-RS vs Spring MVC for REST API development JAX-RS hinges on providing a set of Java Annotations and applying them to plain Java objects. The annotations help us to abstract the low-level details of the client-server communication. To simplify their implementations, it offers annotations to handle HTTP requests and responses and bind them in the code. JAX-RS is only a specification and needs a compatible implementation to be used.
Spring MVC is a complete framework with REST capabilities. Like JAX-RS, it also provides us with useful annotations to abstract from low-level details. Its main advantage is being a part of the Spring Framework ecosystem. So it works with Spring dependency injection and the other components like Spring AOP, Spring Data REST, and Spring Security.
References:
REST API: JAX-RS vs Spring Root context The root context for the application can be defined using:
server.servlet.context-path=/some-path Error Handling Error Handling for REST with Spring Global exception handling You can use the @ControllerAdvice annotation and extend the ResponseEntityExceptionHandler class to define how you want to handle certain errors for example:
@ControllerAdvice public class RestResponseEntityExceptionHandler extends ResponseEntityExceptionHandler { @ExceptionHandler({ MyCustomApplicationException.class }) public ResponseEntity<Object> handleCustomException(Exception ex, WebRequest request) { return new ResponseEntity<Object>( "Custom Error message here", new HttpHeaders(), HttpStatus.BAD_REQUEST); } ... } Get all endpoints There are many ways to achieve this, as discussed here. But if you want to do this programmatically then you can use the @EventListener annotation to retrieve the ApplicationContext from the ContextRefreshedEvent as follows:
@EventListener public void handleContextRefresh(ContextRefreshedEvent event) { ApplicationContext applicationContext = event.getApplicationContext(); RequestMappingHandlerMapping requestMappingHandlerMapping = applicationContext .getBean("requestMappingHandlerMapping", RequestMappingHandlerMapping.class); Map<RequestMappingInfo, HandlerMethod> map = requestMappingHandlerMapping.getHandlerMethods(); map.forEach((key, value) -> LOGGER.info("{} {}", key, value)); } Get Servlet context path @Component public class SpringBean { @Autowired private ServletContext servletContext; @PostConstruct public void showIt() { System.out.println(servletContext.getContextPath()); } } or via the properties using:
@Component public class SpringBean { @Value("#{servletContext.contextPath}") private String servletContextPath; } Download a file @RestController @ReqestMapping("/rest") public class MyRestController { @GetMapping(value = "/download") @ResponseBody public ResponseEntity<ClassPathResource> downloadFile(){ HttpHeaders headers = new HttpHeaders(); headers.setContentDisposition(ContentDisposition.attachment().filename("my-file.txt").build()); return ResponseEntity.ok() .contentType(MediaType.APPLICATION_OCTLET_STREAM) .headers(headers) .body(new ClassPathResource("/my-local-file.md")); } } `,description:"",tags:["java","springboot","web","webservices"],title:"SpringBoot Web Services",uri:"/languages/java/spring-boot/web-services/index.html"},{content:"",description:"",tags:null,title:"sql",uri:"/tags/sql/index.html"},{content:"",description:"",tags:null,title:"sqlite",uri:"/tags/sqlite/index.html"},{content:"",description:"",tags:null,title:"ssh",uri:"/tags/ssh/index.html"},{content:`The Secure Shell Protocol (SSH) is a cryptographic network protocol for operating network services securely over an unsecured network. Its most notable applications are remote login and command-line execution
Generating ssh Keys A ssh key can be generated using the command ssh-keygen
Validating ssh key(s) ssh-keygen -y will prompt you for the passphrase (if there is one).
If you input the **correct** passphrase, it will show you the associated public key. If you input the **wrong** passphrase, it will display *load failed*. If the key has no passphrase, it will not prompt you for a passphrase and will immediately show you the associated public key. Information found here.
Password-less ssh logins Typically password-less logins are based around having your public key listed in the ~/.ssh/authorized_keys file on the target host.
ssh-copy-id If you are using a linux OS you can use the ssh-copy-id tool to assist with copying your ssh key to the target host. The syntax for this is ssh-copy-id <USER_ID_ON_TARGET>@<HOSTNAME>, For example:
ssh-copy-id pi@my-rpi Windows 10 OpenSSH Equivalent of ssh-copy-id The script below will copy your public key to a remote linux host, this will allow for password less logins.
type $env:USERPROFILE\\.ssh\\id_rsa.pub | ssh [user@]<IP-ADDRESS-OR-FQDN> "cat >> .ssh/authorized_keys" See this link for more information on this PowerShell command/script.
Password-less logins using PuTTY You can configure Putty to use passwordless logins.
To setup Putty you need to:
(optional) Generate a ssh key pair, if it doesn’t exist. Save/convert your private ssh key into the *.ppk (PuTTY private key file format) Open the PuTTY Key Generator program Using the Conversion –> Import key function to open your private key Using the Save private key button, save the *.ppk file Copy the public key information from the PuTTY Key Generator’s top panel to the ~/.ssh/authorized_keys file on the remote host. Update you PuTTY configuration The remote username to use: Connection –> Data –> Login Details The *ppk file generated from above: Connection –> SSH –> Auth –> Authentication parameters Save the configuration changes Test! The steps above was mostly inspired from the information here
`,description:"",tags:["ssh","bash","sh","powershell","remote","putty"],title:"ssh - Secure Shell Protocol",uri:"/tools/ssh/index.html"},{content:"",description:"",tags:null,title:"ssl",uri:"/tags/ssl/index.html"},{content:`Stable Diffusion is a deep learning, text-to-image model released in 2022. It is primarily used to generate detailed images conditioned on text descriptions, though it can also be applied to other tasks such as inpainting, outpainting, and generating image-to-image translations guided by a text prompt.
It was developed by the start-up Stability AI in collaboration with a number of academic researchers and non-profit organizations.
User Interface There are a variety of user interfaces that you can use for interacting with Stable Diffusion. Personally I think that AUTOMATIC1111’s WebUI is among one of the best.
Models The table below provides an overview of the models that I have used:
Model Description Realistic Vision Good for photorealistic images but requires lots of prompt engineering and higher step count to get good images Anything V3 Good for creation of “anime” style images Deliberate Creates images which blend digital art and realism. Requires good prompt engineering. Protogen x3.4 OpenJourney v4 OpenJourney is a model inspired by Midjourney, a Discord-based AI that was used to generate artwork. f222 F222 is trained originally for generating nudes, but people found it helpful in generating beautiful female portraits with correct body part relations. Modelshoot incredibly realistic images created from simple prompts. Look like they’ve been shot by a real photographer. Links:
Top 10 Best Stable Diffusion Models You Need to Try Beginner’s guide to Stable Diffusion models and the ones you should know Capabilities Outpainting https://stable-diffusion-art.com/outpainting/
Creating a Systemd service If you would like to make sure stable diffusion is available all the time, including after a system restart, you can create a systemd service to run it as a service.
Create the service file /etc/systemd/system/stable-diffusion.service with sudo nano /etc/systemd/system/stable-diffusion.service and add the following text to the file:
[Unit] Description=Start Stable Diffusion. After=network.target [Service] Type=simple WorkingDirectory=<SD_DIRECTORY>/stable-diffusion-webui ExecStart=<SD_DIRECTORY>/stable-diffusion-webui/webui.sh RemainAfterExit=true ExecStop=/usr/bin/pkill -f launch.py StandardOutput=journal StandardError=journal User=<USER_TO_RUN_SD> Group=<USERS_GROUP> [Install] WantedBy=multi-user.target Then:
# Reload the systemctl daemon to detect the new file/service $ sudo systemctl daemon-reload # Start the service $ sudo systemctl start stable-diffusion # And check its status $ sudo systemctl status stable-diffusion ● stable-diffusion.service - Start Stable Diffusion. Loaded: loaded (/etc/systemd/system/stable-diffusion.service; disabled; vendor preset: enabled) Active: active (running) since Sat 2023-06-24 09:41:19 AEST; 1min 3s ago Main PID: 383419 (python3) Tasks: 20 (limit: 18802) Memory: 6.9G CPU: 15.748s CGroup: /system.slice/stable-diffusion.service ├─383419 python3 launch.py ├─383532 git cat-file --batch-check ├─383533 git cat-file --batch ├─383535 git cat-file --batch-check └─383536 git cat-file --batch # To make the service run on system start $ sudo systemctl enable stable-diffusion Created symlink /etc/systemd/system/multi-user.target.wants/stable-diffusion.service → /etc/systemd/system/stable-diffusion.service. `,description:"",tags:["python","stable_diffusion","image_generation","ai"],title:"Stable Diffusion",uri:"/tools/stable-diffusion/index.html"},{content:`This page is about Stable Diffusion extensions. In particular extensions to AUTOMATIC1111’s webui
`,description:"",tags:["python","stable_diffusion","image_generation","ai","extensions"],title:"Stable Diffusion Extensions",uri:"/tools/stable-diffusion/extensions/index.html"},{content:"",description:"",tags:null,title:"stable_diffusion",uri:"/tags/stable_diffusion/index.html"},{content:"",description:"",tags:null,title:"strings",uri:"/tags/strings/index.html"},{content:`This page provides examples about the use of PowerShell strings and string functions.
Select-String (i.e. Grep for Powershell) The Select-String command is the PowerShell equivalent to grep in linux.
Examples:
# Select 2 lines before and 3 after the match Select-String -Pattern "match" -Context 2,3 # From a file input Select-String -Path "Users\\*.csv" -Pattern "Joe" -NotMatch You can also use the -NotMatch flag to return the strings that do not match the pattern.
Select-String -Pattern "#" -NotMatch See here for more information and examples.
Concatenation The + operator is used to concatenate strings in Powershell, for example:
$firstName = "Shell" $lastName = "Geek" $fullName = $firstName + $lastName Write-Host "Concatenate String:" $fullName For more information see shellgeek.com
Replace There are a few methods of String replacement
Using the Replace() Method $string = 'hello, world' PS> $string.replace('hello','hi') hi, world Using the PowerShell Replace Operator $string = 'hello, world' PS> $string -replace 'hello','hi' hi, world Null or Whitespace check if (![string]::IsNullOrWhiteSpace($MyVariable)) { Write-Host "Variable is null or whitespace" } else { Write-Host "Variable has a value of: $MyVariable" } StartsWith if ($MyVariable.StartsWith("#")) { Write-Host "String starts with '#'" } `,description:"",tags:["scripting","windows","powershell","terminal","strings"],title:"Strings (PowerShell)",uri:"/languages/powershell/strings/index.html"},{content:`SQL (pronounced “ess-que-el”) stands for Structured Query Language. SQL is a domain-specific language used in programming and designed for managing data held in a relational database management system, or for stream processing in a relational data stream management system.
Sub Pages LOBsThis page contains information and simple code examples about dealing with LOBs (CLOBs and BLOBs) in SQL statements. CLOB/BLOB length To find out the length of a LOB object you can use the DBMS_LOB.getLength function: Select up.*, DBMS_LOB.getLength(up.stage_layout) as "BLOB_LENGTH" from ui_properties up where up.stage_layout is not null order by "BLOB_LENGTH" desc; Converting BLOB to CLOB - No truncation Unfortunately there is no built in function that will do this conversion that does not have a length restriction on it.
Select BasicsThis page contains information, and simple code examples, about SQL select scripts.
Table BasicsThis page contains information, and simple code examples, about creating and modifying database tables with SQL.
`,description:"",tags:["development","database","sql"],title:"Structured Query Language (SQL)",uri:"/languages/sql/index.html"},{content:"",description:"",tags:null,title:"svn",uri:"/tags/svn/index.html"},{content:`Subversion (SVN) is an open source version control system developed as a project of the Apache Software Foundation.
TortoiseSVN is an Apache™ Subversion (SVN)® client, implemented as a Windows shell extension. It’s intuitive and easy to use, since it doesn’t require the Subversion command line client to run. And it is free to use, even in a commercial environment.
Status Use the svn status command to print the status of working copy files and directories. svn status [PATH...]
For example:
PS> svn status ? README.md ? package-lock.json M src\\App.js PS> svn --no-ignore status I .ideas ? README.md I node_modules ? package-lock.json M src\\App.js Viewing properties You can view the properties on a particular folder, file or revsision using the snv proplist command: svn proplist [options] [target]
For example:
snv proplist -v . - list the properties for the current folder Setting a property You can set a property using the svn propset command: svn propset PROPNAME [PROPVAL | -F VALFILE] PATH
For example:
svn propset svn:gloabl-ignores -F .\\ignore.txt . - will globally ignore the files and folders listed in the .\\ignore.txt file Editing properties You can use the svn propedit command to edit properties. svn propedit PROPNAME TARGET...
For example:
svn propedit svn:gloabl-ignores --editor-cmd notepad . - Uses the Windows notepad app to edit the contents of the svn:gloabl-ignores property. Ignoring files There are several ways to ignore files in SVN, depending on which version you are using. Below is a simple example of ignoring a folder (and its contents) for a recent version (1.8 and higher) of SVN:
Examples:
svn propset svn:gloabl-ignores node_modules . - will globally ignore the node_modules folder and its contents. `,description:"",tags:["svn","version_control"],title:"SVN",uri:"/tools/svn/index.html"},{content:"",description:"",tags:null,title:"system",uri:"/tags/system/index.html"},{content:"Examples of how you get information about your linux system and hardware.\nWhat version of Linux am I running? To find out what OS version you are running use the following command:\ncat /etc/os-release Graphics card/GPU information You can use the command hwinfo --gfxcard --short or sudo lshw -C display to display information about the graphics card(s) in your system. For example:\n$ hwinfo --gfxcard --short graphics card: nVidia GK107M [GeForce GT 750M] Intel Haswell-ULT Integrated Graphics Controller Primary display adapter: #21 #OR $ sudo lshw -C display *-display description: VGA compatible controller product: Haswell-ULT Integrated Graphics Controller vendor: Intel Corporation physical id: 2 bus info: pci@0000:00:02.0 logical name: /dev/fb0 version: 0b width: 64 bits clock: 33MHz capabilities: msi pm vga_controller bus_master cap_list rom fb configuration: depth=32 driver=i915 latency=0 resolution=1920,1080 resources: irq:48 memory:e3000000-e33fffff memory:c0000000-cfffffff ioport:5000(size=64) memory:c0000-dffff *-display UNCLAIMED description: 3D controller product: GK107M [GeForce GT 750M] vendor: NVIDIA Corporation physical id: 0 bus info: pci@0000:04:00.0 version: a1 width: 64 bits clock: 33MHz capabilities: pm msi pciexpress bus_master cap_list configuration: latency=0 resources: memory:e2000000-e2ffffff memory:d0000000-dfffffff memory:e0000000-e1ffffff ioport:3000(size=128) PCI information $ lspci 00:00.0 Host bridge: Intel Corporation Haswell-ULT DRAM Controller (rev 0b) 00:02.0 VGA compatible controller: Intel Corporation Haswell-ULT Integrated Graphics Controller (rev 0b) 00:03.0 Audio device: Intel Corporation Haswell-ULT HD Audio Controller (rev 0b) 00:14.0 USB controller: Intel Corporation 8 Series USB xHCI HC (rev 04) 00:16.0 Communication controller: Intel Corporation 8 Series HECI #0 (rev 04) 00:1b.0 Audio device: Intel Corporation 8 Series HD Audio Controller (rev 04) 00:1c.0 PCI bridge: Intel Corporation 8 Series PCI Express Root Port 1 (rev e4) 00:1c.2 PCI bridge: Intel Corporation 8 Series PCI Express Root Port 3 (rev e4) 00:1c.3 PCI bridge: Intel Corporation 8 Series PCI Express Root Port 4 (rev e4) 00:1c.4 PCI bridge: Intel Corporation 8 Series PCI Express Root Port 5 (rev e4) 00:1d.0 USB controller: Intel Corporation 8 Series USB EHCI #1 (rev 04) 00:1f.0 ISA bridge: Intel Corporation 8 Series LPC Controller (rev 04) 00:1f.2 SATA controller: Intel Corporation 8 Series SATA Controller 1 [AHCI mode] (rev 04) 00:1f.3 SMBus: Intel Corporation 8 Series SMBus Controller (rev 04) 02:00.0 Network controller: Intel Corporation Wireless 7260 (rev 73) 03:00.0 Unassigned class [ff00]: Realtek Semiconductor Co., Ltd. RTL8411B PCI Express Card Reader (rev 01) 03:00.1 Ethernet controller: Realtek Semiconductor Co., Ltd. RTL8111/8168/8411 PCI Express Gigabit Ethernet Controller (rev 12) 04:00.0 3D controller: NVIDIA Corporation GK107M [GeForce GT 750M] (rev a1) ",description:"",tags:["linux","monitoring","system","hardware"],title:"System Information (Linux)",uri:"/operating-systems/linux/system_info/index.html"},{content:`Systemd is a suite of basic building blocks for a Linux system. It provides a system and service manager that runs as PID 1 and starts the rest of the system.
Of particular interest to users is its use as an init system used to bootstrap user space and manage user processes.
Handy Commands Command Description systemctl status <SERVICE_NAME> Displays the status of the service systemctl start <SERVICE_NAME> Starts the service systemctl stop <SERVICE_NAME> Stops the service systemctl restart <SERVICE_NAME> Restarts the service systemctl enable <SERVICE_NAME> Enables the service to be started on boo sudo systemctl daemon-reload Reload the Systemd daemon to detect new unit files Creating a service Systemd uses *.service files, known as unit files located in the /etc/systemd/system/ directory.
TODO details of how to create a service/unit file….
`,description:"",tags:["operating_system","linux","services"],title:"systemd",uri:"/operating-systems/linux/systemd/index.html"},{content:`This page contains information, and simple code examples, about creating and modifying database tables with SQL.
Create Table The syntax for the CREATE TABLE statement in Oracle/PLSQL is:
CREATE TABLE table_name ( column1 datatype [ NULL | NOT NULL ], column2 datatype [ NULL | NOT NULL ], ... column_n datatype [ NULL | NOT NULL ] ); A basic Oracle CREATE TABLE example.
CREATE TABLE COMPANY_TYPE ( COMPANY_TYPE_ID NUMBER(38,0) NOT NULL, CODE varchar2(10) NOT NULL, DESCRIPTION varchar2(50), CONSTRAINT COMPANY_TYPE_PK PRIMARY KEY (COMPANY_TYPE_ID) ); For more detailed information TechOnTheNet has a great reference page.
Insert The INSERT statement is used to insert a single record or multiple records into a table.
The syntax for the Oracle INSERT statement when inserting a single record using the VALUES keyword is:
INSERT INTO table (column1, column2, ... column_n ) VALUES (expression1, expression2, ... expression_n ); INSERT INTO COMPANY_TYPE (COMPANY_TYPE_ID, CODE, DESCRIPTION) VALUES (COMPANY_TYPE_SEQ.nextval, 'COMPANY', 'A corporation where the ownership is divided into shares'); Delete To delete one or more rows from a table, you use the Oracle DELETE statement as follows:
DELETE FROM table_name WHERE condition; WARNING: If you omit the WHERE clause, the Oracle DELETE statement removes all rows from the table.
Alter Table The ALTER TABLE statement to add a column, modify a column, drop a column, rename a column, add a constraint or rename a table.
Add a Column The syntax To ADD A COLUMN in a table, the Oracle ALTER TABLE syntax is:
ALTER TABLE table_name ADD column_name column_definition; Drop a column For one, or more columns use):
ALTER TABLE table_name drop (col_name1, col_name2); Add foreign key constraint ALTER TABLE table_name ADD CONSTRAINT constraint_name FOREIGN KEY (column1, column2, ... column_n) REFERENCES parent_table (column1, column2, ... column_n); `,description:"",tags:["database","tables","sql","oracle"],title:"Table Basics",uri:"/languages/sql/sql_table_basics/index.html"},{content:"",description:"",tags:null,title:"tables",uri:"/tags/tables/index.html"},{content:"",description:"",tags:null,title:"tail",uri:"/tags/tail/index.html"},{content:"",description:"",tags:null,title:"terminal",uri:"/tags/terminal/index.html"},{content:"",description:"",tags:null,title:"test",uri:"/tags/test/index.html"},{content:"",description:"",tags:null,title:"testing",uri:"/tags/testing/index.html"},{content:`This page contains information, syntax, and simple code examples, about testing in SpringBoot.
SpringBoot has a heap of annotations which can assist with the creation of test code.
@SpringBootTest The @SpringBootTest annotation can be used to bootstrap the full application context. This means that you can use the @Autowire annotation to inject beans from the application context into the test class.
@SpringBootTest public class EmployeeServiceTest { @Autowired private EmployeeService employeeService; // test code ... } Restricting the (application) context If you don’t need the entire application context you can specify the classes within the annotation using;
@SpringBootTest(classes = {MyConfig.class}) public class MyConfigTest { @Autowired private MyConfig config; // test code ... } @TestConfiguration You can use the @TestConfiguration annotation to load a particular configuration for a test.
There are two ways of using the annotation. Either on a static inner class in the same test class where we want to @Autowire the bean;
@SpringBootTest public class EmployeeServiceTest { @TestConfiguration static class EmployeeServiceTestConfiguration { @Bean public EmployeeService employeeService() { // test specific logic return new EmployeeService() { }; } } @Autowired private EmployeeService employeeService; } Or you can create a separate test configuration class:
@TestConfiguration public class EmployeeServiceTestConfiguration { @Bean public EmployeeService employeeService() { // test specific logic return new EmployeeService() { }; } } and then use @import on the test classes where you want to use it:
@SpringBootTest @Import(EmployeeServiceTestConfiguration.class) public class EmployeeServiceTest { @Autowired private EmployeeService employeeService; // remaining test code } `,description:"",tags:["java","springboot","test","testing"],title:"Testing in SpringBoot",uri:"/languages/java/spring-boot/testing/index.html"},{content:"",description:"",tags:null,title:"theme",uri:"/tags/theme/index.html"},{content:"",description:"",tags:null,title:"timelapse",uri:"/tags/timelapse/index.html"},{content:"",description:"",tags:null,title:"tomcat",uri:"/tags/tomcat/index.html"},{content:"",description:"",tags:null,title:"tools",uri:"/tags/tools/index.html"},{content:`This page contains some information about some tools that help with JavaScript development.
Code Formatting Prettier is a popular code formatting tool which can automatically apply white spaces to make javascript more readable.
Dynamic Imports Browser support for Dynamic Imports is still a bit flakey so tools (such as webpack and rollup.js) have been written to help with this.
Package Managers Package managers help with downloading all the required libraries to run your JavaScript code.
Two popular package managers are npm and yarn.
Transpilers and Compliers If you want to use ECMAScript 2015 (or newer) or [TypeScript]({% link _languages/typescript.md %}) you will need a Transpiler to convert your JavaScript code into something that most browsers can handle.
Babel is a popular JavaScript Transpiler/Compiler which can convert JavaScript to be ES5 compatible.
`,description:"",tags:["javascript"],title:"Tools",uri:"/languages/javascript/tools/index.html"},{content:`This page contains information about handy (or popular) Typescript tools.
Transpilers and Compliers TypeScript has its own complier to convert TypeScript into vanilla JavaScript.
`,description:"",tags:["typescript"],title:"Tools",uri:"/languages/typescript/tools/index.html"},{content:`This page contains links to the tools that I have discovered, and documented, as a part of my development journey.
Pages AnsibleAnsible provides open-source automation that reduces complexity and runs everywhere.
Apache HTTP server (httpd)The Apache HTTP Server is a free and open-source cross-platform web server software, released under the terms of Apache License 2.0.
AWS Command Line Interface (CLI)The AWS Command Line Interface (CLI) is a unified tool to manage AWS services. Using the tool you can control multiple AWS services from the command line and automate them through scripts.
DockerDocker is a set of platform as a service products that use OS-level virtualization to deliver software in packages called containers. The service has both free and premium tiers. The software that hosts the containers is called the Docker Engine.
gitGit is a distributed version-control system for tracking changes in source code during software development. It is designed for coordinating work among programmers, but it can be used to track changes in any set of files. Its goals include speed, data integrity, and support for distributed, non-linear workflows.
GithubGithub is a popular public Source Code Repository (SRC) which was recently purchased by Microsoft?. As the name suggests Github uses git for use of the repository.
H2 DatabaseH2 is a lightweight Java SQL database.
HoverflyHoverfly is a tool written in go which can be used to simulate API.
HugoHugo is a very fast Static Site Generator (SSG) built with Go.
IntelliJIntelliJ IDEA is an integrated development environment written in Java for developing computer software. It is developed by JetBrains, and is available as an Apache 2 Licensed community edition, and in a proprietary commercial edition. Both can be used for commercial development.
JekyllJekyll is a Ruby tool which can generate static websites and blogs using plain text files.
Jenkins
KubernetesThis page and it’s children contain information about Kubernetes that I have discovered, and documented.
MavenMaven is a build automation tool used primarily for Java projects. Maven can also be used to build and manage projects written in C#, Ruby, Scala, and other languages. The Maven project is hosted by the Apache Software Foundation, where it was formerly part of the Jakarta Project.
NginxNginx (pronounced “engine x” /ˌɛndʒɪnˈɛks/ EN-jin-EKS, stylized as NGINX) is a web server that can also be used as a reverse proxy, load balancer, mail proxy and HTTP cache.
openHABOpen Home Automation Bus (openHAB) is an open source home automation software written in Java. It is deployed on premises and connects to devices and services from different vendors.
OpenSSLOpenSSL is a software library for applications that provide secure communications over computer networks against eavesdropping or need to identify the party at the other end.
Oracle XE (Express Edition)Oracle XE (Express Edition) is the same powerful Oracle Database that enterprises rely on worldwide, packaged for simple download, ease-of-use, and a full-featured experience. You get an Oracle Database to use in any environment, plus the ability to embed and redistribute – all completely free!
Pi-holePi-hole is a Linux network-level advertisement and Internet tracker blocking application which acts as a DNS sinkhole and optionally a DHCP server, intended for use on a private network. It is designed for low-power embedded devices with network capability, such as the Raspberry Pi, but can be installed on almost any Linux machine.[
ServerlessServerless is a framework which assist with building and running serverless functions (i.e. lambda).
ssh - Secure Shell ProtocolThe Secure Shell Protocol (SSH) is a cryptographic network protocol for operating network services securely over an unsecured network. Its most notable applications are remote login and command-line execution
Stable DiffusionStable Diffusion is a deep learning, text-to-image model released in 2022. It is primarily used to generate detailed images conditioned on text descriptions, though it can also be applied to other tasks such as inpainting, outpainting, and generating image-to-image translations guided by a text prompt.
SVNSubversion (SVN) is an open source version control system developed as a project of the Apache Software Foundation.
TortoiseSVN is an Apache™ Subversion (SVN)® client, implemented as a Windows shell extension. It’s intuitive and easy to use, since it doesn’t require the Subversion command line client to run. And it is free to use, even in a commercial environment.
TraefikTraefik is a modern HTTP reverse proxy and load balancer that makes deploying microservices easy.
Unreal EngineTODO
Wireguard (VPN)WireGuard® is an extremely simple yet fast and modern VPN that utilizes state-of-the-art cryptography.
`,description:"",tags:null,title:"Tools",uri:"/tools/index.html"},{content:"",description:"",tags:null,title:"traefik",uri:"/tags/traefik/index.html"},{content:`Traefik is a modern HTTP reverse proxy and load balancer that makes deploying microservices easy.
Traefik integrates with your existing infrastructure components (Docker, Swarm mode, Kubernetes, Marathon, Consul, Etcd, Rancher, Amazon ECS, …) and configures itself automatically and dynamically.
Configuration There are three different, mutually exclusive (e.g. you can use only one at a time), ways to define static configuration options in Traefik:
In a configuration file In the command-line arguments As environment variables These ways are evaluated in the order listed above. `,description:"",tags:["container","kubernetes","docker","development"],title:"Traefik",uri:"/tools/traefik/index.html"},{content:`This page contains some information about the TypeScript transpiler.
tsconfig.json The presence of a tsconfig.json file in a directory indicates that the directory is the root of a TypeScript project. The tsconfig.json file specifies the root files and the transpiler options to use when transpiling the project into JavaScript.
target property Sets the ES version to transpile the TypeScript code to when
tsc tsc is the command to start the TypeScript Transpiler.
Watch mode ( -w flag) You can start the transpiler in watch mode which will watch your files and re-compile them into JavaScript code when it detects changes.
PS > tsc -w `,description:"",tags:["typescript"],title:"Transpiler",uri:"/languages/typescript/transpiler/index.html"},{content:"",description:"",tags:null,title:"typescript",uri:"/tags/typescript/index.html"},{content:`TypeScript is effectively a method to bring Strong typing (and more) to the JavaScript Language.
TypeScript is a Typed superset of JavaScript.
It adds helpful and productive features without changing JavaScript, and is often used in larger or more complex projects.
Static typing TypeScript brings static typing to the JavaScript language, this means:
Variables are assigned a type, and that type never changes. Fields must be predefined and can’t be added manually. Static typing enables better tooling (refactoring, autocomplete etc.) and catches error at compile time, rather than runtime.
It also better enables the Developer to convey their intent of their code.
The TypeScript Transpiler The transpiler converts TypeScript code into JavaScript. As a part of this process it strips out all of the type decorations out of the code.
Initialising a TypeScript Project Dependencies First, you need TypeScript installed of course. You can decide to have it installed globally on your machine, or locally to your project.
And to install locally, run:
PS > npm i typescript --save-dev Setup To initialize a TypeScript project, simply use the --init flag.
PS > tsc --init After running tsc with the –init flag, a tsconfig.json will be added to your project folder with a few sensible defaults and an extensive list of commented-out possible configurations.
Sub Pages Basic SyntaxThis page contains information about the basics of TypeScript syntax. Describing your code with types The general rule of thumb about using types is the more information you can provide the more the tooling will be able to help you. Variables Simply add a semicolon after the variable name and then the type. let trackingNumber: string = 'FD1234567'; let createDate: Date = new Date(); Functions and parameters function getItem(trackingNumber: string): object { // Do stuff in here.
ToolsThis page contains information about handy (or popular) Typescript tools. Transpilers and Compliers TypeScript has its own complier to convert TypeScript into vanilla JavaScript.
TranspilerThis page contains some information about the TypeScript transpiler. tsconfig.json The presence of a tsconfig.json file in a directory indicates that the directory is the root of a TypeScript project. The tsconfig.json file specifies the root files and the transpiler options to use when transpiling the project into JavaScript. target property Sets the ES version to transpile the TypeScript code to when tsc tsc is the command to start the TypeScript Transpiler.
`,description:"",tags:["typescript"],title:"TypeScript",uri:"/languages/typescript/index.html"},{content:`Ubuntu is a Linux distribution based on Debian and composed mostly of free and open-source software.
Ubuntu is officially released in three editions: Desktop, Server, and Core for Internet of things devices and robots. All the editions can run on the computer alone, or in a virtual machine.
Ubuntu on Raspberry Pi USB HDD Support Version 20.10 supports booting from SSD “out-of-the-box”.
`,description:"",tags:["operating_system","linux","debian","raspberry_pi"],title:"Ubuntu",uri:"/operating-systems/ubuntu/index.html"},{content:`This page contains some information about common, or handy, UI (browser) components in Javascript.
The window Object There is a global window object available when working in a browser, all global variables are properties of the window object.
Confirmation Window The confirmation window is available in every browser and will create a simple popup dialog requesting the user to select a button.
var answer = window.confirm("Click OK, get true. Click CANCEL, get false."); Prompt Window The prompt window allows shows a popup dialog which can capture a string input from the user.
var answer = window.prompt("Type YES, NO or MAYBE. Then click OK."); setTimeout() method The setTimeout() method of the WindowOrWorkerGlobalScope mixin (and successor to Window.setTimeout()) sets a timer which executes a function or specified piece of code once the timer expires.
var timeoutID; function delayedAlert() { timeoutID = window.setTimeout(window.alert, 2*1000, 'That was really slow!'); } function clearAlert() { window.clearTimeout(timeoutID); } The document object The document object is the interface that the browser provides so that you can interact with what is on the page.
document properties The document object has the following handy properties:
title - the title of the page `,description:"",tags:["javascript"],title:"UI Components",uri:"/languages/javascript/ui_components/index.html"},{content:"",description:"",tags:null,title:"unix",uri:"/tags/unix/index.html"},{content:`vi is a screen-oriented text editor originally created for the Unix operating system.
Handy Commands`,description:"",tags:["unix","linux"],title:"Unix VI text editor",uri:"/operating-systems/linux/tools/vi/index.html"},{content:"",description:"",tags:null,title:"unmarshalling",uri:"/tags/unmarshalling/index.html"},{content:`TODO
Terms Blendspace - a way to blend different annimations together Animatiom Blue Print - Allows you to use animations within a Blue Print Socket - Allows you to attached weapson/items to a skeliton
Cast - Allows you to access variables from another class
Animation Notifies - Used to add sounds and effects (such as particle effects) to an animation Amimation Montage - Allows you to call animations from a Blue Print, animation montage also handles the blending of the animation with the characters current movements. ** To play montage you need to add a Default slot to your AnimGraph ** Widgets - Used to add user interface on your character Physics Asset - A group of different capsuals around a skeletal mesh Physical Material/Surface - an object that can be assigned to surfaces in order to differentiate how hits/collisions effects are handled
Map Editor Shortcut Description Ctrl + D Duplicate the current item SPACE Cycle through the placement/rotation/scale widgets Play In Editor (PIE) Play In Editor (PIE) starts the game at the Player Start location and give you control of the player character.
Keyboard shortcuts Shortcut Description Alt + P Starts PIE mode Shift + F1 Unlocks the mouse cursor from the PIE window P (In Level editor) Shows collisions Blueprints Handy Hints & Tips Task/Action How to Remove a node link Alt + click on input or output point of the line Split a node link Double click on the node link Handy Nodes Node name Description Link(s) Is Valid Checks that the variable/object is empty/null Format Text Allows you to format text (and Strings) Formatting Animation Blue Prints Make sure you have a Default Slot before your output pose so that you can play animation montages, such as death and attack montages.
Animation Montage Death Montage Make sure you uncheck the Enable Auto Blend out for death montages to prevent the animation resetting to the default pose after the death animation has been played.
Audio Attenuation If your attenuation is not working correctly you may need to set the Virtualization Mode of the audio to Play when Silent.
Sound Wave –> Details –> Voice Management –> Virtualization Mode –> Play when Silent
Collisions Projectile collisions If you are having trouble getting the Pysical Surface from the Phys Mat of your Hit. Enabled the Return Material on Move checkbox of your projectile Blueprint.
Projectile_BP –> Collision –> Advanced –> Return Material on Move (Ticked)
Source
Troubleshooting Assertion failed: WorldPartition This is likely due to the engine unable to open the default map. Open the <game_dir>/Config/DefaultEngine.ini and set EditorStartupMap= to nothing.
See: Stackoverflow
Complex collisions only return default physical material This might be due to a bug in UE but the work around is to set the Phys Material Override on the Static Mesh Component in the Map/level.
Links Tutorials Create a Zombie First Person Shooter Game | Unreal Engine 5 Beginner Tutorial `,description:"",tags:["development","gaming"],title:"Unreal Engine",uri:"/tools/unreal_engine/index.html"},{content:"",description:"",tags:null,title:"utilities",uri:"/tags/utilities/index.html"},{content:"",description:"",tags:null,title:"variables",uri:"/tags/variables/index.html"},{content:`Ansible Vault encrypts variables and files so you can protect sensitive content such as passwords or keys rather than leaving it visible as plaintext in playbooks or roles.
The examples below use a single password (and vault) which is requested when the command is executed. This is fine for simple use-cases. There are other was to deal with vaults and passwords which are explained here.
Encrypting a variable echo -n 'password' | ansible-vault encrypt_string --ask-vault-pass --stdin-name 'vault_db_user_password' Using an encrypted variable ... ansible_become_password: "{{ vault_db_user_password }}" View encrypted variable (from a file) Syntax: ansible localhost -m ansible.builtin.debug -a var="<VARIABLE_NAME>" -e "@<PATH_TO_FILE>" --ask-vault-pass
ansible localhost -m ansible.builtin.debug -a var="vault_db_user_password" -e "@group_vars/all/vault.yaml" --ask-vault-pass Running a playbook with encrypted variables ansible-playbook --ask-vault-pass -i inventory.yaml my-first-playbook.yaml `,description:"",tags:["ansible","password","encrypt"],title:"Vault",uri:"/tools/ansible/vault/index.html"},{content:"",description:"",tags:null,title:"version_control",uri:"/tags/version_control/index.html"},{content:`Versions ECMASscript 5 (also known as ES5) is compatible with all major web browers.
There are newer versions (ECMAScript 2015 a.k.a. ES6) released every year, but to maintain backwards compatibility you may need to use a Transpiler to convert it into ES5 or ES6.
Transpilers bablejs.io is a popular Transpiler.
`,description:"",tags:["javascript"],title:"Versions and Transpilers",uri:"/languages/javascript/versions/index.html"},{content:"",description:"",tags:null,title:"vpn",uri:"/tags/vpn/index.html"},{content:"",description:"",tags:null,title:"web",uri:"/tags/web/index.html"},{content:"",description:"",tags:null,title:"webserver",uri:"/tags/webserver/index.html"},{content:"",description:"",tags:null,title:"webservice",uri:"/tags/webservice/index.html"},{content:"",description:"",tags:null,title:"webservices",uri:"/tags/webservices/index.html"},{content:"",description:"",tags:null,title:"wii",uri:"/tags/wii/index.html"},{content:"",description:"",tags:null,title:"windows",uri:"/tags/windows/index.html"},{content:`Microsoft Windows is a group of several proprietary graphical operating system families developed and marketed by Microsoft.
Handy links Changing the Bluetooth name on a Windows computer `,description:"",tags:["operating_system","windows"],title:"Windows",uri:"/operating-systems/windows/index.html"},{content:"",description:"",tags:null,title:"wireguard",uri:"/tags/wireguard/index.html"},{content:`WireGuard® is an extremely simple yet fast and modern VPN that utilizes state-of-the-art cryptography.
It aims to be faster, simpler, leaner, and more useful than IPsec, while avoiding the massive headache. It intends to be considerably more performant than OpenVPN.
Concepts It helps to think of WireGuard primarily as a network interface, like any other. It will have the usual attributes, like IP address, CIDR, and there will be some routing associated with it. But it also has WireGuard specific attributes, which handle the VPN part of things.
All of this can be configured via different tools. WireGuard itself ships its own tools in the userspace package wireguard-tools: wg and wg-quick.
Important attributes of a WireGuard interface are:
private key: together with the corresponding public key, they are used to authenticate and encrypt data. This is generated with the wg genkey command. listen port: the UDP port that WireGuard will be listening to for incoming traffic. List of peers, each one with: public key: the public counterpart of the private key. Generated from the private key of that peer, using the wg pubkey command. endpoint: where to send the encrypted traffic to. This is optional, but at least one of the corresponding peers must have it to bootstrap the connection. allowed IPs: list of inner tunnel destination networks or addresses for this peer when sending traffic, or, when receiving traffic, which source networks or addresses are allowed to send traffic to us. Tutorials Ubuntu - Wireguard on an internal system Generating WireGuard QR codes Handy Commands Command Description wg genkey > wg_private Generate new private key writing it to the file private wg pubkey < wg_private Generate the public key from a private key ip addr List the IP addresses wg Show the status of the wireguard interfaces sudo wg set wg0 peer <PEER_PUBLIC_KEY> remove Remove the peer from the wireguard interface sudo ip link delete wg0 Delete the wg0 interface sudo wg-quick up wg0 Apply the /etc/wireguard/wg0.conf config file sudo wg-quick down wg0 Removed the wg0 configuration Basic (peer to peer) connection The steps below show how configure a very basic peer-to-peer connection manually using wireguard.
Peer 1 wg genkey > wg_private wg pubkey < wg_private sudo ip link add wg0 type wireguard sudo ip addr add 10.0.0.1/24 dev wg0 sudo wg set wg0 private-key ./wg_private sudo ip link set wg0 up sudo wg set wg0 peer <PEER_2_PUBLIC_KEY> allowed-ips 10.0.0.X/32 endpoint <PEER_2_IP or 0.0.0.0>:<PEER_2_PORT> ping 10.0.0.X - ping peer 2 Peer 2 wg genkey > wg_private wg pubkey < wg_private sudo ip link add wg0 type wireguard sudo ip addr add 10.0.0.X/24 dev wg0 sudo wg set wg0 private-key ./wg_private sudo ip link set wg0 up sudo wg set wg0 peer <PEER_1_PUBLIC_KEY> allowed-ips 10.0.0.1/32 endpoint <PEER_1_ADDRESS>:<PEER_1_PORT> persistent-keepalive 25 ping 10.0.0.1 - ping peer 1 Generating QR codes for clients install qrencode package: sudo apt install qrencode Create keys for client (replace mobile with device name): sudo mkdir -p /etc/wireguard/clients; wg genkey | sudo tee /etc/wireguard/clients/mobile.key | wg pubkey | sudo tee /etc/wireguard/clients/mobile.key.pub Create the client config: sudo nano /etc/wireguard/clients/ben_s7_tablet.conf [Interface] PrivateKey = <Contents of /etc/wireguard/clients/mobile.key> Address = <YOUR_VPN_PRIVATE_IP>/24 DNS = <YOUR_NETWORK_DNS_ADDRESS> [Peer] PublicKey = <YOUR_SERVER_PUBLIC_KEY> AllowedIPs = <THE_IP_RANGE_OF_THE PRIVATE_NETWORK>/24 Endpoint = <YOUR_SERVER_WAN_IP or PUBLIC DNS>:51820 Add the IP address and public key (/etc/wireguard/clients/mobile.key.pub) to the peer configuration. Generate the QR code: # Switch to root user sudo -i # Generate the QR code qrencode -t ansiutf8 < /etc/wireguard/clients/mobile.conf Configuration By default, wireguard looks for configuration files in the /etc/wireguard/ directory. The wg-quick up <config> looks for a configuration file within the /etc/wireguard/ directory.
For example; if you run the command wg-quick up wg0, wireguard will load the configuration from /etc/wireguard/wg0.conf
A simple configuration file can look like this:
[Interface] Address = 10.10.10.10/32 ListenPort = 51000 # or whatever port you want to run wg on PrivateKey = <contents of this-machines-private.key> [Peer] # laptop PublicKey = <contents of laptop-public.key> AllowedIPs = 10.10.10.11/32 # any available IP in the VPN range Creating a Systemd service If you would like to make sure wireguard runs all the time and starts after a system restart you can create a systemd service.
Create the service file /etc/systemd/system/wireguard.service with: sudo nano /etc/systemd/system/wireguard.service and add the following text to the file:
[Unit] Description=Start wireguard. After=network.target [Service] Type=simple ExecStart=wg-quick up wg0 RemainAfterExit=true ExecStop=wg-quick down wg0 StandardOutput=journal StandardError=journal User=root Group=root [Install] WantedBy=multi-user.target Then:
# Reload the systemctl daemon to detect the new file/service $ sudo systemctl daemon-reload # Start the service $ sudo systemctl start wireguard # And check its status $ sudo systemctl status wireguard ● wireguard.service - Start wireguard. Loaded: loaded (/etc/systemd/system/wireguard.service; disabled; vendor preset: enabled) Active: active (exited) since Sat 2023-06-24 08:38:02 AEST; 2s ago Process: 19584 ExecStart=/usr/bin/wg-quick up wg0 (code=exited, status=0/SUCCESS) Main PID: 19584 (code=exited, status=0/SUCCESS) Jun 24 08:38:02 server wg-quick[19584]: [#] wg setconf wg0 /dev/fd/63 Jun 24 08:38:02 server wg-quick[19584]: [#] ip -4 address add .... # You can also check wireguard directly using $ sudo wg # To make the service run on system start $ sudo systemctl enable wireguard Created symlink /etc/systemd/system/multi-user.target.wants/wireguard.service → /etc/systemd/system/wireguard.service. You can test that everything is working as expected by rebooting the system sudo reboot now and checking that the wireguard service and wireguard are working:
# Reboot $ sudo reboot now # After the system is back up... # Check the status of the service $ sudo systemctl status wireguard # And check wireguard $ sudo wg `,description:"",tags:["wireguard","vpn","linux","android","windows"],title:"Wireguard (VPN)",uri:"/tools/wireguard/index.html"},{content:"",description:"",tags:null,title:"wsdl",uri:"/tags/wsdl/index.html"},{content:`WSDL is an XML notation for describing a web service.
A WSDL definition tells a client how to compose a web service request and describes the interface that is provided by the web service provider.
Importing local wsdl files You can import local wsdl files into another wsdl, for example:
<wsdl:import namespace="http://com.example/Utility" location="./utility.wsdl" /> Importing local schema files You can import local schema (.xsd) files for use in your wsdl, for example:
<wsdl:types> <xsd:schema targetNamespace="http://com.example/Imports"> <xsd:import schemaLocation="./local.schema.xsd" namespace="http://com.example/Something" /> </xsd:schema> </wsdl:types> `,description:"",tags:["development","xml","xsd","wsdl","soap","webservice"],title:"WSDL",uri:"/languages/wsdl/index.html"},{content:"",description:"",tags:null,title:"xml",uri:"/tags/xml/index.html"},{content:`Extensible Markup Language (XML) is a markup language and file format for storing, transmitting, and reconstructing arbitrary data.
It defines a set of rules for encoding documents in a format that is both human-readable and machine-readable.
XML Pages SOAPSimple Object Access Protocol (SOAP) is a messaging protocol specification for exchanging structured information in the implementation of web services in computer networks.
XPathXPath is an expression language designed to support the query or transformation of XML documents.
`,description:"",tags:null,title:"XML",uri:"/languages/xml/index.html"},{content:"",description:"",tags:null,title:"xpath",uri:"/tags/xpath/index.html"},{content:`XPath is an expression language designed to support the query or transformation of XML documents.
It was defined by the World Wide Web Consortium and can be used to compute values from the content of an XML document. Support for XPath exists in applications that support XML, such as web browsers, and many programming languages.
Matching node value Given:
<app> <thing>one</thing> </app> You can use /app[thing='one']
Handy links XPath online real-time tester, evaluator and generator for XML & HTML `,description:"",tags:["xpath","development","xml"],title:"XPath",uri:"/languages/xml/xpath/index.html"},{content:"",description:"",tags:null,title:"xsd",uri:"/tags/xsd/index.html"},{content:"",description:"",tags:null,title:"yaml",uri:"/tags/yaml/index.html"}]